# å°ˆé¡Œé›·é” - Topic Radar
# Python å¾Œç«¯ï¼šRSS æŠ“å– + é—œéµå­—éæ¿¾ + AI æ‘˜è¦ (Perplexity) + AI é—œéµå­— (Claude)

import os
import re
import json
import time
import hashlib
import feedparser
import requests
from datetime import datetime
from flask import Flask, jsonify, request
from flask_cors import CORS
from apscheduler.schedulers.background import BackgroundScheduler
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = Flask(__name__, static_folder='.', static_url_path='')
CORS(app)

# ============ è¨­å®š ============

# API Keys
PERPLEXITY_API_KEY = os.getenv('PERPLEXITY_API_KEY', '')
PERPLEXITY_MODEL = os.getenv('PERPLEXITY_MODEL', 'sonar')
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY', '')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '')

# å°ˆé¡Œè¨­å®šå„²å­˜æª”æ¡ˆ
TOPICS_FILE = 'topics_config.json'

# å°ç£åª’é«” RSS ä¾†æº
RSS_SOURCES_TW = {
    'è¯åˆå ±': 'https://udn.com/rssfeed/news/2/0',
    'è¯åˆå ±è²¡ç¶“': 'https://udn.com/rssfeed/news/2/6645',
    'è‡ªç”±æ™‚å ±': 'https://news.ltn.com.tw/rss/all.xml',
    'è‡ªç”±è²¡ç¶“': 'https://news.ltn.com.tw/rss/business.xml',
    'ETtoday': 'https://feeds.feedburner.com/ettoday/realtime',
    'ETtodayè²¡ç¶“': 'https://feeds.feedburner.com/ettoday/finance',
    'å ±å°è€…': 'https://www.twreporter.org/a/rss2.xml',
    'Google News TW': 'https://news.google.com/rss?hl=zh-TW&gl=TW&ceid=TW:zh-Hant',
    'å…¬è¦–æ–°è': 'https://news.pts.org.tw/xml/newsfeed.xml',
    'é¡é€±åˆŠ': 'https://www.mirrormedia.mg/rss/news.xml',
}

# åœ‹éš›åª’é«” RSS ä¾†æºï¼ˆè‹±æ–‡/æ—¥æ–‡ï¼‰
RSS_SOURCES_INTL = {
    'BBC News': 'https://feeds.bbci.co.uk/news/rss.xml',
    'The Guardian': 'https://www.theguardian.com/world/rss',
    'The Japan Times': 'https://www.japantimes.co.jp/feed',
    'NHK (æ—¥æ–‡)': 'https://www3.nhk.or.jp/rss/news/cat0.xml',
    'æœæ—¥æ–°è (æ—¥æ–‡)': 'http://rss.asahi.com/rss/asahi/newsheadlines.rdf',
}

# é è¨­å°ˆé¡Œè¨­å®š
DEFAULT_TOPICS = {
    'migrant_workers': {
        'name': 'æœå‹™æ¥­ç§»å·¥',
        'keywords': ['ç§»å·¥', 'å¤–å‹', 'å‹å‹•éƒ¨', 'ç¼ºå·¥', 'å¤–ç±å‹å·¥', 'ç§»æ°‘ç½²', 'æœå‹™æ¥­', 'é¤é£²æ¥­', 'ä»²ä»‹'],
    },
    'labor_pension': {
        'name': 'å‹ä¿å¹´é‡‘æ”¹é©',
        'keywords': ['å‹ä¿', 'å¹´é‡‘', 'é€€ä¼‘é‡‘', 'å‹å‹•åŸºé‡‘', 'ç²¾ç®—', 'ç ´ç”¢', 'å‹ä¿å±€', 'å‹é€€', 'è€å¹´çµ¦ä»˜'],
    },
    'housing_tax': {
        'name': 'å›¤æˆ¿ç¨…2.0',
        'keywords': ['å›¤æˆ¿ç¨…', 'æˆ¿å±‹ç¨…', 'æŒæœ‰ç¨…', 'æˆ¿åƒ¹', 'ç©ºå±‹', 'å¤šå±‹', 'ç¨…ç‡', 'éè‡ªä½'],
    },
}

# è³‡æ–™å„²å­˜
DATA_STORE = {
    'topics': {},           # æ¯å€‹å°ˆé¡Œçš„å°ç£æ–°èåˆ—è¡¨
    'international': {},    # æ¯å€‹å°ˆé¡Œçš„åœ‹éš›æ–°èåˆ—è¡¨ï¼ˆç¿»è­¯å¾Œï¼‰
    'summaries': {},        # æ¯å€‹å°ˆé¡Œçš„ AI æ‘˜è¦
    'last_update': None,
}

TOPICS = {}

# ============ å°ˆé¡Œè¨­å®šç®¡ç† ============

def load_topics_config():
    """å¾æª”æ¡ˆè¼‰å…¥å°ˆé¡Œè¨­å®š"""
    global TOPICS
    try:
        if os.path.exists(TOPICS_FILE):
            with open(TOPICS_FILE, 'r', encoding='utf-8') as f:
                TOPICS = json.load(f)
        else:
            TOPICS = DEFAULT_TOPICS.copy()
            save_topics_config()
    except Exception as e:
        print(f"[ERROR] è¼‰å…¥å°ˆé¡Œè¨­å®šå¤±æ•—: {e}")
        TOPICS = DEFAULT_TOPICS.copy()

def save_topics_config():
    """å„²å­˜å°ˆé¡Œè¨­å®šåˆ°æª”æ¡ˆ"""
    try:
        with open(TOPICS_FILE, 'w', encoding='utf-8') as f:
            json.dump(TOPICS, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[ERROR] å„²å­˜å°ˆé¡Œè¨­å®šå¤±æ•—: {e}")

def generate_topic_id(name):
    """æ ¹æ“šåç¨±ç”Ÿæˆå”¯ä¸€ ID"""
    timestamp = int(time.time() * 1000) % 100000
    prefix = re.sub(r'[^\w]', '', name)[:10]
    return f"{prefix}_{timestamp}"

# ============ AI é—œéµå­—ç”Ÿæˆ (Gemini) ============

def generate_keywords_with_ai(topic_name):
    """ä½¿ç”¨ Gemini Flash ç”Ÿæˆè­°é¡Œç›¸é—œé—œéµå­—ï¼ˆä¸­è‹±æ—¥ä¸‰èªï¼‰"""
    if not GEMINI_API_KEY:
        print("[WARN] ç„¡ Gemini API Keyï¼Œä½¿ç”¨é è¨­é—œéµå­—")
        return {
            'zh': [topic_name],
            'en': [],
            'ja': []
        }

    try:
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
        headers = {
            "Content-Type": "application/json"
        }
        params = {
            "key": GEMINI_API_KEY
        }

        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–°èè³‡æ–™åº«ç®¡ç†å“¡ã€‚è«‹é‡å°ã€Œ{topic_name}ã€é€™å€‹æ–°èè­°é¡Œï¼Œåˆ—å‡ºæœå°‹é—œéµå­—ã€‚

è¦æ±‚ï¼š
1. ç¹é«”ä¸­æ–‡é—œéµå­—ï¼š10-15 å€‹ï¼ˆæ ¸å¿ƒè©å½™ã€ç›¸é—œå–®ä½ã€åŒç¾©è©ï¼‰
2. è‹±æ–‡é—œéµå­—ï¼š8-10 å€‹ï¼ˆå°æ‡‰çš„è‹±æ–‡è©å½™ï¼Œç”¨æ–¼æœå°‹åœ‹éš›æ–°èï¼‰
3. æ—¥æ–‡é—œéµå­—ï¼š8-10 å€‹ï¼ˆå°æ‡‰çš„æ—¥æ–‡è©å½™ï¼Œç”¨æ–¼æœå°‹æ—¥æœ¬æ–°èï¼‰

æ ¼å¼ï¼ˆè«‹åš´æ ¼éµå®ˆï¼‰ï¼š
ZH: é—œéµå­—1, é—œéµå­—2, é—œéµå­—3
EN: keyword1, keyword2, keyword3
JA: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3

ç›´æ¥è¼¸å‡ºï¼Œä¸è¦æœ‰å…¶ä»–é–‹å ´ç™½æˆ–è§£é‡‹ã€‚"""

        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 800
            }
        }

        response = requests.post(url, headers=headers, params=params, json=payload, timeout=30)
        response.raise_for_status()

        data = response.json()
        content = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')

        # è§£æä¸‰èªé—œéµå­—
        keywords = {'zh': [], 'en': [], 'ja': []}
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('ZH:'):
                keywords['zh'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('EN:'):
                keywords['en'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('JA:'):
                keywords['ja'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]

        # ç¢ºä¿è‡³å°‘æœ‰åŸºæœ¬é—œéµå­—
        if not keywords['zh']:
            keywords['zh'] = [topic_name]

        print(f"[AI] Gemini ç‚ºã€Œ{topic_name}ã€ç”Ÿæˆäº†é—œéµå­—: ZH={len(keywords['zh'])}, EN={len(keywords['en'])}, JA={len(keywords['ja'])}")
        return keywords

    except Exception as e:
        print(f"[ERROR] Gemini é—œéµå­—ç”Ÿæˆå¤±æ•—: {e}")
        return {
            'zh': [topic_name],
            'en': [],
            'ja': []
        }

# ============ Gemini Flash ç¿»è­¯ ============

def translate_with_gemini(text, source_lang='auto', max_retries=3):
    """ä½¿ç”¨ Gemini Flash ç¿»è­¯æ¨™é¡Œåˆ°ç¹é«”ä¸­æ–‡"""
    if not GEMINI_API_KEY:
        return f"[æœªç¿»è­¯] {text}"

    for attempt in range(max_retries):
        try:
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
            headers = {
                "Content-Type": "application/json"
            }

            params = {
                "key": GEMINI_API_KEY
            }

            payload = {
                "contents": [{
                    "parts": [{
                        "text": f"è«‹å°‡ä»¥ä¸‹æ–°èæ¨™é¡Œç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼Œåªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–èªªæ˜ï¼š\n\n{text}"
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 200
                }
            }

            response = requests.post(url, headers=headers, params=params, json=payload, timeout=15)
            
            # å¦‚æœæ˜¯ 429 Too Many Requestsï¼Œç­‰å¾…å¾Œé‡è©¦
            if response.status_code == 429:
                wait_time = (attempt + 1) * 2  # 2, 4, 6 ç§’
                print(f"[WARN] Gemini API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
                continue
            
            response.raise_for_status()

            data = response.json()
            translated = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '').strip()

            return translated if translated else f"[ç¿»è­¯å¤±æ•—] {text}"

        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep((attempt + 1) * 2)
                continue
            print(f"[ERROR] Gemini ç¿»è­¯å¤±æ•—: {e}")
            return f"[ç¿»è­¯å¤±æ•—] {text}"
    
    return f"[ç¿»è­¯å¤±æ•—] {text}"

# ============ Perplexity AI æ‘˜è¦ ============

def generate_topic_summary(topic_id):
    """ä½¿ç”¨ Perplexity AI ç”Ÿæˆå°ˆé¡Œæ‘˜è¦"""
    if not PERPLEXITY_API_KEY:
        return "ï¼ˆå°šæœªè¨­å®š Perplexity API Keyï¼‰"
    
    topic_config = TOPICS.get(topic_id)
    if not topic_config:
        return "ï¼ˆæœªçŸ¥å°ˆé¡Œï¼‰"
    
    topic_name = topic_config['name']
    
    try:
        url = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # å–å¾—ç›®å‰æœ€æ–°çš„å¹¾å‰‡æ–°èæ¨™é¡Œä½œç‚ºåƒè€ƒï¼ˆè¼”åŠ©ï¼‰
        if topic_id in DATA_STORE['topics']:
            recent_titles = [f"- {n['title']} ({n['published'].strftime('%Y/%m/%d') if isinstance(n['published'], datetime) else ''})" 
                           for n in DATA_STORE['topics'][topic_id][:5]]
            context = "\n".join(recent_titles)
        else:
            context = "ï¼ˆæš«ç„¡ç›¸é—œ RSS æ–°èï¼‰"

        current_time = datetime.now().strftime('%Y/%m/%d')
        
        payload = {
            "model": PERPLEXITY_MODEL,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½è³‡æ·±å°ˆé¡Œè¨˜è€…ï¼Œæ­£åœ¨ç‚ºå·²ç¶“ç†Ÿæ‚‰è­°é¡ŒèƒŒæ™¯çš„åŒäº‹æ›´æ–°æœ€æ–°é€²å±•ã€‚è«‹ç”¨ã€Œé€²åº¦å ±å‘Šã€çš„æ ¼å¼æ’°å¯«ï¼Œå‡è¨­è®€è€…å·²äº†è§£è­°é¡ŒèƒŒæ™¯ï¼Œä¸éœ€è¦é‡è¤‡èªªæ˜åŸºæœ¬æ¦‚å¿µæˆ–æ­·å²è„ˆçµ¡ã€‚ç›´æ¥åˆ‡å…¥æœ€æ–°å‹•æ…‹å’Œè®ŠåŒ–ã€‚"
                },
                {
                    "role": "user",
                    "content": f"è­°é¡Œï¼š{topic_name}\næ—¥æœŸï¼š{current_time}\n\nè«‹ç”¨é€²åº¦å ±å‘Šæ ¼å¼ï¼Œé‡é»èªªæ˜ï¼š\n1. æœ¬é€±æˆ–è¿‘æœŸæœ‰ä»€éº¼æ–°å‹•æ…‹ï¼Ÿï¼ˆæ”¿ç­–ç™¼å¸ƒã€å”å•†é€²å±•ã€é‡è¦äº‹ä»¶ã€çˆ­è­°ï¼‰\n2. ç›®å‰æ¨é€²åˆ°ä»€éº¼éšæ®µï¼Ÿæœ‰ä»€éº¼é—œéµé€²å±•æˆ–è½‰æŠ˜ï¼Ÿ\n3. æ¥ä¸‹ä¾†å€¼å¾—é—œæ³¨çš„ç„¦é»æ˜¯ä»€éº¼ï¼Ÿ\n\næ ¼å¼è¦æ±‚ï¼š\n- 200 å­—ä»¥å…§ï¼Œç¹é«”ä¸­æ–‡\n- ç”¨ã€Œé€²åº¦æ›´æ–°ã€èªæ°£ï¼Œä¸æ˜¯ã€Œè­°é¡Œä»‹ç´¹ã€\n- ä¸è¦ä½¿ç”¨å¼•ç”¨æ¨™è¨˜ï¼ˆ[1][2] ç­‰ï¼‰\n- èªæ°£å°ˆæ¥­ã€å®¢è§€ã€ç²¾ç°¡"
                }
            ],
            "max_tokens": 500,
            "temperature": 0.2
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=45)
        response.raise_for_status()
        
        data = response.json()
        summary = data.get('choices', [{}])[0].get('message', {}).get('content', '')
        
        # ç§»é™¤å¯èƒ½çš„å¼•ç”¨æ¨™è¨˜ [1], [2] ç­‰
        summary = re.sub(r'\[\d+\]', '', summary)
        
        # ç§»é™¤ markdown æ ¼å¼ç¬¦è™Ÿï¼ˆ#ã€**ã€*ã€###ç­‰ï¼‰
        summary = re.sub(r'^#{1,6}\s*', '', summary, flags=re.MULTILINE)  # ç§»é™¤æ¨™é¡Œç¬¦è™Ÿ
        summary = re.sub(r'\*\*([^*]+)\*\*', r'\1', summary)  # ç§»é™¤ç²—é«” **text**
        summary = re.sub(r'\*([^*]+)\*', r'\1', summary)  # ç§»é™¤æ–œé«” *text*
        summary = re.sub(r'^[-*]\s+', '', summary, flags=re.MULTILINE)  # ç§»é™¤åˆ—è¡¨ç¬¦è™Ÿ
        
        return summary.strip() if summary else "ï¼ˆç„¡æ³•ç”Ÿæˆæ‘˜è¦ï¼‰"
    
    except Exception as e:
        print(f"[ERROR] Perplexity æ‘˜è¦å¤±æ•—: {e}")
        return f"ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼‰"

# ============ RSS æŠ“å– ============

def fetch_rss(url, source_name, timeout=15):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=timeout, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        
        items = []
        for entry in feed.entries[:30]:
            published = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                published = datetime(*entry.updated_parsed[:6])
            else:
                published = datetime.now()
            
            items.append({
                'title': entry.get('title', ''),
                'link': entry.get('link', ''),
                'source': source_name,
                'published': published,
                'summary': entry.get('summary', '')[:200]
            })
        return items
    except Exception as e:
        print(f"[ERROR] æŠ“å– {source_name} å¤±æ•—: {e}")
        return []

def keyword_match(text, keywords):
    """é—œéµå­—æ¯”å°"""
    if not text or not keywords:
        return False
    text_lower = text.lower()
    for kw in keywords:
        if kw.lower() in text_lower:
            return True
    return False

def update_topic_news():
    print(f"\n[UPDATE] é–‹å§‹æ›´æ–°æ–°è - {datetime.now().strftime('%H:%M:%S')}")

    # 1. æŠ“å–å°ç£æ–°è
    all_news_tw = []
    for name, url in RSS_SOURCES_TW.items():
        all_news_tw.extend(fetch_rss(url, name))

    # 2. æŠ“å–åœ‹éš›æ–°è
    all_news_intl = []
    for name, url in RSS_SOURCES_INTL.items():
        all_news_intl.extend(fetch_rss(url, name))

    # 3. éæ¿¾å°ç£æ–°èå’Œåœ‹éš›æ–°è
    for tid, cfg in TOPICS.items():
        keywords = cfg.get('keywords', {})

        # è™•ç†èˆŠæ ¼å¼ï¼ˆç´”åˆ—è¡¨ï¼‰vs æ–°æ ¼å¼ï¼ˆå­—å…¸ï¼‰
        if isinstance(keywords, list):
            keywords_zh = keywords
            keywords_en = []
            keywords_ja = []
        else:
            keywords_zh = keywords.get('zh', [])
            keywords_en = keywords.get('en', [])
            keywords_ja = keywords.get('ja', [])

        # éæ¿¾å°ç£æ–°èï¼ˆä½¿ç”¨ä¸­æ–‡é—œéµå­—ï¼‰
        if not keywords_zh:
            DATA_STORE['topics'][tid] = []
        else:
            filtered_tw = []
            seen_tw = set()
            for item in all_news_tw:
                content = f"{item['title']} {item['summary']}"
                if keyword_match(content, keywords_zh):
                    h = hashlib.md5(item['title'].encode()).hexdigest()
                    if h not in seen_tw:
                        seen_tw.add(h)
                        filtered_tw.append(item)

            filtered_tw.sort(key=lambda x: x['published'], reverse=True)
            DATA_STORE['topics'][tid] = filtered_tw[:20]

        # éæ¿¾åœ‹éš›æ–°èï¼ˆä½¿ç”¨è‹±æ—¥æ–‡é—œéµå­—ï¼‰
        intl_keywords = keywords_en + keywords_ja
        if not intl_keywords:
            DATA_STORE['international'][tid] = []
        else:
            filtered_intl = []
            seen_intl = set()
            for item in all_news_intl:
                content = f"{item['title']} {item['summary']}"
                if keyword_match(content, intl_keywords):
                    h = hashlib.md5(item['title'].encode()).hexdigest()
                    if h not in seen_intl:
                        seen_intl.add(h)
                        # ç¿»è­¯æ¨™é¡Œï¼ˆåŠ å…¥å»¶é²é¿å… API é€Ÿç‡é™åˆ¶ï¼‰
                        original_title = item['title']
                        translated_title = translate_with_gemini(original_title)
                        item['title_original'] = original_title
                        item['title'] = translated_title
                        filtered_intl.append(item)
                        time.sleep(0.5)  # æ¯æ¬¡ç¿»è­¯å¾Œç­‰å¾… 0.5 ç§’

            filtered_intl.sort(key=lambda x: x['published'], reverse=True)
            DATA_STORE['international'][tid] = filtered_intl[:10]

    DATA_STORE['last_update'] = datetime.now().isoformat()
    print("[UPDATE] å®Œæˆ")
    # æ‘˜è¦æ›´æ–°æ”¹ç”¨æ’ç¨‹ï¼ˆæ¯å¤© 8:00 å’Œ 18:00ï¼‰ï¼Œä¸åœ¨æ–°èæ›´æ–°æ™‚è§¸ç™¼

def update_all_summaries():
    print(f"\n[SUMMARY] é–‹å§‹ AI æ‘˜è¦...")
    for tid in TOPICS.keys():
        summary = generate_topic_summary(tid)
        DATA_STORE['summaries'][tid] = {
            'text': summary,
            'updated_at': datetime.now().isoformat()
        }
        time.sleep(1)
    print("[SUMMARY] å®Œæˆ")

# ============ API ============

@app.route('/')
def index():
    return app.send_static_file('index.html')

@app.route('/admin')
def admin():
    return app.send_static_file('admin.html')

@app.route('/api/all')
def get_all():
    result = {'topics': {}, 'last_update': DATA_STORE['last_update']}
    for tid, cfg in TOPICS.items():
        news = DATA_STORE['topics'].get(tid, [])
        intl_news = DATA_STORE['international'].get(tid, [])
        summary = DATA_STORE['summaries'].get(tid, {})

        # æ ¼å¼åŒ–å°ç£æ–°è
        fmt_news = []
        for n in news[:10]:
            dt = n['published']
            now = datetime.now()
            if dt.date() == now.date():
                time_str = dt.strftime('%H:%M')
            else:
                time_str = dt.strftime('%m/%d')

            fmt_news.append({
                'title': n['title'],
                'link': n['link'],
                'source': n['source'],
                'time': time_str
            })

        # æ ¼å¼åŒ–åœ‹éš›æ–°è
        fmt_intl_news = []
        for n in intl_news[:5]:
            dt = n['published']
            now = datetime.now()
            if dt.date() == now.date():
                time_str = dt.strftime('%H:%M')
            else:
                time_str = dt.strftime('%m/%d')

            fmt_intl_news.append({
                'title': n['title'],
                'title_original': n.get('title_original', ''),
                'link': n['link'],
                'source': n['source'],
                'time': time_str
            })

        # è™•ç†é—œéµå­—é¡¯ç¤ºï¼ˆåªé¡¯ç¤ºä¸­æ–‡é—œéµå­—ï¼‰
        keywords = cfg.get('keywords', [])
        if isinstance(keywords, dict):
            display_keywords = keywords.get('zh', [])
        else:
            display_keywords = keywords

        result['topics'][tid] = {
            'id': tid,
            'name': cfg['name'],
            'icon': cfg.get('icon', 'ğŸ“Œ'),
            'keywords': display_keywords,
            'summary': summary.get('text', ''),
            'summary_updated': summary.get('updated_at'),
            'news': fmt_news,
            'international': fmt_intl_news
        }
    return jsonify(result)

@app.route('/api/refresh', methods=['POST'])
def refresh():
    update_topic_news()
    return jsonify({'status': 'ok'})

@app.route('/api/refresh-summary', methods=['POST'])
def refresh_summary():
    update_all_summaries()
    return jsonify({'status': 'ok'})

@app.route('/api/admin/topics', methods=['GET'])
def get_topics():
    # å›å‚³å°ˆé¡Œè¨­å®šåŠæ‘˜è¦è³‡è¨Š
    result = {}
    for tid, cfg in TOPICS.items():
        # è™•ç†é—œéµå­—æ ¼å¼ï¼ˆæ–°æ ¼å¼ dict vs èˆŠæ ¼å¼ listï¼‰
        keywords = cfg.get('keywords', [])
        if isinstance(keywords, dict):
            display_keywords = keywords.get('zh', [])
        else:
            display_keywords = keywords
        
        # å–å¾—æ‘˜è¦
        summary_data = DATA_STORE['summaries'].get(tid, {})
        
        # å–å¾—æ–°èæ•¸é‡
        news_count = len(DATA_STORE['topics'].get(tid, []))
        
        result[tid] = {
            'name': cfg['name'],
            'keywords': display_keywords,
            'icon': cfg.get('icon', ''),
            'summary': summary_data.get('text', ''),
            'summary_updated': summary_data.get('updated_at'),
            'news_count': news_count
        }
    return jsonify({'topics': result, 'last_update': DATA_STORE['last_update']})

@app.route('/api/admin/topics', methods=['POST'])
def add_topic():
    data = request.json
    name = data.get('name', '').strip()
    if not name:
        return jsonify({'error': 'Empty name'}), 400
    
    # AI ç”Ÿæˆé—œéµå­—
    keywords = generate_keywords_with_ai(name)
    
    tid = generate_topic_id(name)
    TOPICS[tid] = {'name': name, 'keywords': keywords}
    save_topics_config()
    
    # ç«‹å³æ›´æ–°è©²å°ˆé¡Œæ–°è
    update_topic_news()
    
    return jsonify({'status': 'ok'})

@app.route('/api/admin/topics/<tid>', methods=['PUT'])
def update_topic(tid):
    if tid not in TOPICS:
        return jsonify({'error': 'Not found'}), 404
    data = request.json
    if 'keywords' in data:
        TOPICS[tid]['keywords'] = data['keywords']
    save_topics_config()
    update_topic_news()
    return jsonify({'status': 'ok'})

@app.route('/api/admin/topics/<tid>', methods=['DELETE'])
def delete_topic(tid):
    if tid in TOPICS:
        del TOPICS[tid]
        save_topics_config()
    return jsonify({'status': 'ok'})

# ============ Main ============

def init_scheduler():
    scheduler = BackgroundScheduler(timezone='Asia/Taipei')
    # 30åˆ†é˜æ›´æ–°ä¸€æ¬¡æ–°è
    scheduler.add_job(update_topic_news, 'interval', minutes=30)
    # AI æ‘˜è¦ï¼šæ¯å¤© 8:00 å’Œ 18:00 åŸ·è¡Œ
    scheduler.add_job(update_all_summaries, 'cron', hour=8, minute=0)
    scheduler.add_job(update_all_summaries, 'cron', hour=18, minute=0)
    scheduler.start()
    print("[SCHEDULER] æ’ç¨‹å·²å•Ÿå‹• - æ–°èæ¯30åˆ†é˜, æ‘˜è¦æ¯å¤©08:00/18:00")

import urllib3
urllib3.disable_warnings()

# ============ æ¨¡çµ„è¼‰å…¥æ™‚åˆå§‹åŒ–ï¼ˆGunicorn éœ€è¦ï¼‰============
load_topics_config()
init_scheduler()

if __name__ == '__main__':
    print("[INIT] åˆå§‹åŒ–è³‡æ–™...")
    update_topic_news()
    
    if PERPLEXITY_API_KEY:
        # å•Ÿå‹•æ™‚è‡ªå‹•ç”Ÿæˆæ‘˜è¦
        print("[INIT] ç”Ÿæˆ AI æ‘˜è¦...")
        update_all_summaries() 
        
    app.run(port=5001, debug=True, use_reloader=False)

