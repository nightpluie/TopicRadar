# å°ˆé¡Œé›·é” - Topic Radar
# Python å¾Œç«¯ï¼šRSS æŠ“å– + é—œéµå­—éæ¿¾ + AI æ‘˜è¦ (Perplexity) + AI é—œéµå­— (Claude)

import os
import re
import json
import time
import hashlib
import feedparser
import requests
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
from flask import Flask, jsonify, request
from flask_cors import CORS
from apscheduler.schedulers.background import BackgroundScheduler
from dotenv import load_dotenv

# å°åŒ—æ™‚å€
TAIPEI_TZ = ZoneInfo('Asia/Taipei')

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = Flask(__name__, static_folder='.', static_url_path='')
CORS(app)

# ============ è¨­å®š ============

# API Keys
PERPLEXITY_API_KEY = os.getenv('PERPLEXITY_API_KEY', '')
PERPLEXITY_MODEL = os.getenv('PERPLEXITY_MODEL', 'sonar')
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY', '')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '')

# å°ˆé¡Œè¨­å®šå„²å­˜æª”æ¡ˆ
TOPICS_FILE = 'topics_config.json'

# å°ç£åª’é«” RSS ä¾†æº
RSS_SOURCES_TW = {
    'è¯åˆå ±': 'https://udn.com/rssfeed/news/2/0',
    'è¯åˆå ±è²¡ç¶“': 'https://udn.com/rssfeed/news/2/6645',
    'è‡ªç”±æ™‚å ±': 'https://news.ltn.com.tw/rss/all.xml',
    'è‡ªç”±è²¡ç¶“': 'https://news.ltn.com.tw/rss/business.xml',
    'ETtoday': 'https://feeds.feedburner.com/ettoday/realtime',
    'ETtodayè²¡ç¶“': 'https://feeds.feedburner.com/ettoday/finance',
    'å ±å°è€…': 'https://www.twreporter.org/a/rss2.xml',
    'Google News TW': 'https://news.google.com/rss?hl=zh-TW&gl=TW&ceid=TW:zh-Hant',
    'å…¬è¦–æ–°è': 'https://news.pts.org.tw/xml/newsfeed.xml',
    'é¡é€±åˆŠ': 'https://www.mirrormedia.mg/rss/news.xml',
}

# åœ‹éš›åª’é«” RSS ä¾†æºï¼ˆè‹±æ–‡/æ—¥æ–‡ï¼‰
RSS_SOURCES_INTL = {
    'BBC News': 'https://feeds.bbci.co.uk/news/rss.xml',
    'The Guardian': 'https://www.theguardian.com/world/rss',
    'NHK (æ—¥æ–‡)': 'https://www3.nhk.or.jp/rss/news/cat0.xml',
    'æœæ—¥æ–°è (æ—¥æ–‡)': 'http://rss.asahi.com/rss/asahi/newsheadlines.rdf',
}

# Google News åœ‹éš›ç‰ˆä¾†æº
GOOGLE_NEWS_INTL_REGIONS = {
    'æ—¥æœ¬': {'code': 'JP', 'lang': 'ja'},
    'ç¾åœ‹': {'code': 'US', 'lang': 'en'},
    'æ³•åœ‹': {'code': 'FR', 'lang': 'fr'},
}

# é è¨­å°ˆé¡Œè¨­å®š
DEFAULT_TOPICS = {
    'migrant_workers': {
        'name': 'æœå‹™æ¥­ç§»å·¥',
        'keywords': ['ç§»å·¥', 'å¤–å‹', 'å‹å‹•éƒ¨', 'ç¼ºå·¥', 'å¤–ç±å‹å·¥', 'ç§»æ°‘ç½²', 'æœå‹™æ¥­', 'é¤é£²æ¥­', 'ä»²ä»‹'],
    },
    'labor_pension': {
        'name': 'å‹ä¿å¹´é‡‘æ”¹é©',
        'keywords': ['å‹ä¿', 'å¹´é‡‘', 'é€€ä¼‘é‡‘', 'å‹å‹•åŸºé‡‘', 'ç²¾ç®—', 'ç ´ç”¢', 'å‹ä¿å±€', 'å‹é€€', 'è€å¹´çµ¦ä»˜'],
    },
    'housing_tax': {
        'name': 'å›¤æˆ¿ç¨…2.0',
        'keywords': ['å›¤æˆ¿ç¨…', 'æˆ¿å±‹ç¨…', 'æŒæœ‰ç¨…', 'æˆ¿åƒ¹', 'ç©ºå±‹', 'å¤šå±‹', 'ç¨…ç‡', 'éè‡ªä½'],
    },
}

# è³‡æ–™å„²å­˜
DATA_STORE = {
    'topics': {},           # æ¯å€‹å°ˆé¡Œçš„å°ç£æ–°èåˆ—è¡¨
    'international': {},    # æ¯å€‹å°ˆé¡Œçš„åœ‹éš›æ–°èåˆ—è¡¨ï¼ˆç¿»è­¯å¾Œï¼‰
    'summaries': {},        # æ¯å€‹å°ˆé¡Œçš„ AI æ‘˜è¦
    'last_update': None,
}

# è¼‰å…¥é€²åº¦ç‹€æ…‹
LOADING_STATUS = {
    'is_loading': False,
    'current': 0,
    'total': 0,
    'current_topic': '',
    'phase': ''  # 'news' æˆ– 'summary'
}

TOPICS = {}

# è³‡æ–™å¿«å–æª”æ¡ˆè·¯å¾‘
DATA_CACHE_FILE = 'data_cache.json'

# ============ è³‡æ–™å¿«å–ç®¡ç† ============

def save_data_cache():
    """å„²å­˜è³‡æ–™åˆ°å¿«å–æª”æ¡ˆï¼ˆæœƒè¦†è“‹èˆŠæª”ï¼‰"""
    try:
        # æº–å‚™è¦åºåˆ—åŒ–çš„è³‡æ–™ï¼ˆè™•ç† datetime ç‰©ä»¶ï¼‰
        cache_data = {
            'topics': {},
            'international': {},
            'summaries': DATA_STORE['summaries'],
            'last_update': DATA_STORE['last_update']
        }

        # è™•ç†æ–°èè³‡æ–™ï¼ˆå°‡ datetime è½‰æˆå­—ä¸²ï¼‰
        for tid, news_list in DATA_STORE['topics'].items():
            cache_data['topics'][tid] = []
            for news in news_list:
                news_copy = news.copy()
                if 'published' in news_copy and isinstance(news_copy['published'], datetime):
                    news_copy['published'] = news_copy['published'].isoformat()
                cache_data['topics'][tid].append(news_copy)

        for tid, news_list in DATA_STORE['international'].items():
            cache_data['international'][tid] = []
            for news in news_list:
                news_copy = news.copy()
                if 'published' in news_copy and isinstance(news_copy['published'], datetime):
                    news_copy['published'] = news_copy['published'].isoformat()
                cache_data['international'][tid].append(news_copy)

        # å¯«å…¥æª”æ¡ˆï¼ˆè¦†è“‹ï¼‰
        with open(DATA_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

        print(f"[CACHE] è³‡æ–™å·²å„²å­˜åˆ° {DATA_CACHE_FILE}")

    except Exception as e:
        print(f"[CACHE] å„²å­˜å¤±æ•—: {e}")

def load_data_cache():
    """å¾å¿«å–æª”æ¡ˆè¼‰å…¥è³‡æ–™"""
    global DATA_STORE

    if not os.path.exists(DATA_CACHE_FILE):
        print(f"[CACHE] å¿«å–æª”æ¡ˆä¸å­˜åœ¨ï¼Œå°‡ä½¿ç”¨ç©ºè³‡æ–™")
        return

    try:
        with open(DATA_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)

        # è¼‰å…¥æ‘˜è¦å’Œæœ€å¾Œæ›´æ–°æ™‚é–“
        DATA_STORE['summaries'] = cache_data.get('summaries', {})
        DATA_STORE['last_update'] = cache_data.get('last_update')

        # è¼‰å…¥æ–°èè³‡æ–™ï¼ˆå°‡å­—ä¸²è½‰å› datetimeï¼‰
        DATA_STORE['topics'] = {}
        for tid, news_list in cache_data.get('topics', {}).items():
            DATA_STORE['topics'][tid] = []
            for news in news_list:
                news_copy = news.copy()
                if 'published' in news_copy and isinstance(news_copy['published'], str):
                    news_copy['published'] = datetime.fromisoformat(news_copy['published'])
                DATA_STORE['topics'][tid].append(news_copy)

        DATA_STORE['international'] = {}
        for tid, news_list in cache_data.get('international', {}).items():
            DATA_STORE['international'][tid] = []
            for news in news_list:
                news_copy = news.copy()
                if 'published' in news_copy and isinstance(news_copy['published'], str):
                    news_copy['published'] = datetime.fromisoformat(news_copy['published'])
                DATA_STORE['international'][tid].append(news_copy)

        print(f"[CACHE] å¾å¿«å–è¼‰å…¥äº† {len(DATA_STORE['topics'])} å€‹å°ˆé¡Œçš„è³‡æ–™")

    except Exception as e:
        print(f"[CACHE] è¼‰å…¥å¿«å–å¤±æ•—: {e}")

# ============ å°ˆé¡Œè¨­å®šç®¡ç† ============

def load_topics_config():
    """å¾æª”æ¡ˆè¼‰å…¥å°ˆé¡Œè¨­å®š"""
    global TOPICS
    try:
        if os.path.exists(TOPICS_FILE):
            with open(TOPICS_FILE, 'r', encoding='utf-8') as f:
                TOPICS = json.load(f)
        else:
            TOPICS = DEFAULT_TOPICS.copy()
            save_topics_config()
    except Exception as e:
        print(f"[ERROR] è¼‰å…¥å°ˆé¡Œè¨­å®šå¤±æ•—: {e}")
        TOPICS = DEFAULT_TOPICS.copy()

def save_topics_config():
    """å„²å­˜å°ˆé¡Œè¨­å®šåˆ°æª”æ¡ˆ"""
    try:
        with open(TOPICS_FILE, 'w', encoding='utf-8') as f:
            json.dump(TOPICS, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[ERROR] å„²å­˜å°ˆé¡Œè¨­å®šå¤±æ•—: {e}")

def generate_topic_id(name):
    """æ ¹æ“šåç¨±ç”Ÿæˆå”¯ä¸€ ID"""
    timestamp = int(time.time() * 1000) % 100000
    prefix = re.sub(r'[^\w]', '', name)[:10]
    return f"{prefix}_{timestamp}"

# ============ AI é—œéµå­—ç”Ÿæˆ (Gemini) ============

def generate_keywords_with_ai(topic_name):
    """ä½¿ç”¨ Gemini Flash ç”Ÿæˆè­°é¡Œç›¸é—œé—œéµå­—ï¼ˆä¸­è‹±æ—¥ä¸‰èªï¼‰"""
    if not GEMINI_API_KEY:
        print("[WARN] ç„¡ Gemini API Keyï¼Œä½¿ç”¨é è¨­é—œéµå­—")
        return {
            'zh': [topic_name],
            'en': [],
            'ja': []
        }

    try:
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
        headers = {
            "Content-Type": "application/json"
        }
        params = {
            "key": GEMINI_API_KEY
        }

        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–°èè³‡æ–™åº«ç®¡ç†å“¡ã€‚è«‹é‡å°ã€Œ{topic_name}ã€é€™å€‹æ–°èè­°é¡Œï¼Œåˆ—å‡ºæœå°‹é—œéµå­—ã€‚

è¦æ±‚ï¼š
1. ç¹é«”ä¸­æ–‡é—œéµå­—ï¼š10-15 å€‹ï¼ˆæ ¸å¿ƒè©å½™ã€ç›¸é—œå–®ä½ã€åŒç¾©è©ï¼‰
2. è‹±æ–‡é—œéµå­—ï¼š8-10 å€‹ï¼ˆå°æ‡‰çš„è‹±æ–‡è©å½™ï¼Œç”¨æ–¼æœå°‹åœ‹éš›æ–°èï¼‰
3. æ—¥æ–‡é—œéµå­—ï¼š8-10 å€‹ï¼ˆå°æ‡‰çš„æ—¥æ–‡è©å½™ï¼Œç”¨æ–¼æœå°‹æ—¥æœ¬æ–°èï¼‰

æ ¼å¼ï¼ˆè«‹åš´æ ¼éµå®ˆï¼‰ï¼š
ZH: é—œéµå­—1, é—œéµå­—2, é—œéµå­—3
EN: keyword1, keyword2, keyword3
JA: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3

ç›´æ¥è¼¸å‡ºï¼Œä¸è¦æœ‰å…¶ä»–é–‹å ´ç™½æˆ–è§£é‡‹ã€‚"""

        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 800
            }
        }

        response = requests.post(url, headers=headers, params=params, json=payload, timeout=30)
        response.raise_for_status()

        data = response.json()
        content = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')

        # è§£æä¸‰èªé—œéµå­—
        keywords = {'zh': [], 'en': [], 'ja': []}
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('ZH:'):
                keywords['zh'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('EN:'):
                keywords['en'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('JA:'):
                keywords['ja'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]

        # ç¢ºä¿è‡³å°‘æœ‰åŸºæœ¬é—œéµå­—
        if not keywords['zh']:
            keywords['zh'] = [topic_name]

        print(f"[AI] Gemini ç‚ºã€Œ{topic_name}ã€ç”Ÿæˆäº†é—œéµå­—: ZH={len(keywords['zh'])}, EN={len(keywords['en'])}, JA={len(keywords['ja'])}")
        return keywords

    except Exception as e:
        print(f"[ERROR] Gemini é—œéµå­—ç”Ÿæˆå¤±æ•—: {e}")
        return {
            'zh': [topic_name],
            'en': [],
            'ja': []
        }

# ============ Gemini Flash ç¿»è­¯ ============

def translate_with_gemini(text, source_lang='auto', max_retries=3):
    """ä½¿ç”¨ Gemini Flash ç¿»è­¯æ¨™é¡Œåˆ°ç¹é«”ä¸­æ–‡"""
    if not GEMINI_API_KEY:
        return f"[æœªç¿»è­¯] {text}"

    for attempt in range(max_retries):
        try:
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
            headers = {
                "Content-Type": "application/json"
            }

            params = {
                "key": GEMINI_API_KEY
            }

            payload = {
                "contents": [{
                    "parts": [{
                        "text": f"è«‹å°‡ä»¥ä¸‹æ–°èæ¨™é¡Œç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼Œåªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–èªªæ˜ï¼š\n\n{text}"
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 200
                }
            }

            response = requests.post(url, headers=headers, params=params, json=payload, timeout=15)
            
            # å¦‚æœæ˜¯ 429 Too Many Requestsï¼Œç­‰å¾…å¾Œé‡è©¦
            if response.status_code == 429:
                wait_time = (attempt + 1) * 2  # 2, 4, 6 ç§’
                print(f"[WARN] Gemini API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
                continue
            
            response.raise_for_status()

            data = response.json()
            translated = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '').strip()

            return translated if translated else f"[ç¿»è­¯å¤±æ•—] {text}"

        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep((attempt + 1) * 2)
                continue
            print(f"[ERROR] Gemini ç¿»è­¯å¤±æ•—: {e}")
            return f"[ç¿»è­¯å¤±æ•—] {text}"
    
    return f"[ç¿»è­¯å¤±æ•—] {text}"

# ============ Perplexity AI æ‘˜è¦ ============

def generate_topic_summary(topic_id):
    """ä½¿ç”¨ Perplexity AI ç”Ÿæˆå°ˆé¡Œæ‘˜è¦"""
    if not PERPLEXITY_API_KEY:
        return "ï¼ˆå°šæœªè¨­å®š Perplexity API Keyï¼‰"
    
    topic_config = TOPICS.get(topic_id)
    if not topic_config:
        return "ï¼ˆæœªçŸ¥å°ˆé¡Œï¼‰"
    
    topic_name = topic_config['name']
    
    try:
        url = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # å–å¾—ç›®å‰æœ€æ–°çš„å¹¾å‰‡æ–°èæ¨™é¡Œä½œç‚ºåƒè€ƒï¼ˆè¼”åŠ©ï¼‰
        if topic_id in DATA_STORE['topics']:
            recent_titles = [f"- {n['title']} ({n['published'].strftime('%Y/%m/%d') if isinstance(n['published'], datetime) else ''})" 
                           for n in DATA_STORE['topics'][topic_id][:5]]
            context = "\n".join(recent_titles)
        else:
            context = "ï¼ˆæš«ç„¡ç›¸é—œ RSS æ–°èï¼‰"

        current_time = datetime.now(TAIPEI_TZ).strftime('%Y/%m/%d')
        
        payload = {
            "model": PERPLEXITY_MODEL,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½è³‡æ·±å°ˆé¡Œè¨˜è€…ï¼Œæ­£åœ¨ç‚ºå·²ç¶“ç†Ÿæ‚‰è­°é¡ŒèƒŒæ™¯çš„åŒäº‹æ›´æ–°æœ€æ–°é€²å±•ã€‚å‡è¨­è®€è€…å·²äº†è§£è­°é¡ŒèƒŒæ™¯ï¼Œä¸éœ€è¦é‡è¤‡èªªæ˜åŸºæœ¬æ¦‚å¿µæˆ–æ­·å²è„ˆçµ¡ã€‚ç›´æ¥åˆ‡å…¥æœ€æ–°å‹•æ…‹å’Œè®ŠåŒ–ã€‚"
                },
                {
                    "role": "user",
                    "content": f"è­°é¡Œï¼š{topic_name}\næ—¥æœŸï¼š{current_time}\n\nè«‹ç”¨ç´”æ–‡å­—æ ¼å¼ï¼ˆä¸è¦markdownï¼‰è¼¸å‡ºæœ€æ–°é€²å±•æ‘˜è¦ï¼š\n\nå…§å®¹è¦æ±‚ï¼š\n1. æœ¬é€±æˆ–è¿‘æœŸæœ‰ä»€éº¼æ–°å‹•æ…‹ï¼Ÿï¼ˆæ”¿ç­–ç™¼å¸ƒã€å”å•†é€²å±•ã€é‡è¦äº‹ä»¶ã€çˆ­è­°ï¼‰\n2. ç›®å‰æ¨é€²åˆ°ä»€éº¼éšæ®µï¼Ÿæœ‰ä»€éº¼é—œéµé€²å±•æˆ–è½‰æŠ˜ï¼Ÿ\n3. ç¸½å…±120å­—ä»¥å…§ï¼Œç¹é«”ä¸­æ–‡\n4. å¦‚æœæœ‰2-3å€‹é‡é»ï¼Œæ¯å€‹é‡é»è‡ªæˆä¸€å¥ï¼Œç”¨å¥è™Ÿçµå°¾å³å¯\n\næ ¼å¼è¦å‰‡ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š\n- ç¬¬ä¸€å€‹å­—ç›´æ¥é–‹å§‹å¯«å…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•ç©ºè¡Œã€ç©ºæ ¼æˆ–å‰ç¶´\n- ä¸è¦ä»»ä½•æ¨™é¡Œï¼ˆå¦‚ã€Œæœ€æ–°å‹•æ…‹ã€ã€Œé€²åº¦å ±å‘Šã€ç­‰ï¼‰\n- ä¸è¦å¼•ç”¨æ¨™è¨˜ [1][2]\n- ä¸è¦markdownç¬¦è™Ÿï¼ˆ#ã€**ã€*ã€-ï¼‰\n- ä¸è¦åœ¨çµå°¾æ¨™è¨»å­—æ•¸\n- ä¸è¦ç©ºè¡Œåˆ†æ®µï¼Œæ‰€æœ‰å…§å®¹é€£çºŒæ›¸å¯«\n- æ¯å€‹é‡é»ç”¨å¥è™Ÿçµå°¾ï¼Œç„¶å¾Œç›´æ¥æ¥ä¸‹ä¸€å€‹é‡é»\n\nç¯„ä¾‹æ ¼å¼ï¼ˆæ³¨æ„æ²’æœ‰ç©ºè¡Œï¼‰ï¼š\nå‹ä¿å¹´é‡‘æ”¹é©è‰æ¡ˆå·²æ–¼2026å¹´1æœˆæ­£å¼å•Ÿå‹•ï¼Œé è¨ˆæœ€ä½æŠ•ä¿è–ªè³‡èª¿å‡è‡³29,500å…ƒã€‚æ³•æ¡ˆå¯©è­°é è¨ˆåœ¨2026å¹´3æœˆå®Œæˆåˆå¯©ï¼Œä½†è—ç¶ å°æ–¼å¹´é½¡ç´šè·ä»å­˜åœ¨åˆ†æ­§ã€‚æ¥ä¸‹ä¾†éœ€é—œæ³¨ç«‹æ³•é™¢å¯©è­°é€²åº¦åŠå„æ–¹å”å•†çµæœã€‚"
                }
            ],
            "max_tokens": 400,
            "temperature": 0.2
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=45)
        response.raise_for_status()
        
        data = response.json()
        content = data.get('choices', [{}])[0].get('message', {}).get('content', '')

        # ç§»é™¤å¯èƒ½çš„å¼•ç”¨æ¨™è¨˜ [1], [2] ç­‰
        content = re.sub(r'\[\d+\]', '', content)

        # ç§»é™¤ markdown æ ¼å¼ç¬¦è™Ÿï¼ˆ#ã€**ã€*ã€###ç­‰ï¼‰
        content = re.sub(r'^#{1,6}\s*', '', content, flags=re.MULTILINE)  # ç§»é™¤æ¨™é¡Œç¬¦è™Ÿ
        content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)  # ç§»é™¤ç²—é«” **text**
        content = re.sub(r'\*([^*]+)\*', r'\1', content)  # ç§»é™¤æ–œé«” *text*
        content = re.sub(r'^[-*]\s+', '', content, flags=re.MULTILINE)  # ç§»é™¤åˆ—è¡¨ç¬¦è™Ÿ

        # ç§»é™¤é–‹é ­å¯èƒ½å‡ºç¾çš„æ¨™é¡Œï¼ˆå¦‚ã€Œé€²åº¦å ±å‘Šï¼šã€ã€Œæœ€æ–°å‹•æ…‹ï¼šã€ç­‰ï¼‰
        content = re.sub(r'^[^ï¼š]*é€²åº¦å ±å‘Š[ï¼š:]\s*', '', content)
        content = re.sub(r'^[^ï¼š]*æœ€æ–°å‹•æ…‹[ï¼š:]\s*', '', content)
        content = re.sub(r'^[^ï¼š]*æ‘˜è¦[ï¼š:]\s*', '', content)

        # ç§»é™¤çµå°¾çš„å­—æ•¸æ¨™è¨˜ï¼ˆå¦‚ã€Œï¼ˆ200å­—ï¼‰ã€ã€Œ(200å­—)ã€ç­‰ï¼‰
        content = re.sub(r'[ï¼ˆ(]\s*\d+\s*å­—\s*[ï¼‰)]$', '', content)

        # ç¬¬ä¸€æ¬¡æ¸…ç†ï¼šç§»é™¤é¦–å°¾ç©ºç™½
        content = content.strip()

        # ç¬¬äºŒæ¬¡æ¸…ç†ï¼šç§»é™¤é–‹é ­çš„æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€tabã€æ›è¡Œï¼‰
        while content and content[0] in (' ', '\t', '\n', '\r'):
            content = content[1:]

        # ç¬¬ä¸‰æ¬¡æ¸…ç†ï¼šä½¿ç”¨ regex ç§»é™¤é–‹é ­æ‰€æœ‰ç©ºç™½
        content = re.sub(r'^[\s\n\r]+', '', content)

        # ç§»é™¤çµå°¾çš„æ‰€æœ‰é€£çºŒç©ºè¡Œ
        content = re.sub(r'[\s\n\r]+$', '', content)

        # æœ€å¾Œä¸€æ¬¡ strip ç¢ºä¿ä¹¾æ·¨
        content = content.strip()

        return content if content else "ï¼ˆç„¡æ³•ç”Ÿæˆæ‘˜è¦ï¼‰"
    
    except Exception as e:
        print(f"[ERROR] Perplexity æ‘˜è¦å¤±æ•—: {e}")
        return "ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼‰"

# ============ RSS æŠ“å– ============

def fetch_rss(url, source_name, timeout=15, max_items=50):
    """æŠ“å– RSSï¼Œå¢åŠ æœ€å¤§æŠ“å–æ•¸é‡ä»¥ç¢ºä¿èƒ½æ‰¾åˆ°è¶³å¤ çš„ç›¸é—œæ–°è"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=timeout, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        items = []
        # å¢åŠ æŠ“å–æ•¸é‡å¾ 30 åˆ° max_itemsï¼Œç¢ºä¿æœ‰è¶³å¤ æ–°èå¯éæ¿¾
        for entry in feed.entries[:max_items]:
            # ç²å–æ¨™é¡Œï¼Œè·³éç©ºæ¨™é¡Œ
            title = entry.get('title', '').strip()
            if not title:
                continue

            published = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                # RSS æ™‚é–“é€šå¸¸æ˜¯ UTCï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                published = published.astimezone(TAIPEI_TZ)
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                published = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                published = published.astimezone(TAIPEI_TZ)
            else:
                # ä½¿ç”¨å°åŒ—æ™‚é–“çš„ç•¶å‰æ™‚é–“
                published = datetime.now(TAIPEI_TZ)

            items.append({
                'title': title,
                'link': entry.get('link', ''),
                'source': source_name,
                'published': published,
                'summary': entry.get('summary', '')[:200]
            })
        return items
    except Exception as e:
        print(f"[ERROR] æŠ“å– {source_name} å¤±æ•—: {e}")
        return []

def fetch_google_news_by_keywords(keywords, max_items=50):
    """ä½¿ç”¨ Google News æœç´¢ç‰¹å®šé—œéµå­—çš„æ–°èï¼Œä¸¦æå–åŸå§‹åª’é«”ä¾†æº"""
    if not keywords:
        return []

    # ä½¿ç”¨ç¬¬ä¸€å€‹é—œéµå­—ä½œç‚ºæœç´¢è©
    search_term = keywords[0] if isinstance(keywords, list) else keywords
    # Google News æœç´¢ RSS
    url = f"https://news.google.com/rss/search?q={search_term}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"

    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        items = []
        for entry in feed.entries[:max_items]:
            # ç²å–æ¨™é¡Œï¼Œè·³éç©ºæ¨™é¡Œ
            title = entry.get('title', '').strip()
            if not title:
                continue

            # æå–åŸå§‹åª’é«”ä¾†æºï¼ˆGoogle News RSS ç‰¹æœ‰ï¼‰
            source_name = 'Google News'
            if hasattr(entry, 'source') and entry.source:
                source_name = entry.source.get('title', 'Google News')

            # è™•ç†æ™‚é–“
            published = None
            is_date_only = False

            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                published = published.astimezone(TAIPEI_TZ)

                # æª¢æŸ¥æ˜¯å¦æ˜¯æ•´é»æ™‚é–“ï¼ˆå¯èƒ½åªæ˜¯æ—¥æœŸçš„å ä½ç¬¦ï¼‰
                # ä¾‹å¦‚ 08:00:00 å¯èƒ½åªä»£è¡¨ç•¶å¤©ï¼Œä¸æ˜¯çœŸå¯¦æ™‚é–“
                if published.minute == 0 and published.second == 0:
                    is_date_only = True
            else:
                published = datetime.now(TAIPEI_TZ)

            items.append({
                'title': title,
                'link': entry.get('link', ''),
                'source': source_name,  # ä½¿ç”¨åŸå§‹åª’é«”åç¨±
                'published': published,
                'summary': entry.get('summary', '')[:200],
                'is_date_only': is_date_only  # æ¨™è¨˜åƒ…æœ‰æ—¥æœŸ
            })
        return items
    except Exception as e:
        print(f"[ERROR] Google News æœç´¢å¤±æ•—: {e}")
        return []

def fetch_google_news_intl(keywords, region_code, lang, max_items=30):
    """ä½¿ç”¨ Google News åœ‹éš›ç‰ˆæœç´¢ç‰¹å®šåœ‹å®¶çš„æ–°è"""
    if not keywords:
        return []

    # ä½¿ç”¨ç¬¬ä¸€å€‹é—œéµå­—ä½œç‚ºæœç´¢è©
    search_term = keywords[0] if isinstance(keywords, list) else keywords
    # Google News åœ‹éš›ç‰ˆ RSSï¼ˆæ ¹æ“šåœ‹å®¶ä»£ç¢¼å’Œèªè¨€ï¼‰
    url = f"https://news.google.com/rss/search?q={search_term}&hl={lang}&gl={region_code}&ceid={region_code}:{lang}"

    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        items = []
        for entry in feed.entries[:max_items]:
            title = entry.get('title', '').strip()
            if not title:
                continue

            # æå–åŸå§‹åª’é«”ä¾†æº
            source_name = f'Google News ({region_code})'
            if hasattr(entry, 'source') and entry.source:
                source_name = entry.source.get('title', source_name)

            # è™•ç†æ™‚é–“
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                published = published.astimezone(TAIPEI_TZ)
            else:
                published = datetime.now(TAIPEI_TZ)

            items.append({
                'title': title,
                'link': entry.get('link', ''),
                'source': source_name,
                'published': published,
                'summary': entry.get('summary', '')[:200]
            })
        return items
    except Exception as e:
        print(f"[ERROR] Google News {region_code} æœç´¢å¤±æ•—: {e}")
        return []

def keyword_match(text, keywords, negative_keywords=None):
    """
    é—œéµå­—æ¯”å°ï¼Œæ”¯æ´è² é¢é—œéµå­—éæ¿¾

    Args:
        text: è¦æª¢æŸ¥çš„æ–‡å­—
        keywords: æ­£é¢é—œéµå­—åˆ—è¡¨ï¼ˆåŒ¹é…ä»»ä¸€å³å¯ï¼‰
        negative_keywords: è² é¢é—œéµå­—åˆ—è¡¨ï¼ˆåŒ…å«ä»»ä¸€å‰‡æ’é™¤ï¼‰
    """
    if not text or not keywords:
        return False

    text_lower = text.lower()

    # å…ˆæª¢æŸ¥è² é¢é—œéµå­—ï¼Œå¦‚æœåŒ…å«å‰‡ç›´æ¥æ’é™¤
    if negative_keywords:
        for neg_kw in negative_keywords:
            if neg_kw.lower() in text_lower:
                return False

    # å†æª¢æŸ¥æ­£é¢é—œéµå­—
    for kw in keywords:
        if kw.lower() in text_lower:
            return True

    return False

def update_single_topic_news(topic_id):
    """åªæ›´æ–°å–®ä¸€å°ˆé¡Œçš„æ–°èï¼ˆç”¨æ–¼æ–°å¢å°ˆé¡Œæ™‚ï¼‰"""
    if topic_id not in TOPICS:
        return

    cfg = TOPICS[topic_id]
    print(f"\n[UPDATE] æ›´æ–°å–®ä¸€å°ˆé¡Œæ–°è: {cfg['name']}")

    # 1. æŠ“å–å°ç£æ–°è
    all_news_tw = []
    for name, url in RSS_SOURCES_TW.items():
        all_news_tw.extend(fetch_rss(url, name, max_items=50))

    # 2. æŠ“å–åœ‹éš›æ–°è
    all_news_intl = []
    for name, url in RSS_SOURCES_INTL.items():
        all_news_intl.extend(fetch_rss(url, name, max_items=50))

    # 3. æŠ“å–è©²å°ˆé¡Œçš„ Google News åœ‹éš›ç‰ˆ
    keywords = cfg.get('keywords', {})
    if isinstance(keywords, dict):
        keywords_en = keywords.get('en', [])
        keywords_ja = keywords.get('ja', [])

        if keywords_ja:
            all_news_intl.extend(fetch_google_news_intl(keywords_ja, 'JP', 'ja', max_items=20))
        if keywords_en:
            all_news_intl.extend(fetch_google_news_intl(keywords_en, 'US', 'en', max_items=20))
            all_news_intl.extend(fetch_google_news_intl(keywords_en, 'FR', 'fr', max_items=20))

    # 4. éæ¿¾è©²å°ˆé¡Œçš„æ–°è
    keywords_zh = keywords.get('zh', []) if isinstance(keywords, dict) else keywords
    keywords_en = keywords.get('en', []) if isinstance(keywords, dict) else []
    keywords_ja = keywords.get('ja', []) if isinstance(keywords, dict) else []
    negative_keywords = cfg.get('negative_keywords', [])

    # éæ¿¾å°ç£æ–°è
    filtered_tw = [n for n in all_news_tw if keyword_match(n['title'], keywords_zh, negative_keywords)]
    print(f"[SEARCH] {cfg['name']}: æ‰¾åˆ° {len(filtered_tw)} å‰‡å°ç£æ–°è")

    # Google News è£œå……ï¼ˆå¦‚éœ€è¦ï¼‰
    if len(filtered_tw) < 10:
        print(f"[SEARCH] {cfg['name']}: åªæœ‰ {len(filtered_tw)} å‰‡ï¼Œä½¿ç”¨ Google News æœç´¢è£œå……...")
        google_news = fetch_google_news_by_keywords(keywords_zh, max_items=20)
        for n in google_news:
            if keyword_match(n['title'], keywords_zh, negative_keywords):
                filtered_tw.append(n)
        print(f"[SEARCH] {cfg['name']}: è£œå……å¾Œå…± {len(filtered_tw)} å‰‡æ–°è")

    # æ›´æ–°è©²å°ˆé¡Œçš„å°ç£æ–°è
    existing = DATA_STORE['topics'].get(topic_id, [])
    existing_hashes = {n['hash'] for n in existing}

    new_items = []
    for n in filtered_tw[:10]:
        n_hash = hashlib.md5(n['title'].encode()).hexdigest()
        n['hash'] = n_hash
        if n_hash not in existing_hashes:
            new_items.append(n)

    all_items = new_items + existing
    DATA_STORE['topics'][topic_id] = all_items[:10]

    if new_items:
        print(f"[UPDATE] {cfg['name']}: æ–°å¢ {len(new_items)} å‰‡æ–°èï¼Œç•¶å‰ {len(DATA_STORE['topics'][topic_id])} å‰‡")

    # éæ¿¾åœ‹éš›æ–°è
    intl_keywords = keywords_en + keywords_ja
    filtered_intl = [n for n in all_news_intl if keyword_match(n['title'], intl_keywords, negative_keywords)]

    # ç¿»è­¯åœ‹éš›æ–°è
    for n in filtered_intl:
        if 'title_original' not in n:
            n['title_original'] = n['title']
            translated = translate_with_gemini(n['title'])
            n['title'] = translated if translated else n['title']
            time.sleep(0.5)

    # Google News åœ‹éš›è£œå……
    if len(filtered_intl) < 5:
        for region_name, region_info in GOOGLE_NEWS_INTL_REGIONS.items():
            if len(filtered_intl) >= 5:
                break
            search_keywords = keywords_ja if region_info['lang'] == 'ja' else keywords_en
            google_intl = fetch_google_news_intl(search_keywords, region_info['code'], region_info['lang'], max_items=20)
            for n in google_intl:
                if keyword_match(n['title'], search_keywords, negative_keywords):
                    n['title_original'] = n['title']
                    translated = translate_with_gemini(n['title'])
                    n['title'] = translated if translated else n['title']
                    filtered_intl.append(n)
                    time.sleep(0.5)

    # æ›´æ–°è©²å°ˆé¡Œçš„åœ‹éš›æ–°è
    existing_intl = DATA_STORE['international'].get(topic_id, [])
    existing_intl_hashes = {n['hash'] for n in existing_intl}

    new_intl_items = []
    for n in filtered_intl[:10]:
        n_hash = hashlib.md5(n.get('title_original', n['title']).encode()).hexdigest()
        n['hash'] = n_hash
        if n_hash not in existing_intl_hashes:
            new_intl_items.append(n)

    all_intl_items = new_intl_items + existing_intl
    DATA_STORE['international'][topic_id] = all_intl_items[:10]

    if new_intl_items:
        print(f"[UPDATE] {cfg['name']} (åœ‹éš›): æ–°å¢ {len(new_intl_items)} å‰‡æ–°èï¼Œç•¶å‰ {len(DATA_STORE['international'][topic_id])} å‰‡")

    DATA_STORE['last_update'] = datetime.now(TAIPEI_TZ).isoformat()

    # å„²å­˜åˆ°å¿«å–æª”æ¡ˆ
    save_data_cache()

    print(f"[UPDATE] {cfg['name']} æ›´æ–°å®Œæˆ")

def update_topic_news():
    global LOADING_STATUS
    total_topics = len(TOPICS)
    LOADING_STATUS = {
        'is_loading': True,
        'current': 0,
        'total': total_topics,
        'current_topic': '',
        'phase': 'news'
    }
    print(f"\n[UPDATE] é–‹å§‹æ›´æ–°æ–°è - {datetime.now(TAIPEI_TZ).strftime('%H:%M:%S')}")

    # 1. æŠ“å–å°ç£æ–°èï¼ˆå¢åŠ æŠ“å–æ•¸é‡ï¼‰
    all_news_tw = []
    for name, url in RSS_SOURCES_TW.items():
        all_news_tw.extend(fetch_rss(url, name, max_items=50))

    # 2. æŠ“å–åœ‹éš›æ–°èï¼ˆå¢åŠ æŠ“å–æ•¸é‡ï¼‰
    all_news_intl = []
    for name, url in RSS_SOURCES_INTL.items():
        all_news_intl.extend(fetch_rss(url, name, max_items=50))

    # 2.5 æŠ“å– Google News åœ‹éš›ç‰ˆæ–°èï¼ˆæ—¥æœ¬ã€ç¾åœ‹ã€æ³•åœ‹ï¼‰
    # ç‚ºæ¯å€‹å°ˆé¡Œçš„åœ‹éš›é—œéµå­—æŠ“å–å°æ‡‰åœ‹å®¶çš„æ–°è
    google_news_intl = []
    for tid, cfg in TOPICS.items():
        keywords = cfg.get('keywords', {})
        if isinstance(keywords, dict):
            keywords_en = keywords.get('en', [])
            keywords_ja = keywords.get('ja', [])

            # æ—¥æœ¬ Google Newsï¼ˆä½¿ç”¨æ—¥æ–‡é—œéµå­—ï¼‰
            if keywords_ja:
                google_news_intl.extend(fetch_google_news_intl(keywords_ja, 'JP', 'ja', max_items=20))

            # ç¾åœ‹ Google Newsï¼ˆä½¿ç”¨è‹±æ–‡é—œéµå­—ï¼‰
            if keywords_en:
                google_news_intl.extend(fetch_google_news_intl(keywords_en, 'US', 'en', max_items=20))

            # æ³•åœ‹ Google Newsï¼ˆä½¿ç”¨è‹±æ–‡é—œéµå­—ï¼‰
            if keywords_en:
                google_news_intl.extend(fetch_google_news_intl(keywords_en, 'FR', 'fr', max_items=20))

    all_news_intl.extend(google_news_intl)

    # 3. éæ¿¾å°ç£æ–°èå’Œåœ‹éš›æ–°è
    topic_index = 0
    for tid, cfg in TOPICS.items():
        topic_index += 1
        LOADING_STATUS['current'] = topic_index
        LOADING_STATUS['current_topic'] = cfg['name']
        keywords = cfg.get('keywords', {})

        # è™•ç†èˆŠæ ¼å¼ï¼ˆç´”åˆ—è¡¨ï¼‰vs æ–°æ ¼å¼ï¼ˆå­—å…¸ï¼‰
        if isinstance(keywords, list):
            keywords_zh = keywords
            keywords_en = []
            keywords_ja = []
        else:
            keywords_zh = keywords.get('zh', [])
            keywords_en = keywords.get('en', [])
            keywords_ja = keywords.get('ja', [])

        # ç²å–è² é¢é—œéµå­—
        negative_keywords = cfg.get('negative_keywords', [])

        # éæ¿¾å°ç£æ–°èï¼ˆä½¿ç”¨ä¸­æ–‡é—œéµå­—ï¼‰- ç¢ºä¿è‡³å°‘10å‰‡
        if not keywords_zh:
            DATA_STORE['topics'][tid] = []
        else:
            # å–å¾—ç¾æœ‰æ–°èåˆ—è¡¨
            existing_news = DATA_STORE['topics'].get(tid, [])
            existing_hashes = {hashlib.md5(item['title'].encode()).hexdigest(): item
                             for item in existing_news}

            # éæ¿¾æ–°æ–°è
            filtered_tw = []
            seen_tw = set(existing_hashes.keys())
            new_items = []

            for item in all_news_tw:
                content = f"{item['title']} {item['summary']}"
                if keyword_match(content, keywords_zh, negative_keywords):
                    h = hashlib.md5(item['title'].encode()).hexdigest()
                    if h not in seen_tw:
                        seen_tw.add(h)
                        new_items.append(item)

            # åˆä½µï¼šæ–°æ–°è + ç¾æœ‰æ–°èï¼ŒæŒ‰æ™‚é–“æ’åº
            all_items = new_items + existing_news
            all_items.sort(key=lambda x: x['published'], reverse=True)

            # å¦‚æœæ–°èæ•¸é‡å°‘æ–¼ 10 å‰‡ï¼Œä½¿ç”¨ Google News æœç´¢è£œå……
            if len(all_items) < 10:
                print(f"[SEARCH] {cfg['name']}: åªæœ‰ {len(all_items)} å‰‡ï¼Œä½¿ç”¨ Google News æœç´¢è£œå……...")
                google_news = fetch_google_news_by_keywords(keywords_zh, max_items=100)

                # éæ¿¾ä¸¦å»é‡
                existing_hashes_all = {hashlib.md5(item['title'].encode()).hexdigest() for item in all_items}
                for item in google_news:
                    if len(all_items) >= 10:
                        break
                    content = f"{item['title']} {item['summary']}"
                    if keyword_match(content, keywords_zh, negative_keywords):
                        h = hashlib.md5(item['title'].encode()).hexdigest()
                        if h not in existing_hashes_all:
                            existing_hashes_all.add(h)
                            all_items.append(item)

                all_items.sort(key=lambda x: x['published'], reverse=True)
                print(f"[SEARCH] {cfg['name']}: è£œå……å¾Œå…± {len(all_items)} å‰‡æ–°è")

            # ä¿æŒæœ€æ–°çš„ 10 å‰‡ï¼ˆä¸€å‰‡ä¸€å‰‡æ›¿æ›ï¼‰
            DATA_STORE['topics'][tid] = all_items[:10]

            if new_items:
                print(f"[UPDATE] {cfg['name']}: æ–°å¢ {len(new_items)} å‰‡æ–°èï¼Œç•¶å‰ {len(DATA_STORE['topics'][tid])} å‰‡")

        # éæ¿¾åœ‹éš›æ–°èï¼ˆä½¿ç”¨è‹±æ—¥æ–‡é—œéµå­—ï¼‰- ç¢ºä¿è‡³å°‘10å‰‡
        intl_keywords = keywords_en + keywords_ja
        if not intl_keywords:
            DATA_STORE['international'][tid] = []
        else:
            # å–å¾—ç¾æœ‰åœ‹éš›æ–°è
            existing_intl = DATA_STORE['international'].get(tid, [])
            existing_intl_hashes = {hashlib.md5(item.get('title_original', item['title']).encode()).hexdigest(): item
                                   for item in existing_intl}

            # éæ¿¾æ–°çš„åœ‹éš›æ–°è
            filtered_intl = []
            seen_intl = set(existing_intl_hashes.keys())
            new_intl_items = []

            for item in all_news_intl:
                content = f"{item['title']} {item['summary']}"
                if keyword_match(content, intl_keywords, negative_keywords):
                    h = hashlib.md5(item['title'].encode()).hexdigest()
                    if h not in seen_intl:
                        seen_intl.add(h)
                        # ç¿»è­¯æ¨™é¡Œï¼ˆåŠ å…¥å»¶é²é¿å… API é€Ÿç‡é™åˆ¶ï¼‰
                        original_title = item['title']
                        translated_title = translate_with_gemini(original_title)
                        item['title_original'] = original_title
                        item['title'] = translated_title
                        new_intl_items.append(item)
                        time.sleep(0.5)  # æ¯æ¬¡ç¿»è­¯å¾Œç­‰å¾… 0.5 ç§’

            # åˆä½µï¼šæ–°æ–°è + ç¾æœ‰æ–°èï¼ŒæŒ‰æ™‚é–“æ’åº
            all_intl_items = new_intl_items + existing_intl
            all_intl_items.sort(key=lambda x: x['published'], reverse=True)

            # å¦‚æœæ–°èæ•¸é‡å°‘æ–¼ 5 å‰‡ï¼Œä½¿ç”¨ Google News åœ‹éš›ç‰ˆè£œå……
            if len(all_intl_items) < 5:
                print(f"[SEARCH] {cfg['name']} (åœ‹éš›): åªæœ‰ {len(all_intl_items)} å‰‡ï¼Œä½¿ç”¨ Google News åœ‹éš›ç‰ˆè£œå……...")

                # ä¾åºå¾æ—¥æœ¬ã€ç¾åœ‹ã€æ³•åœ‹ Google News è£œå……
                for region_name, region_info in GOOGLE_NEWS_INTL_REGIONS.items():
                    if len(all_intl_items) >= 5:
                        break

                    # æ ¹æ“šèªè¨€é¸æ“‡é—œéµå­—
                    search_keywords = keywords_ja if region_info['lang'] == 'ja' else keywords_en
                    if not search_keywords:
                        continue

                    google_intl = fetch_google_news_intl(
                        search_keywords,
                        region_info['code'],
                        region_info['lang'],
                        max_items=20
                    )

                    # éæ¿¾ä¸¦ç¿»è­¯
                    existing_hashes_all = {hashlib.md5(item.get('title_original', item['title']).encode()).hexdigest()
                                         for item in all_intl_items}
                    for item in google_intl:
                        if len(all_intl_items) >= 5:
                            break
                        content = f"{item['title']} {item['summary']}"
                        if keyword_match(content, intl_keywords, negative_keywords):
                            h = hashlib.md5(item['title'].encode()).hexdigest()
                            if h not in existing_hashes_all:
                                existing_hashes_all.add(h)
                                # ç¿»è­¯æ¨™é¡Œ
                                original_title = item['title']
                                translated_title = translate_with_gemini(original_title)
                                item['title_original'] = original_title
                                item['title'] = translated_title
                                all_intl_items.append(item)
                                time.sleep(0.5)

                all_intl_items.sort(key=lambda x: x['published'], reverse=True)
                print(f"[SEARCH] {cfg['name']} (åœ‹éš›): è£œå……å¾Œå…± {len(all_intl_items)} å‰‡æ–°è")

            # ä¿æŒæœ€æ–°çš„ 10 å‰‡ï¼ˆä¸€å‰‡ä¸€å‰‡æ›¿æ›ï¼‰
            DATA_STORE['international'][tid] = all_intl_items[:10]

            if new_intl_items:
                print(f"[UPDATE] {cfg['name']} (åœ‹éš›): æ–°å¢ {len(new_intl_items)} å‰‡æ–°èï¼Œç•¶å‰ {len(DATA_STORE['international'][tid])} å‰‡")

    DATA_STORE['last_update'] = datetime.now(TAIPEI_TZ).isoformat()
    LOADING_STATUS['is_loading'] = False
    LOADING_STATUS['current'] = total_topics

    # å„²å­˜åˆ°å¿«å–æª”æ¡ˆ
    save_data_cache()

    print("[UPDATE] å®Œæˆ")
    # æ‘˜è¦æ›´æ–°æ”¹ç”¨æ’ç¨‹ï¼ˆæ¯å¤© 8:00 å’Œ 18:00ï¼‰ï¼Œä¸åœ¨æ–°èæ›´æ–°æ™‚è§¸ç™¼

def update_all_summaries():
    print(f"\n[SUMMARY] é–‹å§‹ AI æ‘˜è¦...")
    for tid in TOPICS.keys():
        summary_text = generate_topic_summary(tid)
        DATA_STORE['summaries'][tid] = {
            'text': summary_text,
            'updated_at': datetime.now(TAIPEI_TZ).isoformat()
        }
        time.sleep(1)

    # å„²å­˜åˆ°å¿«å–æª”æ¡ˆ
    save_data_cache()

    print("[SUMMARY] å®Œæˆ")

# ============ API ============

@app.route('/')
def index():
    return app.send_static_file('index.html')

@app.route('/admin')
def admin():
    return app.send_static_file('admin.html')

@app.route('/api/all')
def get_all():
    result = {'topics': {}, 'last_update': DATA_STORE['last_update']}
    for tid, cfg in TOPICS.items():
        news = DATA_STORE['topics'].get(tid, [])
        intl_news = DATA_STORE['international'].get(tid, [])
        summary = DATA_STORE['summaries'].get(tid, {})

        # æ ¼å¼åŒ–å°ç£æ–°è
        fmt_news = []
        now = datetime.now(TAIPEI_TZ)
        for n in news[:10]:
            dt = n['published']
            # ç¢ºä¿ dt æœ‰æ™‚å€è³‡è¨Š
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=TAIPEI_TZ)

            # æ ¹æ“šæ—¥æœŸæ±ºå®šé¡¯ç¤ºæ ¼å¼
            is_date_only = n.get('is_date_only', False)

            if is_date_only:
                # Google News ç­‰åªæœ‰æ—¥æœŸçš„æ–°èï¼Œåªé¡¯ç¤ºæ—¥æœŸ
                time_str = dt.strftime('%m/%d')
            elif dt.date() == now.date():
                # ä»Šå¤©çš„æ–°èé¡¯ç¤ºæ™‚é–“
                time_str = dt.strftime('%H:%M')
            else:
                # å…¶ä»–æ—¥æœŸé¡¯ç¤ºæœˆ/æ—¥
                time_str = dt.strftime('%m/%d')

            fmt_news.append({
                'title': n['title'],
                'link': n['link'],
                'source': n['source'],
                'time': time_str
            })

        # æ ¼å¼åŒ–åœ‹éš›æ–°èï¼ˆæœ€å¤š10å‰‡ï¼‰
        fmt_intl_news = []
        for n in intl_news[:10]:
            dt = n['published']
            # ç¢ºä¿ dt æœ‰æ™‚å€è³‡è¨Š
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=TAIPEI_TZ)

            # æ ¹æ“šæ—¥æœŸæ±ºå®šé¡¯ç¤ºæ ¼å¼
            if dt.date() == now.date():
                time_str = dt.strftime('%H:%M')
            else:
                time_str = dt.strftime('%m/%d')

            fmt_intl_news.append({
                'title': n['title'],
                'title_original': n.get('title_original', ''),
                'link': n['link'],
                'source': n['source'],
                'time': time_str
            })

        # è™•ç†é—œéµå­—é¡¯ç¤ºï¼ˆåªé¡¯ç¤ºä¸­æ–‡é—œéµå­—ï¼‰
        keywords = cfg.get('keywords', [])
        if isinstance(keywords, dict):
            display_keywords = keywords.get('zh', [])
        else:
            display_keywords = keywords

        result['topics'][tid] = {
            'id': tid,
            'name': cfg['name'],
            'icon': cfg.get('icon', 'ğŸ“Œ'),
            'keywords': display_keywords,
            'summary': summary.get('text', ''),
            'summary_updated': summary.get('updated_at'),
            'news': fmt_news,
            'international': fmt_intl_news,
            'order': cfg.get('order', 999)
        }
    return jsonify(result)

@app.route('/api/refresh', methods=['POST'])
def refresh():
    update_topic_news()
    return jsonify({'status': 'ok'})

@app.route('/api/refresh-summary', methods=['POST'])
def refresh_summary():
    update_all_summaries()
    return jsonify({'status': 'ok'})

@app.route('/api/loading-status')
def loading_status():
    """å›å‚³è¼‰å…¥é€²åº¦ç‹€æ…‹"""
    return jsonify(LOADING_STATUS)

@app.route('/api/admin/topics', methods=['GET'])
def get_topics():
    # å›å‚³å°ˆé¡Œè¨­å®šåŠæ‘˜è¦è³‡è¨Š
    result = {}
    for tid, cfg in TOPICS.items():
        # è™•ç†é—œéµå­—æ ¼å¼ï¼ˆæ–°æ ¼å¼ dict vs èˆŠæ ¼å¼ listï¼‰
        keywords = cfg.get('keywords', [])
        if isinstance(keywords, dict):
            display_keywords = keywords.get('zh', [])
        else:
            display_keywords = keywords
        
        # å–å¾—æ‘˜è¦
        summary_data = DATA_STORE['summaries'].get(tid, {})
        
        # å–å¾—æ–°èæ•¸é‡
        news_count = len(DATA_STORE['topics'].get(tid, []))
        
        result[tid] = {
            'name': cfg['name'],
            'keywords': display_keywords,
            'negative_keywords': cfg.get('negative_keywords', []),
            'icon': cfg.get('icon', ''),
            'summary': summary_data.get('text', ''),
            'summary_updated': summary_data.get('updated_at'),
            'news_count': news_count,
            'order': cfg.get('order', 999)
        }
    return jsonify({'topics': result, 'last_update': DATA_STORE['last_update']})

@app.route('/api/admin/topics', methods=['POST'])
def add_topic():
    data = request.json
    name = data.get('name', '').strip()
    if not name:
        return jsonify({'error': 'Empty name'}), 400

    # AI ç”Ÿæˆé—œéµå­—
    keywords = generate_keywords_with_ai(name)

    # è¨ˆç®—æ–°å°ˆé¡Œçš„ orderï¼ˆæ”¾åœ¨æœ€å¾Œï¼‰
    max_order = max([t.get('order', 0) for t in TOPICS.values()], default=-1)
    new_order = max_order + 1

    tid = generate_topic_id(name)
    TOPICS[tid] = {'name': name, 'keywords': keywords, 'order': new_order}
    save_topics_config()

    # åªæ›´æ–°æ–°å°ˆé¡Œçš„æ–°èï¼ˆä¸æ›´æ–°å…¶ä»–å°ˆé¡Œï¼‰
    update_single_topic_news(tid)

    # åªç‚ºæ–°å°ˆé¡Œç”Ÿæˆæ‘˜è¦
    if PERPLEXITY_API_KEY:
        print(f"[INIT] ç‚ºæ–°å°ˆé¡Œã€Œ{name}ã€ç”Ÿæˆ AI æ‘˜è¦...")
        summary_text = generate_topic_summary(tid)
        DATA_STORE['summaries'][tid] = {
            'text': summary_text,
            'updated_at': datetime.now(TAIPEI_TZ).isoformat()
        }

    return jsonify({'status': 'ok'})

@app.route('/api/admin/topics/<tid>', methods=['PUT'])
def update_topic(tid):
    if tid not in TOPICS:
        return jsonify({'error': 'Not found'}), 404
    data = request.json
    if 'keywords' in data:
        TOPICS[tid]['keywords'] = data['keywords']
    if 'negative_keywords' in data:
        TOPICS[tid]['negative_keywords'] = data['negative_keywords']
    save_topics_config()
    update_topic_news()
    return jsonify({'status': 'ok'})

@app.route('/api/admin/topics/<tid>', methods=['DELETE'])
def delete_topic(tid):
    if tid in TOPICS:
        del TOPICS[tid]
        save_topics_config()
    return jsonify({'status': 'ok'})

@app.route('/api/admin/topics/reorder', methods=['PUT'])
def reorder_topics():
    """æ›´æ–°å°ˆé¡Œæ’åº"""
    data = request.json
    order_list = data.get('order', [])

    print(f"[REORDER] æ”¶åˆ°æ’åºè«‹æ±‚: {order_list}")

    # æ›´æ–°æ¯å€‹å°ˆé¡Œçš„ order æ¬„ä½
    for item in order_list:
        tid = item.get('id')
        order = item.get('order')
        if tid in TOPICS:
            TOPICS[tid]['order'] = order
            print(f"[REORDER] æ›´æ–° {TOPICS[tid]['name']} çš„é †åºç‚º {order}")

    save_topics_config()
    print("[REORDER] é †åºå·²å„²å­˜åˆ° topics_config.json")
    return jsonify({'status': 'ok'})

# ============ Main ============

def init_scheduler():
    scheduler = BackgroundScheduler(timezone='Asia/Taipei')
    # 30åˆ†é˜æ›´æ–°ä¸€æ¬¡æ–°è
    scheduler.add_job(update_topic_news, 'interval', minutes=30)
    # AI æ‘˜è¦ï¼šæ¯å¤© 8:00 å’Œ 18:00 åŸ·è¡Œ
    scheduler.add_job(update_all_summaries, 'cron', hour=8, minute=0)
    scheduler.add_job(update_all_summaries, 'cron', hour=18, minute=0)
    scheduler.start()
    print("[SCHEDULER] æ’ç¨‹å·²å•Ÿå‹• - æ–°èæ¯30åˆ†é˜, æ‘˜è¦æ¯å¤©08:00/18:00")

import urllib3
urllib3.disable_warnings()

# ============ æ¨¡çµ„è¼‰å…¥æ™‚åˆå§‹åŒ–ï¼ˆGunicorn éœ€è¦ï¼‰============
load_topics_config()
load_data_cache()  # å…ˆå¾å¿«å–è¼‰å…¥è³‡æ–™ï¼ˆå¿«é€Ÿå•Ÿå‹•ï¼‰
init_scheduler()

if __name__ == '__main__':
    import threading
    import sys

    # åœ¨èƒŒæ™¯ç·šç¨‹åŸ·è¡Œåˆå§‹åŒ–è³‡æ–™
    def background_init():
        print("[INIT] èƒŒæ™¯æ›´æ–°è³‡æ–™...", flush=True)
        sys.stdout.flush()
        update_topic_news()
        if PERPLEXITY_API_KEY:
            print("[INIT] ç”Ÿæˆ AI æ‘˜è¦...", flush=True)
            sys.stdout.flush()
            update_all_summaries()
        print("[INIT] èƒŒæ™¯æ›´æ–°å®Œæˆ", flush=True)
        sys.stdout.flush()

    # å•Ÿå‹•èƒŒæ™¯ç·šç¨‹
    init_thread = threading.Thread(target=background_init, daemon=True)
    init_thread.start()

    print("[SERVER] ä¼ºæœå™¨å•Ÿå‹•ä¸­... (å·²è¼‰å…¥å¿«å–è³‡æ–™ï¼Œæ–°è³‡æ–™å°‡åœ¨èƒŒæ™¯æ›´æ–°)")
    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)

