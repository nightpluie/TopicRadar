# å°ˆé¡Œé›·é” - Topic Radar
# Python å¾Œç«¯ï¼šRSS æŠ“å– + é—œéµå­—éæ¿¾ + AI æ‘˜è¦ (Perplexity) + AI é—œéµå­— (Claude)

import os
import re
import json
import time
import hashlib
import feedparser
import requests
import threading
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
from flask import Flask, jsonify, request, make_response
from flask_cors import CORS
from apscheduler.schedulers.background import BackgroundScheduler
from dotenv import load_dotenv

# å°åŒ—æ™‚å€
TAIPEI_TZ = ZoneInfo('Asia/Taipei')

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = Flask(__name__, static_folder='.', static_url_path='')
CORS(app)

# ============ è¨­å®š ============

# API Keys
PERPLEXITY_API_KEY = os.getenv('PERPLEXITY_API_KEY', '')
PERPLEXITY_MODEL = os.getenv('PERPLEXITY_MODEL', 'sonar')
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY', '')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '')

# å°ˆé¡Œè¨­å®šå„²å­˜æª”æ¡ˆ
TOPICS_FILE = 'topics_config.json'

# å°ç£åª’é«” RSS ä¾†æº
RSS_SOURCES_TW = {
    'è¯åˆå ±': 'https://udn.com/rssfeed/news/2/0',
    'è¯åˆå ±è²¡ç¶“': 'https://udn.com/rssfeed/news/2/6645',
    'è‡ªç”±æ™‚å ±': 'https://news.ltn.com.tw/rss/all.xml',
    'è‡ªç”±è²¡ç¶“': 'https://news.ltn.com.tw/rss/business.xml',
    'ETtoday': 'https://feeds.feedburner.com/ettoday/realtime',
    'ETtodayè²¡ç¶“': 'https://feeds.feedburner.com/ettoday/finance',
    'å ±å°è€…': 'https://www.twreporter.org/a/rss2.xml',
    'Google News TW': 'https://news.google.com/rss?hl=zh-TW&gl=TW&ceid=TW:zh-Hant',
    'å…¬è¦–æ–°è': 'https://news.pts.org.tw/xml/newsfeed.xml',
    'é¡é€±åˆŠ': 'https://www.mirrormedia.mg/rss/news.xml',
    # ä¸­å¤®ç¤¾ (11 å€‹é »é“å®Œæ•´æ¶µè“‹)
    'ä¸­å¤®ç¤¾æ”¿æ²»': 'https://feeds.feedburner.com/rsscna/politics',
    'ä¸­å¤®ç¤¾åœ‹éš›': 'https://feeds.feedburner.com/rsscna/intworld',
    'ä¸­å¤®ç¤¾å…©å²¸': 'https://feeds.feedburner.com/rsscna/mainland',
    'ä¸­å¤®ç¤¾ç”¢ç¶“': 'https://feeds.feedburner.com/rsscna/finance',
    'ä¸­å¤®ç¤¾ç§‘æŠ€': 'https://feeds.feedburner.com/rsscna/technology',
    'ä¸­å¤®ç¤¾ç”Ÿæ´»': 'https://feeds.feedburner.com/rsscna/lifehealth',
    'ä¸­å¤®ç¤¾ç¤¾æœƒ': 'https://feeds.feedburner.com/rsscna/social',
    'ä¸­å¤®ç¤¾åœ°æ–¹': 'https://feeds.feedburner.com/rsscna/local',
    'ä¸­å¤®ç¤¾æ–‡åŒ–': 'https://feeds.feedburner.com/rsscna/culture',
    'ä¸­å¤®ç¤¾é‹å‹•': 'https://feeds.feedburner.com/rsscna/sport',
    'ä¸­å¤®ç¤¾å¨›æ¨‚': 'https://feeds.feedburner.com/rsscna/stars',
}

# åœ‹éš›åª’é«” RSS ä¾†æºï¼ˆè‹±æ–‡/æ—¥æ–‡ï¼‰
RSS_SOURCES_INTL = {
    'BBC News': 'https://feeds.bbci.co.uk/news/rss.xml',
    'The Guardian': 'https://www.theguardian.com/world/rss',
    'NHK (æ—¥æ–‡)': 'https://www3.nhk.or.jp/rss/news/cat0.xml',
    'æœæ—¥æ–°è (æ—¥æ–‡)': 'http://rss.asahi.com/rss/asahi/newsheadlines.rdf',
}

# Google News åœ‹éš›ç‰ˆä¾†æº
GOOGLE_NEWS_INTL_REGIONS = {
    'æ—¥æœ¬': {'code': 'JP', 'lang': 'ja'},
    'ç¾åœ‹': {'code': 'US', 'lang': 'en'},
    'æ³•åœ‹': {'code': 'FR', 'lang': 'fr'},
    'éŸ“åœ‹': {'code': 'KR', 'lang': 'ko'},
}

# é è¨­å°ˆé¡Œè¨­å®š
DEFAULT_TOPICS = {
    'migrant_workers': {
        'name': 'æœå‹™æ¥­ç§»å·¥',
        'keywords': ['ç§»å·¥', 'å¤–å‹', 'å‹å‹•éƒ¨', 'ç¼ºå·¥', 'å¤–ç±å‹å·¥', 'ç§»æ°‘ç½²', 'æœå‹™æ¥­', 'é¤é£²æ¥­', 'ä»²ä»‹'],
    },
    'labor_pension': {
        'name': 'å‹ä¿å¹´é‡‘æ”¹é©',
        'keywords': ['å‹ä¿', 'å¹´é‡‘', 'é€€ä¼‘é‡‘', 'å‹å‹•åŸºé‡‘', 'ç²¾ç®—', 'ç ´ç”¢', 'å‹ä¿å±€', 'å‹é€€', 'è€å¹´çµ¦ä»˜'],
    },
    'housing_tax': {
        'name': 'å›¤æˆ¿ç¨…2.0',
        'keywords': ['å›¤æˆ¿ç¨…', 'æˆ¿å±‹ç¨…', 'æŒæœ‰ç¨…', 'æˆ¿åƒ¹', 'ç©ºå±‹', 'å¤šå±‹', 'ç¨…ç‡', 'éè‡ªä½'],
    },
}

# è³‡æ–™å„²å­˜
DATA_STORE = {
    'topics': {},           # æ¯å€‹å°ˆé¡Œçš„å°ç£æ–°èåˆ—è¡¨
    'international': {},    # æ¯å€‹å°ˆé¡Œçš„åœ‹éš›æ–°èåˆ—è¡¨ï¼ˆç¿»è­¯å¾Œï¼‰
    'summaries': {},        # æ¯å€‹å°ˆé¡Œçš„ AI æ‘˜è¦
    'last_update': None,
    'topic_owners': {},     # å°ˆé¡Œæ“æœ‰è€…å°æ‡‰è¡¨ {topic_id: user_id}
}

# è¼‰å…¥é€²åº¦ç‹€æ…‹
LOADING_STATUS = {
    'is_loading': False,
    'current': 0,
    'total': 0,
    'current_topic': '',
    'phase': ''  # 'news' æˆ– 'summary'
}

TOPICS = {}

# è³‡æ–™å¿«å–æª”æ¡ˆè·¯å¾‘
DATA_CACHE_FILE = 'data_cache.json'

# ============ è³‡æ–™å¿«å–ç®¡ç† ============

def save_data_cache():
    """å„²å­˜è³‡æ–™åˆ°å¿«å–ï¼ˆèªè­‰æ¨¡å¼ä¸‹åŒæ­¥åˆ° Supabaseï¼Œå¦å‰‡å­˜æª”æ¡ˆï¼‰"""
    try:
        # åœ¨èªè­‰æ¨¡å¼ä¸‹ï¼ŒåŒæ­¥åˆ° Supabase
        if AUTH_ENABLED:
            reserved_keys = ['topics', 'international', 'summaries', 'last_update', 'topic_owners']
            active_users = [k for k in DATA_STORE.keys() if k not in reserved_keys]
            
            for uid in active_users:
                user_content = DATA_STORE[uid]
                
                # æ”¶é›†æ‰€æœ‰æ¶‰åŠçš„ topic_id
                tids = set()
                if 'topics' in user_content: tids.update(user_content['topics'].keys())
                if 'international' in user_content: tids.update(user_content['international'].keys())
                if 'summaries' in user_content: tids.update(user_content['summaries'].keys())
                
                for tid in tids:
                    dom = user_content.get('topics', {}).get(tid, [])
                    intl = user_content.get('international', {}).get(tid, [])
                    summ = user_content.get('summaries', {}).get(tid, {})
                    
                    auth.save_topic_cache_item(uid, tid, dom, intl, summ)
            
            # print(f"[SYNC] å·²åŒæ­¥ {len(active_users)} ä½ä½¿ç”¨è€…çš„å¿«å–åˆ°è³‡æ–™åº«")
            return

        # ================= èˆŠç‰ˆæª”æ¡ˆå„²å­˜é‚è¼¯ (Legacy) =================
        # éèªè­‰æ¨¡å¼ï¼šä½¿ç”¨èˆŠæ ¼å¼ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
        cache_data = {
            'topics': {},
            'international': {},
            'summaries': DATA_STORE['summaries'],
            'last_update': DATA_STORE['last_update']
        }

        for tid, news_list in DATA_STORE['topics'].items():
            cache_data['topics'][tid] = []
            for news in news_list:
                news_copy = news.copy()
                if 'published' in news_copy and isinstance(news_copy['published'], datetime):
                    news_copy['published'] = news_copy['published'].isoformat()
                cache_data['topics'][tid].append(news_copy)

        for tid, news_list in DATA_STORE['international'].items():
            cache_data['international'][tid] = []
            for news in news_list:
                news_copy = news.copy()
                if 'published' in news_copy and isinstance(news_copy['published'], datetime):
                    news_copy['published'] = news_copy['published'].isoformat()
                cache_data['international'][tid].append(news_copy)

        with open(DATA_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

        print(f"[CACHE] è³‡æ–™å·²å„²å­˜åˆ° {DATA_CACHE_FILE}")

    except Exception as e:
        print(f"[CACHE] å„²å­˜å¤±æ•—: {e}")

def load_data_cache():
    """å¾å¿«å–æª”æ¡ˆè¼‰å…¥è³‡æ–™ï¼ˆæ”¯æ´æ–°èˆŠæ ¼å¼ï¼‰"""
    global DATA_STORE

    if not os.path.exists(DATA_CACHE_FILE):
        print(f"[CACHE] å¿«å–æª”æ¡ˆä¸å­˜åœ¨ï¼Œå°‡ä½¿ç”¨ç©ºè³‡æ–™")
        return

    try:
        with open(DATA_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)

        # æª¢æŸ¥å¿«å–ç‰ˆæœ¬
        cache_version = cache_data.get('version', '1.0')

        if cache_version == '2.0':
            # æ–°æ ¼å¼ï¼šå¤šä½¿ç”¨è€…åˆ†çµ„
            print(f"[CACHE] è¼‰å…¥å¤šä½¿ç”¨è€…å¿«å–ï¼ˆv2.0ï¼‰...")

            # è¼‰å…¥å°ˆé¡Œæ“æœ‰è€…å°æ‡‰è¡¨
            DATA_STORE['topic_owners'] = cache_data.get('topic_owners', {})

            # åˆå§‹åŒ–
            DATA_STORE['topics'] = {}
            DATA_STORE['international'] = {}
            DATA_STORE['summaries'] = {}
            DATA_STORE['last_update'] = None

            # åˆä½µæ‰€æœ‰ä½¿ç”¨è€…çš„è³‡æ–™
            user_data = cache_data.get('users', {})
            for user_id, user_cache in user_data.items():
                # åˆå§‹åŒ–ä½¿ç”¨è€…è³‡æ–™çµæ§‹
                DATA_STORE[user_id] = {
                    'topics': {},
                    'international': {},
                    'summaries': {},
                    'last_update': user_cache.get('last_update', None)
                }

                # è¼‰å…¥å°ç£æ–°è
                for tid, news_list in user_cache.get('topics', {}).items():
                    # å…¨åŸŸè³‡æ–™
                    DATA_STORE['topics'][tid] = []
                    # ä½¿ç”¨è€…è³‡æ–™
                    DATA_STORE[user_id]['topics'][tid] = []
                    
                    for news in news_list:
                        news_copy = news.copy()
                        if 'published' in news_copy and isinstance(news_copy['published'], str):
                            news_copy['published'] = datetime.fromisoformat(news_copy['published'])
                        
                        # å¯«å…¥å…¨åŸŸ
                        DATA_STORE['topics'][tid].append(news_copy)
                        # å¯«å…¥ä½¿ç”¨è€…å°ˆå±¬
                        DATA_STORE[user_id]['topics'][tid].append(news_copy)

                # è¼‰å…¥åœ‹éš›æ–°è
                for tid, news_list in user_cache.get('international', {}).items():
                    # å…¨åŸŸè³‡æ–™
                    DATA_STORE['international'][tid] = []
                    # ä½¿ç”¨è€…è³‡æ–™
                    DATA_STORE[user_id]['international'][tid] = []

                    for news in news_list:
                        news_copy = news.copy()
                        if 'published' in news_copy and isinstance(news_copy['published'], str):
                            news_copy['published'] = datetime.fromisoformat(news_copy['published'])
                        
                        # å¯«å…¥å…¨åŸŸ
                        DATA_STORE['international'][tid].append(news_copy)
                        # å¯«å…¥ä½¿ç”¨è€…å°ˆå±¬
                        DATA_STORE[user_id]['international'][tid].append(news_copy)

                # è¼‰å…¥æ‘˜è¦
                DATA_STORE[user_id]['summaries'] = user_cache.get('summaries', {}).copy()
                for tid, summary in user_cache.get('summaries', {}).items():
                    # å…¨åŸŸè³‡æ–™
                    DATA_STORE['summaries'][tid] = summary

                # æ›´æ–°æœ€å¾Œæ›´æ–°æ™‚é–“ï¼ˆä½¿ç”¨æœ€æ–°çš„ï¼‰
                user_last_update = user_cache.get('last_update')
                if user_last_update:
                    if not DATA_STORE['last_update'] or user_last_update > DATA_STORE['last_update']:
                        DATA_STORE['last_update'] = user_last_update

            user_count = len(user_data)
            topic_count = len(DATA_STORE['topics']) # é€™è£¡è¨ˆç®—çš„æ˜¯å”¯ä¸€å°ˆé¡Œæ•¸
            print(f"[CACHE] å¾å¿«å–è¼‰å…¥äº† {user_count} å€‹ä½¿ç”¨è€…çš„ {topic_count} å€‹å°ˆé¡Œè³‡æ–™åˆ°è¨˜æ†¶é«”")

        else:
            # èˆŠæ ¼å¼ï¼šå‘å¾Œç›¸å®¹
            print(f"[CACHE] è¼‰å…¥èˆŠæ ¼å¼å¿«å–ï¼ˆv1.0ï¼‰...")

            # è¼‰å…¥æ‘˜è¦å’Œæœ€å¾Œæ›´æ–°æ™‚é–“
            DATA_STORE['summaries'] = cache_data.get('summaries', {})
            DATA_STORE['last_update'] = cache_data.get('last_update')
            DATA_STORE['topic_owners'] = {}  # èˆŠæ ¼å¼æ²’æœ‰æ“æœ‰è€…è³‡è¨Š

            # è¼‰å…¥æ–°èè³‡æ–™ï¼ˆå°‡å­—ä¸²è½‰å› datetimeï¼‰
            DATA_STORE['topics'] = {}
            for tid, news_list in cache_data.get('topics', {}).items():
                DATA_STORE['topics'][tid] = []
                for news in news_list:
                    news_copy = news.copy()
                    if 'published' in news_copy and isinstance(news_copy['published'], str):
                        news_copy['published'] = datetime.fromisoformat(news_copy['published'])
                    DATA_STORE['topics'][tid].append(news_copy)

            DATA_STORE['international'] = {}
            for tid, news_list in cache_data.get('international', {}).items():
                DATA_STORE['international'][tid] = []
                for news in news_list:
                    news_copy = news.copy()
                    if 'published' in news_copy and isinstance(news_copy['published'], str):
                        news_copy['published'] = datetime.fromisoformat(news_copy['published'])
                    DATA_STORE['international'][tid].append(news_copy)

            print(f"[CACHE] å¾å¿«å–è¼‰å…¥äº† {len(DATA_STORE['topics'])} å€‹å°ˆé¡Œçš„è³‡æ–™")

    except Exception as e:
        print(f"[CACHE] è¼‰å…¥å¿«å–å¤±æ•—: {e}")

# ============ å°ˆé¡Œè¨­å®šç®¡ç† ============

def load_topics_config():
    """å¾æª”æ¡ˆè¼‰å…¥å°ˆé¡Œè¨­å®š"""
    global TOPICS
    try:
        if os.path.exists(TOPICS_FILE):
            with open(TOPICS_FILE, 'r', encoding='utf-8') as f:
                TOPICS = json.load(f)
        else:
            TOPICS = DEFAULT_TOPICS.copy()
            save_topics_config()
    except Exception as e:
        print(f"[ERROR] è¼‰å…¥å°ˆé¡Œè¨­å®šå¤±æ•—: {e}")
        TOPICS = DEFAULT_TOPICS.copy()

def save_topics_config():
    """å„²å­˜å°ˆé¡Œè¨­å®šåˆ°æª”æ¡ˆ"""
    try:
        with open(TOPICS_FILE, 'w', encoding='utf-8') as f:
            json.dump(TOPICS, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[ERROR] å„²å­˜å°ˆé¡Œè¨­å®šå¤±æ•—: {e}")

def generate_topic_id(name):
    """æ ¹æ“šåç¨±ç”Ÿæˆå”¯ä¸€ ID"""
    timestamp = int(time.time() * 1000) % 100000
    prefix = re.sub(r'[^\w]', '', name)[:10]
    return f"{prefix}_{timestamp}"

def normalize_keywords(raw_keywords):
    """å°‡é—œéµå­—æ ¼å¼çµ±ä¸€ç‚º dictï¼ˆzh/en/ja/koï¼‰"""
    if isinstance(raw_keywords, dict):
        return {
            'zh': raw_keywords.get('zh', []) or [],
            'en': raw_keywords.get('en', []) or [],
            'ja': raw_keywords.get('ja', []) or [],
            'ko': raw_keywords.get('ko', []) or []
        }
    if isinstance(raw_keywords, list):
        return {'zh': raw_keywords, 'en': [], 'ja': [], 'ko': []}
    if raw_keywords:
        return {'zh': [raw_keywords], 'en': [], 'ja': [], 'ko': []}
    return {'zh': [], 'en': [], 'ja': [], 'ko': []}

# ============ AI é—œéµå­—ç”Ÿæˆ (Gemini) ============

def generate_keywords_with_ai(topic_name):
    """ä½¿ç”¨ Gemini Flash ç”Ÿæˆè­°é¡Œç›¸é—œé—œéµå­—ï¼ˆä¸­è‹±æ—¥éŸ“å››èªï¼‰"""
    if not GEMINI_API_KEY:
        print("[WARN] ç„¡ Gemini API Keyï¼Œä½¿ç”¨é è¨­é—œéµå­—")
        return {
            'zh': [topic_name],
            'en': [],
            'ja': [],
            'ko': []
        }

    try:
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
        headers = {
            "Content-Type": "application/json"
        }
        params = {
            "key": GEMINI_API_KEY
        }

        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–°èè³‡æ–™åº«ç®¡ç†å“¡ã€‚è«‹é‡å°ã€Œ{topic_name}ã€é€™å€‹æ–°èè­°é¡Œï¼Œåˆ—å‡ºæœå°‹é—œéµå­—ã€‚

è¦æ±‚ï¼š
1. ç¹é«”ä¸­æ–‡é—œéµå­—ï¼š10-15 å€‹ï¼ˆæ ¸å¿ƒè©å½™ã€ç›¸é—œå–®ä½ã€åŒç¾©è©ï¼‰
2. è‹±æ–‡é—œéµå­—ï¼š8-10 å€‹ï¼ˆå°æ‡‰çš„è‹±æ–‡è©å½™ï¼Œç”¨æ–¼æœå°‹åœ‹éš›æ–°èï¼‰
3. æ—¥æ–‡é—œéµå­—ï¼š8-10 å€‹ï¼ˆå°æ‡‰çš„æ—¥æ–‡è©å½™ï¼Œç”¨æ–¼æœå°‹æ—¥æœ¬æ–°èï¼‰
4. éŸ“æ–‡é—œéµå­—ï¼š8-10 å€‹ï¼ˆå°æ‡‰çš„éŸ“æ–‡è©å½™ï¼Œç”¨æ–¼æœå°‹éŸ“åœ‹æ–°èï¼‰

æ ¼å¼ï¼ˆè«‹åš´æ ¼éµå®ˆï¼‰ï¼š
ZH: é—œéµå­—1, é—œéµå­—2, é—œéµå­—3
EN: keyword1, keyword2, keyword3
JA: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3
KO: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3

ç›´æ¥è¼¸å‡ºï¼Œä¸è¦æœ‰å…¶ä»–é–‹å ´ç™½æˆ–è§£é‡‹ã€‚"""

        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 1000
            }
        }

        response = requests.post(url, headers=headers, params=params, json=payload, timeout=30)
        response.raise_for_status()

        data = response.json()
        content = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')

        # è§£æå››èªé—œéµå­—
        keywords = {'zh': [], 'en': [], 'ja': [], 'ko': []}
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('ZH:'):
                keywords['zh'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('EN:'):
                keywords['en'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('JA:'):
                keywords['ja'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('KO:'):
                keywords['ko'] = [kw.strip() for kw in line[3:].split(',') if kw.strip()]

        # ç¢ºä¿è‡³å°‘æœ‰åŸºæœ¬é—œéµå­—
        if not keywords['zh']:
            keywords['zh'] = [topic_name]

        print(f"[AI] Gemini ç‚ºã€Œ{topic_name}ã€ç”Ÿæˆäº†é—œéµå­—: ZH={len(keywords['zh'])}, EN={len(keywords['en'])}, JA={len(keywords['ja'])}, KO={len(keywords['ko'])}")
        return keywords

    except Exception as e:
        print(f"[ERROR] Gemini é—œéµå­—ç”Ÿæˆå¤±æ•—: {e}")
        return {
            'zh': [topic_name],
            'en': [],
            'ja': [],
            'ko': []
        }

# ============ Gemini Flash ç¿»è­¯ ============

def translate_with_gemini(text, source_lang='auto', target_lang='zh-TW', max_retries=3):
    """ä½¿ç”¨ Gemini Flash ç¿»è­¯æ–‡å­—åˆ°æŒ‡å®šèªè¨€
    
    Args:
        text: è¦ç¿»è­¯çš„æ–‡å­—
        source_lang: ä¾†æºèªè¨€ï¼ˆé è¨­ autoï¼‰
        target_lang: ç›®æ¨™èªè¨€ï¼Œæ”¯æ´ï¼š'zh-TW'ï¼ˆç¹é«”ä¸­æ–‡ï¼‰, 'en'ï¼ˆè‹±æ–‡ï¼‰, 'ja'ï¼ˆæ—¥æ–‡ï¼‰, 'ko'ï¼ˆéŸ“æ–‡ï¼‰
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
    """
    if not GEMINI_API_KEY:
        return f"[æœªç¿»è­¯] {text}"

    # èªè¨€åç¨±å°æ‡‰
    lang_names = {
        'zh-TW': 'ç¹é«”ä¸­æ–‡',
        'en': 'English',
        'ja': 'æ—¥æœ¬èª',
        'ko': 'í•œêµ­ì–´'
    }
    
    target_lang_name = lang_names.get(target_lang, 'ç¹é«”ä¸­æ–‡')

    for attempt in range(max_retries):
        try:
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
            headers = {
                "Content-Type": "application/json"
            }

            params = {
                "key": GEMINI_API_KEY
            }

            payload = {
                "contents": [{
                    "parts": [{
                        "text": f"è«‹å°‡ä»¥ä¸‹æ–‡å­—ç¿»è­¯æˆ{target_lang_name}ï¼Œåªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–èªªæ˜ï¼š\n\n{text}"
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 200
                }
            }

            response = requests.post(url, headers=headers, params=params, json=payload, timeout=15)
            
            # å¦‚æœæ˜¯ 429 Too Many Requestsï¼Œç­‰å¾…å¾Œé‡è©¦
            if response.status_code == 429:
                wait_time = (attempt + 1) * 2  # 2, 4, 6 ç§’
                print(f"[WARN] Gemini API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
                continue
            
            response.raise_for_status()

            data = response.json()
            translated = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '').strip()

            return translated if translated else f"[ç¿»è­¯å¤±æ•—] {text}"

        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep((attempt + 1) * 2)
                continue
            print(f"[ERROR] Gemini ç¿»è­¯å¤±æ•—: {e}")
            return f"[ç¿»è­¯å¤±æ•—] {text}"
    
    return f"[ç¿»è­¯å¤±æ•—] {text}"


def auto_translate_keywords(chinese_keywords):
    """è‡ªå‹•å°‡ä¸­æ–‡é—œéµå­—ç¿»è­¯æˆè‹±æ—¥éŸ“ä¸‰èªï¼ˆä¸€æ¬¡ API è«‹æ±‚å®Œæˆï¼‰
    
    Args:
        chinese_keywords: ä¸­æ–‡é—œéµå­—åˆ—è¡¨
        
    Returns:
        åŒ…å«å››èªé—œéµå­—çš„å­—å…¸ {'zh': [...], 'en': [...], 'ja': [...], 'ko': [...]}
    """
    if not isinstance(chinese_keywords, list) or not chinese_keywords:
        return {'zh': [], 'en': [], 'ja': [], 'ko': []}
    
    if not GEMINI_API_KEY:
        print("[WARN] ç„¡ Gemini API Keyï¼Œè·³éè‡ªå‹•ç¿»è­¯")
        return {'zh': chinese_keywords, 'en': [], 'ja': [], 'ko': []}
    
    # åˆä½µä¸­æ–‡é—œéµå­—ä½œç‚ºç¿»è­¯æº
    source_text = ', '.join(chinese_keywords)
    
    try:
        # ä½¿ç”¨ä¸€æ¬¡ API è«‹æ±‚åŒæ™‚ç¿»è­¯æˆä¸‰èª
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
        headers = {"Content-Type": "application/json"}
        params = {"key": GEMINI_API_KEY}

        prompt = f"""è«‹å°‡ä»¥ä¸‹ä¸­æ–‡é—œéµå­—ç¿»è­¯æˆè‹±æ–‡ã€æ—¥æ–‡ã€éŸ“æ–‡ã€‚

ä¸­æ–‡é—œéµå­—ï¼š{source_text}

æ ¼å¼è¦æ±‚ï¼ˆåš´æ ¼éµå®ˆï¼‰ï¼š
EN: keyword1, keyword2, keyword3
JA: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3
KO: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3

ç›´æ¥è¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦æœ‰å…¶ä»–èªªæ˜ã€‚"""

        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.1,
                "maxOutputTokens": 300
            }
        }

        response = requests.post(url, headers=headers, params=params, json=payload, timeout=20)
        response.raise_for_status()

        data = response.json()
        content = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')

        # è§£æä¸‰èªç¿»è­¯çµæœ
        en_keywords = []
        ja_keywords = []
        ko_keywords = []
        
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('EN:'):
                en_keywords = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('JA:'):
                ja_keywords = [kw.strip() for kw in line[3:].split(',') if kw.strip()]
            elif line.startswith('KO:'):
                ko_keywords = [kw.strip() for kw in line[3:].split(',') if kw.strip()]

        print(f"[AUTO-TRANSLATE] å·²ç¿»è­¯é—œéµå­—: EN={len(en_keywords)}, JA={len(ja_keywords)}, KO={len(ko_keywords)}")
        
        return {
            'zh': chinese_keywords,
            'en': en_keywords,
            'ja': ja_keywords,
            'ko': ko_keywords
        }
    except Exception as e:
        print(f"[ERROR] è‡ªå‹•ç¿»è­¯é—œéµå­—å¤±æ•—: {e}")
        return {
            'zh': chinese_keywords,
            'en': [],
            'ja': [],
            'ko': []
        }


# ============ Perplexity AI æ‘˜è¦ ============

def generate_topic_summary(topic_id, topic_name=None, user_id=None):
    """ä½¿ç”¨ Perplexity AI ç”Ÿæˆå°ˆé¡Œæ‘˜è¦"""
    if not PERPLEXITY_API_KEY:
        return "ï¼ˆå°šæœªè¨­å®š Perplexity API Keyï¼‰"
    
    # 1. æ±ºå®šå°ˆé¡Œåç¨±
    if not topic_name:
        topic_config = TOPICS.get(topic_id)
        if topic_config:
            topic_name = topic_config['name']
        else:
            return "ï¼ˆæœªçŸ¥å°ˆé¡Œï¼‰"
    
    try:
        url = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # 2. æ±ºå®šåƒè€ƒæ–°èä¾†æºï¼ˆæ”¯æ´ä½¿ç”¨è€…å°ˆå±¬è³‡æ–™ï¼‰
        news_source = []
        if user_id and user_id in DATA_STORE:
            # å„ªå…ˆå¾ä½¿ç”¨è€…è³‡æ–™ä¸­å°‹æ‰¾
            news_source = DATA_STORE[user_id].get('topics', {}).get(topic_id, [])
        elif topic_id in DATA_STORE['topics']:
            # å›é€€åˆ°å…¨åŸŸè³‡æ–™
            news_source = DATA_STORE['topics'][topic_id]

        if news_source:
            recent_titles = [f"- {n['title']} ({n['published'].strftime('%Y/%m/%d') if isinstance(n['published'], datetime) else ''})" 
                           for n in news_source[:5]]
            context = "\n".join(recent_titles)
        else:
            context = "ï¼ˆæš«ç„¡ç›¸é—œ RSS æ–°èï¼‰"

        current_time = datetime.now(TAIPEI_TZ).strftime('%Y/%m/%d')
        
        payload = {
            "model": PERPLEXITY_MODEL,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½è³‡æ·±å°ˆé¡Œè¨˜è€…ï¼Œæ­£åœ¨ç‚ºå·²ç¶“ç†Ÿæ‚‰è­°é¡ŒèƒŒæ™¯çš„åŒäº‹æ›´æ–°æœ€æ–°é€²å±•ã€‚å‡è¨­è®€è€…å·²äº†è§£è­°é¡ŒèƒŒæ™¯ï¼Œä¸éœ€è¦é‡è¤‡èªªæ˜åŸºæœ¬æ¦‚å¿µæˆ–æ­·å²è„ˆçµ¡ã€‚ç›´æ¥åˆ‡å…¥æœ€æ–°å‹•æ…‹å’Œè®ŠåŒ–ï¼Œä»¥åŠä½ åˆ¤æ–·æ¥ä¸‹ä¾†å€¼å¾—é—œæ³¨çš„æ–¹å‘ã€‚"
                },
                {
                    "role": "user",
                    "content": f"è­°é¡Œï¼š{topic_name}\næ—¥æœŸï¼š{current_time}\n\nåƒè€ƒæ–°èæ¨™é¡Œï¼ˆè«‹åƒè€ƒé€™äº›å…§å®¹ï¼‰ï¼š\n{context}\n\nè«‹ç”¨ç´”æ–‡å­—æ ¼å¼ï¼ˆä¸è¦markdownï¼‰è¼¸å‡ºæœ€æ–°é€²å±•æ‘˜è¦ï¼š\n\nå…§å®¹è¦æ±‚ï¼š\n1. æœ¬é€±æˆ–è¿‘æœŸæœ‰ä»€éº¼æ–°å‹•æ…‹ï¼Ÿï¼ˆæ”¿ç­–ç™¼å¸ƒã€å”å•†é€²å±•ã€é‡è¦äº‹ä»¶ã€çˆ­è­°ï¼‰\n2. ç›®å‰æ¨é€²åˆ°ä»€éº¼éšæ®µï¼Ÿæœ‰ä»€éº¼é—œéµé€²å±•æˆ–è½‰æŠ˜ï¼Ÿæ¥ä¸‹ä¾†é—œæ³¨é‡é»æ˜¯ä»€éº¼ï¼Ÿ\n3. ç¸½å…±200å­—ä»¥å…§ï¼Œç¹é«”ä¸­æ–‡\n4. å¦‚æœæœ‰2-3å€‹é‡é»ï¼Œæ¯å€‹é‡é»è‡ªæˆä¸€å¥ï¼Œç”¨å¥è™Ÿçµå°¾å³å¯\n\næ ¼å¼è¦å‰‡ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š\n- ç¬¬ä¸€å€‹å­—ç›´æ¥é–‹å§‹å¯«å…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•ç©ºè¡Œã€ç©ºæ ¼æˆ–å‰ç¶´\n- ä¸è¦ä»»ä½•æ¨™é¡Œï¼ˆå¦‚ã€Œæœ€æ–°å‹•æ…‹ã€ã€Œé€²åº¦å ±å‘Šã€ç­‰ï¼‰\n- ä¸è¦å¼•ç”¨æ¨™è¨˜ [1][2]\n- ä¸è¦markdownç¬¦è™Ÿï¼ˆ#ã€**ã€*ã€-ï¼‰\n- ä¸è¦åœ¨çµå°¾æ¨™è¨»å­—æ•¸\n- ä¸è¦ç©ºè¡Œåˆ†æ®µï¼Œæ‰€æœ‰å…§å®¹é€£çºŒæ›¸å¯«\n- æ¯å€‹é‡é»ç”¨å¥è™Ÿçµå°¾ï¼Œç„¶å¾Œç›´æ¥æ¥ä¸‹ä¸€å€‹é‡é»\n\nç¯„ä¾‹æ ¼å¼ï¼ˆæ³¨æ„æ²’æœ‰ç©ºè¡Œï¼‰ï¼š\nå‹ä¿å¹´é‡‘æ”¹é©è‰æ¡ˆå·²æ–¼2026å¹´1æœˆæ­£å¼å•Ÿå‹•ï¼Œé è¨ˆæœ€ä½æŠ•ä¿è–ªè³‡èª¿å‡è‡³29,500å…ƒã€‚æ³•æ¡ˆå¯©è­°é è¨ˆåœ¨2026å¹´3æœˆå®Œæˆåˆå¯©ï¼Œä½†è—ç¶ å°æ–¼å¹´é½¡ç´šè·ä»å­˜åœ¨åˆ†æ­§ã€‚æ¥ä¸‹ä¾†éœ€é—œæ³¨ç«‹æ³•é™¢å¯©è­°é€²åº¦ï¼Œåå¹´é‡‘æ”¹é©åœ˜é«”æ–½å£“æƒ…å½¢ã€‚"
                }
            ],
            "max_tokens": 400,
            "temperature": 0.2
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=45)
        response.raise_for_status()
        
        data = response.json()
        content = data.get('choices', [{}])[0].get('message', {}).get('content', '')

        # ç§»é™¤å¯èƒ½çš„å¼•ç”¨æ¨™è¨˜ [1], [2] ç­‰
        content = re.sub(r'\[\d+\]', '', content)

        # ç§»é™¤ markdown æ ¼å¼ç¬¦è™Ÿï¼ˆ#ã€**ã€*ã€###ç­‰ï¼‰
        content = re.sub(r'^#{1,6}\s*', '', content, flags=re.MULTILINE)  # ç§»é™¤æ¨™é¡Œç¬¦è™Ÿ
        content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)  # ç§»é™¤ç²—é«” **text**
        content = re.sub(r'\*([^*]+)\*', r'\1', content)  # ç§»é™¤æ–œé«” *text*
        content = re.sub(r'^[-*]\s+', '', content, flags=re.MULTILINE)  # ç§»é™¤åˆ—è¡¨ç¬¦è™Ÿ

        # ç§»é™¤é–‹é ­å¯èƒ½å‡ºç¾çš„æ¨™é¡Œï¼ˆå¦‚ã€Œé€²åº¦å ±å‘Šï¼šã€ã€Œæœ€æ–°å‹•æ…‹ï¼šã€ç­‰ï¼‰
        content = re.sub(r'^[^ï¼š]*é€²åº¦å ±å‘Š[ï¼š:]\s*', '', content)
        content = re.sub(r'^[^ï¼š]*æœ€æ–°å‹•æ…‹[ï¼š:]\s*', '', content)
        content = re.sub(r'^[^ï¼š]*æ‘˜è¦[ï¼š:]\s*', '', content)

        # ç§»é™¤çµå°¾çš„å­—æ•¸æ¨™è¨˜ï¼ˆå¦‚ã€Œï¼ˆ200å­—ï¼‰ã€ã€Œ(200å­—)ã€ç­‰ï¼‰
        content = re.sub(r'[ï¼ˆ(]\s*\d+\s*å­—\s*[ï¼‰)]$', '', content)

        # ç¬¬ä¸€æ¬¡æ¸…ç†ï¼šç§»é™¤é¦–å°¾ç©ºç™½
        content = content.strip()

        # ç¬¬äºŒæ¬¡æ¸…ç†ï¼šç§»é™¤é–‹é ­çš„æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€tabã€æ›è¡Œï¼‰
        while content and content[0] in (' ', '\t', '\n', '\r'):
            content = content[1:]

        # ç¬¬ä¸‰æ¬¡æ¸…ç†ï¼šä½¿ç”¨ regex ç§»é™¤é–‹é ­æ‰€æœ‰ç©ºç™½
        content = re.sub(r'^[\s\n\r]+', '', content)

        # ç§»é™¤çµå°¾çš„æ‰€æœ‰é€£çºŒç©ºè¡Œ
        content = re.sub(r'[\s\n\r]+$', '', content)

        # æœ€å¾Œä¸€æ¬¡ strip ç¢ºä¿ä¹¾æ·¨
        content = content.strip()

        return content if content else "ï¼ˆç„¡æ³•ç”Ÿæˆæ‘˜è¦ï¼‰"
    
    except Exception as e:
        print(f"[ERROR] Perplexity æ‘˜è¦å¤±æ•—: {e}")
        return "ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼‰"

# ============ RSS æŠ“å– ============

def fetch_rss(url, source_name, timeout=15, max_items=50):
    """æŠ“å– RSSï¼Œå¢åŠ æœ€å¤§æŠ“å–æ•¸é‡ä»¥ç¢ºä¿èƒ½æ‰¾åˆ°è¶³å¤ çš„ç›¸é—œæ–°è"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=timeout, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        items = []
        # å¢åŠ æŠ“å–æ•¸é‡å¾ 30 åˆ° max_itemsï¼Œç¢ºä¿æœ‰è¶³å¤ æ–°èå¯éæ¿¾
        for entry in feed.entries[:max_items]:
            # ç²å–æ¨™é¡Œï¼Œè·³éç©ºæ¨™é¡Œ
            title = entry.get('title', '').strip()
            if not title:
                continue

            published = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                # RSS æ™‚é–“é€šå¸¸æ˜¯ UTCï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                published = published.astimezone(TAIPEI_TZ)
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                published = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                published = published.astimezone(TAIPEI_TZ)
            else:
                # ä½¿ç”¨å°åŒ—æ™‚é–“çš„ç•¶å‰æ™‚é–“
                published = datetime.now(TAIPEI_TZ)

            items.append({
                'title': title,
                'link': entry.get('link', ''),
                'source': source_name,
                'published': published,
                'summary': entry.get('summary', '')[:200]
            })
        return items
    except Exception as e:
        print(f"[ERROR] æŠ“å– {source_name} å¤±æ•—: {e}")
        return []

def fetch_google_news_by_keywords(keywords, max_items=50):
    """ä½¿ç”¨ Google News æœç´¢ç‰¹å®šé—œéµå­—çš„æ–°èï¼Œä¸¦æå–åŸå§‹åª’é«”ä¾†æº"""
    if not keywords:
        return []

    # ä½¿ç”¨ç¬¬ä¸€å€‹é—œéµå­—ä½œç‚ºæœç´¢è©
    search_term = keywords[0] if isinstance(keywords, list) else keywords
    # Google News æœç´¢ RSS
    url = f"https://news.google.com/rss/search?q={search_term}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"

    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        items = []
        for entry in feed.entries[:max_items]:
            # ç²å–æ¨™é¡Œï¼Œè·³éç©ºæ¨™é¡Œ
            title = entry.get('title', '').strip()
            if not title:
                continue

            # æå–åŸå§‹åª’é«”ä¾†æºï¼ˆGoogle News RSS ç‰¹æœ‰ï¼‰
            source_name = 'Google News'
            if hasattr(entry, 'source') and entry.source:
                source_name = entry.source.get('title', 'Google News')

            # è™•ç†æ™‚é–“
            published = None
            is_date_only = False

            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                published = published.astimezone(TAIPEI_TZ)

                # æª¢æŸ¥æ˜¯å¦æ˜¯æ•´é»æ™‚é–“ï¼ˆå¯èƒ½åªæ˜¯æ—¥æœŸçš„å ä½ç¬¦ï¼‰
                # ä¾‹å¦‚ 08:00:00 å¯èƒ½åªä»£è¡¨ç•¶å¤©ï¼Œä¸æ˜¯çœŸå¯¦æ™‚é–“
                if published.minute == 0 and published.second == 0:
                    is_date_only = True
            else:
                published = datetime.now(TAIPEI_TZ)

            items.append({
                'title': title,
                'link': entry.get('link', ''),
                'source': source_name,  # ä½¿ç”¨åŸå§‹åª’é«”åç¨±
                'published': published,
                'summary': entry.get('summary', '')[:200],
                'is_date_only': is_date_only  # æ¨™è¨˜åƒ…æœ‰æ—¥æœŸ
            })
        return items
    except Exception as e:
        print(f"[ERROR] Google News æœç´¢å¤±æ•—: {e}")
        return []

def fetch_google_news_intl(keywords, region_code, lang, max_items=30):
    """ä½¿ç”¨ Google News åœ‹éš›ç‰ˆæœç´¢ç‰¹å®šåœ‹å®¶çš„æ–°è"""
    if not keywords:
        return []

    # ä½¿ç”¨ç¬¬ä¸€å€‹é—œéµå­—ä½œç‚ºæœç´¢è©
    search_term = keywords[0] if isinstance(keywords, list) else keywords
    # Google News åœ‹éš›ç‰ˆ RSSï¼ˆæ ¹æ“šåœ‹å®¶ä»£ç¢¼å’Œèªè¨€ï¼‰
    url = f"https://news.google.com/rss/search?q={search_term}&hl={lang}&gl={region_code}&ceid={region_code}:{lang}"

    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        items = []
        for entry in feed.entries[:max_items]:
            title = entry.get('title', '').strip()
            if not title:
                continue

            # æå–åŸå§‹åª’é«”ä¾†æº
            source_name = f'Google News ({region_code})'
            if hasattr(entry, 'source') and entry.source:
                source_name = entry.source.get('title', source_name)

            # è™•ç†æ™‚é–“
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                published = published.astimezone(TAIPEI_TZ)
            else:
                published = datetime.now(TAIPEI_TZ)

            items.append({
                'title': title,
                'link': entry.get('link', ''),
                'source': source_name,
                'published': published,
                'summary': entry.get('summary', '')[:200]
            })
        return items
    except Exception as e:
        print(f"[ERROR] Google News {region_code} æœç´¢å¤±æ•—: {e}")
        return []

def keyword_match(text, keywords, negative_keywords=None):
    """
    é—œéµå­—æ¯”å°ï¼Œæ”¯æ´è² é¢é—œéµå­—éæ¿¾

    Args:
        text: è¦æª¢æŸ¥çš„æ–‡å­—
        keywords: æ­£é¢é—œéµå­—åˆ—è¡¨ï¼ˆåŒ¹é…ä»»ä¸€å³å¯ï¼‰
        negative_keywords: è² é¢é—œéµå­—åˆ—è¡¨ï¼ˆåŒ…å«ä»»ä¸€å‰‡æ’é™¤ï¼‰
    """
    if not text or not keywords:
        return False

    text_lower = text.lower()

    # å…ˆæª¢æŸ¥è² é¢é—œéµå­—ï¼Œå¦‚æœåŒ…å«å‰‡ç›´æ¥æ’é™¤
    if negative_keywords:
        for neg_kw in negative_keywords:
            if neg_kw.lower() in text_lower:
                return False

    # å†æª¢æŸ¥æ­£é¢é—œéµå­—
    for kw in keywords:
        if kw.lower() in text_lower:
            return True

    return False

def filter_news_by_keywords(news_list, topic_config, is_international=False):
    """æ ¹æ“šå°ˆé¡Œè¨­å®šéæ¿¾æ–°èåˆ—è¡¨"""
    keywords = topic_config.get('keywords', {})
    negative_keywords = topic_config.get('negative_keywords', [])

    # æå–æ­£é¢é—œéµå­—
    target_keywords = []
    if isinstance(keywords, list):
        target_keywords = keywords
    elif isinstance(keywords, dict):
        if is_international:
            target_keywords = keywords.get('en', []) + keywords.get('ja', []) + keywords.get('ko', [])
        else:
            target_keywords = keywords.get('zh', [])
    
    if not target_keywords:
        return []

    filtered = []
    seen_hashes = set()

    for item in news_list:
        # å…§å®¹çµ„åˆæ¨™é¡Œå’Œæ‘˜è¦ä»¥å¢åŠ åŒ¹é…ç‡
        content = f"{item['title']} {item['summary']}"
        
        if keyword_match(content, target_keywords, negative_keywords):
            # å»é‡
            h = hashlib.md5(item['title'].encode()).hexdigest()
            if h not in seen_hashes:
                seen_hashes.add(h)
                item['hash'] = h
                filtered.append(item)
    
    return filtered

def update_single_topic_news(topic_id):
    """åªæ›´æ–°å–®ä¸€å°ˆé¡Œçš„æ–°èï¼ˆç”¨æ–¼æ–°å¢å°ˆé¡Œæ™‚ï¼‰"""
    if topic_id not in TOPICS:
        return

    cfg = TOPICS[topic_id]
    print(f"\n[UPDATE] æ›´æ–°å–®ä¸€å°ˆé¡Œæ–°è: {cfg['name']}")

    # 1. æŠ“å–å°ç£æ–°è
    all_news_tw = []
    for name, url in RSS_SOURCES_TW.items():
        all_news_tw.extend(fetch_rss(url, name, max_items=50))

    # 2. æŠ“å–åœ‹éš›æ–°è
    all_news_intl = []
    for name, url in RSS_SOURCES_INTL.items():
        all_news_intl.extend(fetch_rss(url, name, max_items=50))

    # 3. æŠ“å–è©²å°ˆé¡Œçš„ Google News åœ‹éš›ç‰ˆ
    keywords = cfg.get('keywords', {})
    if isinstance(keywords, dict):
        keywords_en = keywords.get('en', [])
        keywords_ja = keywords.get('ja', [])
        keywords_ko = keywords.get('ko', [])

        if keywords_ja:
            all_news_intl.extend(fetch_google_news_intl(keywords_ja, 'JP', 'ja', max_items=20))
        if keywords_en:
            all_news_intl.extend(fetch_google_news_intl(keywords_en, 'US', 'en', max_items=20))
            all_news_intl.extend(fetch_google_news_intl(keywords_en, 'FR', 'fr', max_items=20))
        if keywords_ko:
            all_news_intl.extend(fetch_google_news_intl(keywords_ko, 'KR', 'ko', max_items=20))

    # 4. éæ¿¾è©²å°ˆé¡Œçš„æ–°è
    keywords_zh = keywords.get('zh', []) if isinstance(keywords, dict) else keywords
    keywords_en = keywords.get('en', []) if isinstance(keywords, dict) else []
    keywords_ja = keywords.get('ja', []) if isinstance(keywords, dict) else []
    negative_keywords = cfg.get('negative_keywords', [])

    # éæ¿¾å°ç£æ–°è
    filtered_tw = [n for n in all_news_tw if keyword_match(n['title'], keywords_zh, negative_keywords)]
    print(f"[SEARCH] {cfg['name']}: æ‰¾åˆ° {len(filtered_tw)} å‰‡å°ç£æ–°è")

    # Google News è£œå……ï¼ˆå¦‚éœ€è¦ï¼‰
    if len(filtered_tw) < 10:
        print(f"[SEARCH] {cfg['name']}: åªæœ‰ {len(filtered_tw)} å‰‡ï¼Œä½¿ç”¨ Google News æœç´¢è£œå……...")
        google_news = fetch_google_news_by_keywords(keywords_zh, max_items=20)
        for n in google_news:
            if keyword_match(n['title'], keywords_zh, negative_keywords):
                filtered_tw.append(n)
        print(f"[SEARCH] {cfg['name']}: è£œå……å¾Œå…± {len(filtered_tw)} å‰‡æ–°è")

    # æ›´æ–°è©²å°ˆé¡Œçš„å°ç£æ–°è
    existing = DATA_STORE['topics'].get(topic_id, [])
    existing_hashes = {n['hash'] for n in existing}

    new_items = []
    for n in filtered_tw[:10]:
        n_hash = hashlib.md5(n['title'].encode()).hexdigest()
        n['hash'] = n_hash
        if n_hash not in existing_hashes:
            new_items.append(n)

    all_items = new_items + existing
    DATA_STORE['topics'][topic_id] = all_items[:10]

    if new_items:
        print(f"[UPDATE] {cfg['name']}: æ–°å¢ {len(new_items)} å‰‡æ–°èï¼Œç•¶å‰ {len(DATA_STORE['topics'][topic_id])} å‰‡")

    # éæ¿¾åœ‹éš›æ–°è
    intl_keywords = keywords_en + keywords_ja
    filtered_intl = [n for n in all_news_intl if keyword_match(n['title'], intl_keywords, negative_keywords)]

    # ç¿»è­¯åœ‹éš›æ–°è
    for n in filtered_intl:
        if 'title_original' not in n:
            n['title_original'] = n['title']
            translated = translate_with_gemini(n['title'])
            n['title'] = translated if translated else n['title']
            time.sleep(0.5)

    # Google News åœ‹éš›è£œå……
    if len(filtered_intl) < 5:
        for region_name, region_info in GOOGLE_NEWS_INTL_REGIONS.items():
            if len(filtered_intl) >= 5:
                break
            search_keywords = keywords_ja if region_info['lang'] == 'ja' else keywords_en
            google_intl = fetch_google_news_intl(search_keywords, region_info['code'], region_info['lang'], max_items=20)
            for n in google_intl:
                if keyword_match(n['title'], search_keywords, negative_keywords):
                    n['title_original'] = n['title']
                    translated = translate_with_gemini(n['title'])
                    n['title'] = translated if translated else n['title']
                    filtered_intl.append(n)
                    time.sleep(0.5)

    # æ›´æ–°è©²å°ˆé¡Œçš„åœ‹éš›æ–°è
    existing_intl = DATA_STORE['international'].get(topic_id, [])
    existing_intl_hashes = {n['hash'] for n in existing_intl}

    new_intl_items = []
    for n in filtered_intl[:10]:
        n_hash = hashlib.md5(n.get('title_original', n['title']).encode()).hexdigest()
        n['hash'] = n_hash
        if n_hash not in existing_intl_hashes:
            new_intl_items.append(n)

    all_intl_items = new_intl_items + existing_intl
    DATA_STORE['international'][topic_id] = all_intl_items[:10]

    if new_intl_items:
        print(f"[UPDATE] {cfg['name']} (åœ‹éš›): æ–°å¢ {len(new_intl_items)} å‰‡æ–°èï¼Œç•¶å‰ {len(DATA_STORE['international'][topic_id])} å‰‡")

    DATA_STORE['last_update'] = datetime.now(TAIPEI_TZ).isoformat()

    # å„²å­˜åˆ°å¿«å–æª”æ¡ˆ
    save_data_cache()

    print(f"[UPDATE] {cfg['name']} æ›´æ–°å®Œæˆ")

def load_user_data(user_id, check_freshness=False):
    """
    è¼‰å…¥ä½¿ç”¨è€…è³‡æ–™
    check_freshness: True=ç™»å…¥æª¢æŸ¥(5åˆ†é–€æª»), False=ä¸€èˆ¬è¼ªè©¢(60åˆ†é–€æª»)
    """
    global DATA_STORE

    # 1. æª¢æŸ¥æ˜¯å¦æ­£åœ¨è¼‰å…¥ä¸­ï¼Œé¿å…é‡è¤‡è«‹æ±‚ï¼ˆRace Condition Fixï¼‰
    if user_id in DATA_STORE and DATA_STORE[user_id].get('is_loading'):
        # print(f"[LOAD] ä½¿ç”¨è€… {user_id} æ­£åœ¨è¼‰å…¥ä¸­ï¼Œè·³é")
        return True

    should_refresh = False

    # 2. æª¢æŸ¥è¨˜æ†¶é«”å¿«å–æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
    if user_id in DATA_STORE and DATA_STORE[user_id].get('last_update'):
        last_update_str = DATA_STORE[user_id]['last_update']
        
        # è¨­å®šéæœŸé–€æª»
        threshold_seconds = 300 if check_freshness else 3600
        threshold_desc = "5åˆ†é˜" if check_freshness else "60åˆ†é˜"

        if last_update_str:
            try:
                last_update = datetime.fromisoformat(last_update_str)
                # æª¢æŸ¥è³‡æ–™æ˜¯å¦è¶…éé–€æª»
                if (datetime.now(TAIPEI_TZ) - last_update).total_seconds() > threshold_seconds:
                    should_refresh = True
            except ValueError:
                should_refresh = True

        if not should_refresh:
            return True
            
        print(f"[LOAD] ä½¿ç”¨è€… {user_id} è³‡æ–™å·²éæœŸ (>{threshold_desc})ï¼Œè§¸ç™¼èƒŒæ™¯æ›´æ–°...")
    
    # 3. å¦‚æœä¸åœ¨è¨˜æ†¶é«”ä¸­ï¼Œå˜—è©¦å¾è³‡æ–™åº«æ¢å¾©
    elif user_id not in DATA_STORE:
        # é å…ˆä½”ä½ä¸¦æ¨™è¨˜ç‚ºè¼‰å…¥ä¸­ï¼Œé˜²æ­¢ä¸¦ç™¼è«‹æ±‚é‡è¤‡é€²å…¥
        DATA_STORE[user_id] = {
            'topics': {},
            'international': {},
            'summaries': {},
            'last_update': '',
            'is_loading': True,
            'phase': 'news'  # ä½¿ç”¨è€…å°ˆå±¬çš„ phase ç‹€æ…‹
        }

        # å˜—è©¦å¾ Supabase è³‡æ–™åº«è¼‰å…¥å¿«å–
        print(f"[LOAD] å˜—è©¦å¾ Supabase è¼‰å…¥ä½¿ç”¨è€… {user_id} å¿«å–...")
        db_cache = auth.load_user_cache(user_id)
        
        # å€åˆ†ã€Œç„¡å¿«å–ã€({}) å’Œã€Œé€£ç·šå¤±æ•—ã€(None)
        if db_cache is None:
            print(f"[LOAD] è³‡æ–™åº«é€£ç·šå•é¡Œï¼Œç‚ºä½¿ç”¨è€… {user_id} è§¸ç™¼é¦–æ¬¡è¼‰å…¥...")
            # é€£ç·šå¤±æ•—ï¼Œä»å•Ÿå‹• Worker å˜—è©¦æŠ“å–æ–°è³‡æ–™
        elif db_cache:
            # æœ‰å¿«å–è³‡æ–™ï¼Œæ¢å¾©åˆ°è¨˜æ†¶é«”
            loaded_topics = 0
            latest_update_time = ''
            
            for tid, data in db_cache.items():
                DATA_STORE[user_id]['topics'][tid] = data['topics']
                DATA_STORE[user_id]['international'][tid] = data['international']
                DATA_STORE[user_id]['summaries'][tid] = data['summary']
                
                # è¿½è¹¤æœ€æ–°çš„æ›´æ–°æ™‚é–“
                t_updated = data.get('updated_at', '')
                if t_updated and t_updated > latest_update_time:
                    latest_update_time = t_updated
                    
                loaded_topics += 1
            
            if latest_update_time:
                DATA_STORE[user_id]['last_update'] = latest_update_time
                
            print(f"[LOAD] å¾è³‡æ–™åº«æ¢å¾©äº† {loaded_topics} å€‹å°ˆé¡Œçš„è³‡æ–™ (æœ€å¾Œæ›´æ–°: {latest_update_time})")
            
            # æ¢å¾©å®Œæˆï¼Œè§£é™¤è¼‰å…¥é–å®š
            DATA_STORE[user_id]['is_loading'] = False
            
            # éè¿´å‘¼å«è‡ªå·±ï¼Œé€²è¡Œæ–°é®®åº¦æª¢æŸ¥
            return load_user_data(user_id, check_freshness)
        else:
            # {} è¡¨ç¤ºè³‡æ–™åº«ä¸­ç¢ºå¯¦æ²’æœ‰å¿«å–
            print(f"[LOAD] è³‡æ–™åº«ç„¡å¿«å–ï¼Œè§¸ç™¼ä½¿ç”¨è€… {user_id} é¦–æ¬¡è³‡æ–™è¼‰å…¥ (èƒŒæ™¯åŸ·è¡Œ)...")
            # ä¿æŒ is_loading=Trueï¼Œå¾€ä¸‹åŸ·è¡Œä»¥å•Ÿå‹•èƒŒæ™¯åŸ·è¡Œç·’

    # 4. å•Ÿå‹•èƒŒæ™¯åŸ·è¡Œç·’ï¼ˆé©ç”¨æ–¼è³‡æ–™éæœŸæˆ–å…¨æ–°è¼‰å…¥çš„æƒ…æ³ï¼‰
    if user_id in DATA_STORE:
        # ç¢ºä¿æ¨™è¨˜ç‚ºè¼‰å…¥ä¸­
        DATA_STORE[user_id]['is_loading'] = True
        
        thread = threading.Thread(target=_load_user_data_worker, args=(user_id,))
        thread.daemon = True
        thread.start()
    
    return True

def _load_user_data_worker(user_id):
    """èƒŒæ™¯åŸ·è¡Œç·’ï¼šå¯¦éš›åŸ·è¡Œè³‡æ–™æŠ“å–"""
    print(f"[WORKER] é–‹å§‹ç‚ºä½¿ç”¨è€… {user_id} æŠ“å–è³‡æ–™...")
    
    try:
        # å–å¾—ä½¿ç”¨è€…çš„å°ˆé¡Œ
        # æ³¨æ„ï¼šé€™è£¡æ˜¯åœ¨åŸ·è¡Œç·’ä¸­ï¼Œç¢ºä¿ auth æ¨¡çµ„æ˜¯ thread-safe çš„ï¼ˆSupabase client é€šå¸¸æ˜¯ï¼‰
        user_topics = auth.get_user_topics(user_id)

        if not user_topics:
            print(f"[WORKER] ä½¿ç”¨è€… {user_id} æ²’æœ‰å°ˆé¡Œ")
            return

        # è½‰æ›ç‚ºæ›´æ–°æ ¼å¼
        topics_to_load = {}
        for topic in user_topics:
            topics_to_load[topic['id']] = {
                'name': topic['name'],
                'keywords': topic['keywords'],
                'negative_keywords': topic.get('negative_keywords', []),
                'icon': topic.get('icon', 'ğŸ“Œ'),
                'user_id': user_id
            }

        print(f"[WORKER] ç‚ºä½¿ç”¨è€… {user_id} è¼‰å…¥ {len(topics_to_load)} å€‹å°ˆé¡Œçš„æ–°è...")

        # æŠ“å– RSS æ–°è
        all_news_tw = []
        for name, url in RSS_SOURCES_TW.items():
            try:
                all_news_tw.extend(fetch_rss(url, name, max_items=50))
            except Exception as e:
                print(f"[WORKER] RSS æŠ“å–å¤±æ•— ({name}): {e}")

        all_news_intl = []
        for name, url in RSS_SOURCES_INTL.items():
            try:
                all_news_intl.extend(fetch_rss(url, name, max_items=50))
            except Exception as e:
                print(f"[WORKER] RSS æŠ“å–å¤±æ•— ({name}): {e}")

        # ç‚ºæ¯å€‹å°ˆé¡Œéæ¿¾æ–°è
        for tid, cfg in topics_to_load.items():
            # éæ¿¾å°ç£æ–°è
            filtered_tw = filter_news_by_keywords(all_news_tw, cfg)
            
            # å¦‚æœ RSS æ‰¾ä¸åˆ°è¶³å¤ çš„æ–°èï¼ˆå°‘æ–¼ 5 å‰‡ï¼‰ï¼Œå˜—è©¦ç”¨ Google News è£œå……
            if len(filtered_tw) < 5:
                keywords = cfg.get('keywords', {})
                if isinstance(keywords, list):
                    keywords_zh = keywords
                else:
                    keywords_zh = keywords.get('zh', [])
                
                if keywords_zh:
                    print(f"[WORKER] {cfg['name']}: RSS åªæœ‰ {len(filtered_tw)} å‰‡ï¼Œä½¿ç”¨ Google News è£œå……...")
                    try:
                        google_news = fetch_google_news_by_keywords(keywords_zh, max_items=20)
                        
                        # éæ¿¾ä¸¦å»é‡
                        existing_hashes = {hashlib.md5(item['title'].encode()).hexdigest() for item in filtered_tw}
                        negative_keywords = cfg.get('negative_keywords', [])
                        
                        for item in google_news:
                            if len(filtered_tw) >= 10:
                                break
                            content = f"{item['title']} {item['summary']}"
                            if keyword_match(content, keywords_zh, negative_keywords):
                                h = hashlib.md5(item['title'].encode()).hexdigest()
                                if h not in existing_hashes:
                                    existing_hashes.add(h)
                                    filtered_tw.append(item)
                    except Exception as e:
                        print(f"[WORKER] Google News è£œå……å¤±æ•—: {e}")

            # æŒ‰æ™‚é–“æ’åºä¸¦å–å‰ 10 å‰‡
            filtered_tw.sort(key=lambda x: x['published'], reverse=True)
            DATA_STORE[user_id]['topics'][tid] = filtered_tw[:10]

            # éæ¿¾åœ‹éš›æ–°è
            filtered_intl = filter_news_by_keywords(all_news_intl, cfg, is_international=True)
            
            # å¦‚æœ RSS æ‰¾ä¸åˆ°è¶³å¤ çš„åœ‹éš›æ–°èï¼ˆå°‘æ–¼ 5 å‰‡ï¼‰ï¼Œå˜—è©¦ç”¨ Google News è£œå……
            if len(filtered_intl) < 5:
                keywords = cfg.get('keywords', {})
                if isinstance(keywords, dict):
                    keywords_en = keywords.get('en', [])
                    keywords_ja = keywords.get('ja', [])
                    keywords_ko = keywords.get('ko', [])
                    
                    # æ±ºå®šæœå°‹é—œéµå­—
                    search_keywords = keywords_en
                    if keywords_ja: search_keywords = keywords_ja
                    
                    if search_keywords:
                       print(f"[WORKER] {cfg['name']} (åœ‹éš›): RSS åªæœ‰ {len(filtered_intl)} å‰‡ï¼Œä½¿ç”¨ Google News è£œå……...")
                       try:
                           # ç°¡å–®ç­–ç•¥ï¼šä¾æ“šé—œéµå­—èªè¨€é¸æ“‡ä¸€å€‹å€åŸŸè£œå……
                           region = 'US'
                           lang = 'en'
                           if keywords_ja:
                               region = 'JP'
                               lang = 'ja'
                           elif keywords_ko:
                               region = 'KR'
                               lang = 'ko'
                               search_keywords = keywords_ko
                           
                           google_intl = fetch_google_news_intl(search_keywords, region, lang, max_items=10)
                           
                           existing_hashes = {hashlib.md5(item.get('title_original', item['title']).encode()).hexdigest() for item in filtered_intl}
                           negative_keywords = cfg.get('negative_keywords', [])
                           all_intl_keywords = keywords_en + keywords_ja + keywords_ko

                           for item in google_intl:
                               if len(filtered_intl) >= 10:
                                   break
                               content = f"{item['title']} {item['summary']}"
                               if keyword_match(content, all_intl_keywords, negative_keywords):
                                   h = hashlib.md5(item['title'].encode()).hexdigest()
                                   if h not in existing_hashes:
                                       existing_hashes.add(h)
                                       # ç¿»è­¯
                                       if GEMINI_API_KEY:
                                            item['title_original'] = item['title']
                                            item['title'] = translate_with_gemini(item['title'])
                                            time.sleep(0.5)
                                       filtered_intl.append(item)
                       except Exception as e:
                           print(f"[WORKER] Google News (åœ‹éš›) è£œå……å¤±æ•—: {e}")

            # ç¿»è­¯ä¸éœ€è¦è£œå……çš„ï¼ˆåŸæœ¬ RSS æŠ“åˆ°çš„ï¼‰æ¨™é¡Œ
            if GEMINI_API_KEY:
                for news in filtered_intl:
                    if 'title_original' not in news:
                        news['title_original'] = news['title']
                        news['title'] = translate_with_gemini(news['title'])
                        time.sleep(0.2)
            
            # æŒ‰æ™‚é–“æ’åºä¸¦å–å‰ 10 å‰‡
            filtered_intl.sort(key=lambda x: x['published'], reverse=True)
            DATA_STORE[user_id]['international'][tid] = filtered_intl[:10]

        # æ›´æ–°æœ€å¾Œæ›´æ–°æ™‚é–“
        DATA_STORE[user_id]['last_update'] = datetime.now(TAIPEI_TZ).isoformat()
        
        # å„²å­˜å¿«å–åˆ° Supabase
        print(f"[WORKER] æ­£åœ¨å°‡ä½¿ç”¨è€… {user_id} çš„å¿«å–åŒæ­¥åˆ° Supabase...")
        for tid in topics_to_load.keys():
            domestic = DATA_STORE[user_id]['topics'].get(tid, [])
            intl = DATA_STORE[user_id]['international'].get(tid, [])
            summary = DATA_STORE[user_id]['summaries'].get(tid, {})
            auth.save_topic_cache_item(user_id, tid, domestic, intl, summary)
        
        # èˆŠçš„æª”æ¡ˆå¿«å–å¯ä¿ç•™å¯ç§»é™¤ï¼Œé€™è£¡æˆ‘å€‘å…ˆç§»é™¤ä»¥é¿å…æ··æ·†
        # save_data_cache()

        print(f"[WORKER] ä½¿ç”¨è€… {user_id} çš„è³‡æ–™è¼‰å…¥å®Œæˆ")

    except Exception as e:
        print(f"[WORKER] è¼‰å…¥ä½¿ç”¨è€… {user_id} è³‡æ–™å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # ç¢ºä¿ç„¡è«–æˆåŠŸå¤±æ•—éƒ½è§£é™¤è¼‰å…¥é–å®šï¼Œé¿å…æ­»é–
        if user_id in DATA_STORE:
            DATA_STORE[user_id]['is_loading'] = False

def update_topic_news():
    global LOADING_STATUS

    # åœ¨èªè­‰æ¨¡å¼ä¸‹ï¼Œåªæ›´æ–°æœ‰å¿«å–çš„ä½¿ç”¨è€…å°ˆé¡Œï¼ˆæŒ‰éœ€è¼‰å…¥ç­–ç•¥ï¼‰
    if AUTH_ENABLED:
        # å¾å¿«å–ä¸­å–å¾—å·²è¼‰å…¥çš„ä½¿ç”¨è€… ID
        cached_user_ids = [uid for uid in DATA_STORE.keys() if uid not in ['topics', 'international', 'summaries', 'last_update']]

        if not cached_user_ids:
            print(f"[UPDATE] æ²’æœ‰ä½¿ç”¨è€…å¿«å–ï¼Œè·³éæ›´æ–°")
            return

        # åªè¼‰å…¥é€™äº›ä½¿ç”¨è€…çš„å°ˆé¡Œ
        topics_to_update = {}
        for user_id in cached_user_ids:
            try:
                user_topics = auth.get_user_topics(user_id)
                for topic in user_topics:
                    topics_to_update[topic['id']] = {
                        'name': topic['name'],
                        'keywords': topic['keywords'],
                        'negative_keywords': topic.get('negative_keywords', []),
                        'icon': topic.get('icon', 'ğŸ“Œ'),
                        'order': topic.get('order', 999),
                        'user_id': topic['user_id']
                    }
            except Exception as e:
                print(f"[UPDATE] ç„¡æ³•è®€å–ä½¿ç”¨è€… {user_id} çš„å°ˆé¡Œ: {e}")

        print(f"[UPDATE] æ›´æ–° {len(cached_user_ids)} å€‹æ´»èºä½¿ç”¨è€…çš„ {len(topics_to_update)} å€‹å°ˆé¡Œ")
    else:
        topics_to_update = TOPICS

    total_topics = len(topics_to_update)
    LOADING_STATUS = {
        'is_loading': True,
        'current': 0,
        'total': total_topics,
        'current_topic': '',
        'phase': 'news'
    }
    print(f"\n[UPDATE] é–‹å§‹æ›´æ–°æ–°è - {datetime.now(TAIPEI_TZ).strftime('%H:%M:%S')}")

    # 1. æŠ“å–å°ç£æ–°èï¼ˆå¢åŠ æŠ“å–æ•¸é‡ï¼‰
    all_news_tw = []
    for name, url in RSS_SOURCES_TW.items():
        all_news_tw.extend(fetch_rss(url, name, max_items=50))

    # 2. æŠ“å–åœ‹éš›æ–°èï¼ˆå¢åŠ æŠ“å–æ•¸é‡ï¼‰
    all_news_intl = []
    for name, url in RSS_SOURCES_INTL.items():
        all_news_intl.extend(fetch_rss(url, name, max_items=50))

    # 2.5 æŠ“å– Google News åœ‹éš›ç‰ˆæ–°èï¼ˆæ—¥æœ¬ã€ç¾åœ‹ã€æ³•åœ‹ï¼‰
    # 2.5 æŠ“å– Google News åœ‹éš›ç‰ˆæ–°èï¼ˆå„ªåŒ–ï¼šå»é‡æœå°‹ + æ”¯æ´éŸ“æ–‡ï¼‰
    # å…ˆæ”¶é›†æ‰€æœ‰éœ€è¦æœå°‹çš„å”¯ä¸€é—œéµå­—çµ„åˆï¼Œé¿å…é‡è¤‡æŠ“å–
    unique_searches = set() # (region, lang, keyword)

    for tid, cfg in topics_to_update.items():
        keywords = cfg.get('keywords', {})
        if isinstance(keywords, dict):
            keywords_en = keywords.get('en', [])
            keywords_ja = keywords.get('ja', [])
            keywords_ko = keywords.get('ko', [])

            # å–ç¬¬ä¸€å€‹é—œéµå­—ä½œç‚ºä»£è¡¨é€²è¡Œæœå°‹
            if keywords_ja: unique_searches.add(('JP', 'ja', keywords_ja[0]))
            if keywords_en: 
                unique_searches.add(('US', 'en', keywords_en[0]))
                unique_searches.add(('FR', 'fr', keywords_en[0]))
            if keywords_ko: unique_searches.add(('KR', 'ko', keywords_ko[0]))

    print(f"[UPDATE] å½™æ•´å¾Œéœ€åŸ·è¡Œ {len(unique_searches)} æ¬¡ Google News æœå°‹")
    
    google_news_intl = []
    # åŸ·è¡Œå»é‡å¾Œçš„æœå°‹
    for region, lang, keyword in unique_searches:
        try:
            google_news_intl.extend(fetch_google_news_intl([keyword], region, lang, max_items=20))
            time.sleep(1) # æº«æŸ”ä¸€é»
        except Exception as e:
            print(f"[UPDATE] Google Search error ({region}/{keyword}): {e}")

    all_news_intl.extend(google_news_intl)

    # 3. éæ¿¾å°ç£æ–°èå’Œåœ‹éš›æ–°è
    topic_index = 0
    for tid, cfg in topics_to_update.items():
        topic_index += 1
        LOADING_STATUS['current'] = topic_index
        LOADING_STATUS['current_topic'] = cfg['name']

        # è¨˜éŒ„å°ˆé¡Œæ“æœ‰è€…ï¼ˆåœ¨èªè­‰æ¨¡å¼ä¸‹ï¼‰
        if AUTH_ENABLED and 'user_id' in cfg:
            DATA_STORE['topic_owners'][tid] = cfg['user_id']

        keywords = cfg.get('keywords', {})

        # è™•ç†èˆŠæ ¼å¼ï¼ˆç´”åˆ—è¡¨ï¼‰vs æ–°æ ¼å¼ï¼ˆå­—å…¸ï¼‰
        if isinstance(keywords, list):
            keywords_zh = keywords
            keywords_en = []
            keywords_ja = []
        else:
            keywords_zh = keywords.get('zh', [])
            keywords_en = keywords.get('en', [])
            keywords_ja = keywords.get('ja', [])

        # ç²å–è² é¢é—œéµå­—
        negative_keywords = cfg.get('negative_keywords', [])

        # éæ¿¾å°ç£æ–°èï¼ˆä½¿ç”¨ä¸­æ–‡é—œéµå­—ï¼‰- ç¢ºä¿è‡³å°‘10å‰‡
        if not keywords_zh:
            DATA_STORE['topics'][tid] = []
        else:
            # å–å¾—ç¾æœ‰æ–°èåˆ—è¡¨
            existing_news = DATA_STORE['topics'].get(tid, [])
            existing_hashes = {hashlib.md5(item['title'].encode()).hexdigest(): item
                             for item in existing_news}

            # éæ¿¾æ–°æ–°è
            filtered_tw = []
            seen_tw = set(existing_hashes.keys())
            new_items = []

            for item in all_news_tw:
                content = f"{item['title']} {item['summary']}"
                if keyword_match(content, keywords_zh, negative_keywords):
                    h = hashlib.md5(item['title'].encode()).hexdigest()
                    if h not in seen_tw:
                        seen_tw.add(h)
                        new_items.append(item)

            # åˆä½µï¼šæ–°æ–°è + ç¾æœ‰æ–°èï¼ŒæŒ‰æ™‚é–“æ’åº
            all_items = new_items + existing_news
            all_items.sort(key=lambda x: x['published'], reverse=True)

            # å¦‚æœæ–°èæ•¸é‡å°‘æ–¼ 10 å‰‡ï¼Œä½¿ç”¨ Google News æœç´¢è£œå……
            if len(all_items) < 10:
                print(f"[SEARCH] {cfg['name']}: åªæœ‰ {len(all_items)} å‰‡ï¼Œä½¿ç”¨ Google News æœç´¢è£œå……...")
                google_news = fetch_google_news_by_keywords(keywords_zh, max_items=100)

                # éæ¿¾ä¸¦å»é‡
                existing_hashes_all = {hashlib.md5(item['title'].encode()).hexdigest() for item in all_items}
                for item in google_news:
                    if len(all_items) >= 10:
                        break
                    content = f"{item['title']} {item['summary']}"
                    if keyword_match(content, keywords_zh, negative_keywords):
                        h = hashlib.md5(item['title'].encode()).hexdigest()
                        if h not in existing_hashes_all:
                            existing_hashes_all.add(h)
                            all_items.append(item)

                all_items.sort(key=lambda x: x['published'], reverse=True)
                print(f"[SEARCH] {cfg['name']}: è£œå……å¾Œå…± {len(all_items)} å‰‡æ–°è")

            # ä¿æŒæœ€æ–°çš„ 10 å‰‡ï¼ˆä¸€å‰‡ä¸€å‰‡æ›¿æ›ï¼‰
            if AUTH_ENABLED and tid in DATA_STORE.get('topic_owners', {}):
                owner_id = DATA_STORE['topic_owners'][tid]
                if owner_id in DATA_STORE:
                    DATA_STORE[owner_id]['topics'][tid] = all_items[:10]
            else:
                DATA_STORE['topics'][tid] = all_items[:10]

            if new_items:
                current_count = len(DATA_STORE[owner_id]['topics'][tid]) if (AUTH_ENABLED and tid in DATA_STORE.get('topic_owners', {}) and owner_id in DATA_STORE) else len(DATA_STORE['topics'][tid])
                print(f"[UPDATE] {cfg['name']}: æ–°å¢ {len(new_items)} å‰‡æ–°èï¼Œç•¶å‰ {current_count} å‰‡")

        # éæ¿¾åœ‹éš›æ–°èï¼ˆä½¿ç”¨è‹±æ—¥æ–‡é—œéµå­—ï¼‰- ç¢ºä¿è‡³å°‘10å‰‡
        intl_keywords = keywords_en + keywords_ja
        if not intl_keywords:
            DATA_STORE['international'][tid] = []
        else:
            # å–å¾—ç¾æœ‰åœ‹éš›æ–°è
            existing_intl = DATA_STORE['international'].get(tid, [])
            existing_intl_hashes = {hashlib.md5(item.get('title_original', item['title']).encode()).hexdigest(): item
                                   for item in existing_intl}

            # éæ¿¾æ–°çš„åœ‹éš›æ–°è
            filtered_intl = []
            seen_intl = set(existing_intl_hashes.keys())
            new_intl_items = []

            for item in all_news_intl:
                content = f"{item['title']} {item['summary']}"
                if keyword_match(content, intl_keywords, negative_keywords):
                    h = hashlib.md5(item['title'].encode()).hexdigest()
                    if h not in seen_intl:
                        seen_intl.add(h)
                        # ç¿»è­¯æ¨™é¡Œï¼ˆåŠ å…¥å»¶é²é¿å… API é€Ÿç‡é™åˆ¶ï¼‰
                        original_title = item['title']
                        translated_title = translate_with_gemini(original_title)
                        item['title_original'] = original_title
                        item['title'] = translated_title
                        new_intl_items.append(item)
                        time.sleep(0.5)  # æ¯æ¬¡ç¿»è­¯å¾Œç­‰å¾… 0.5 ç§’

            # åˆä½µï¼šæ–°æ–°è + ç¾æœ‰æ–°èï¼ŒæŒ‰æ™‚é–“æ’åº
            all_intl_items = new_intl_items + existing_intl
            all_intl_items.sort(key=lambda x: x['published'], reverse=True)

            # å¦‚æœæ–°èæ•¸é‡å°‘æ–¼ 5 å‰‡ï¼Œä½¿ç”¨ Google News åœ‹éš›ç‰ˆè£œå……
            if len(all_intl_items) < 5:
                print(f"[SEARCH] {cfg['name']} (åœ‹éš›): åªæœ‰ {len(all_intl_items)} å‰‡ï¼Œä½¿ç”¨ Google News åœ‹éš›ç‰ˆè£œå……...")

                # ä¾åºå¾æ—¥æœ¬ã€ç¾åœ‹ã€æ³•åœ‹ Google News è£œå……
                for region_name, region_info in GOOGLE_NEWS_INTL_REGIONS.items():
                    if len(all_intl_items) >= 5:
                        break

                    # æ ¹æ“šèªè¨€é¸æ“‡é—œéµå­—
                    search_keywords = keywords_ja if region_info['lang'] == 'ja' else keywords_en
                    if not search_keywords:
                        continue

                    google_intl = fetch_google_news_intl(
                        search_keywords,
                        region_info['code'],
                        region_info['lang'],
                        max_items=20
                    )

                    # éæ¿¾ä¸¦ç¿»è­¯
                    existing_hashes_all = {hashlib.md5(item.get('title_original', item['title']).encode()).hexdigest()
                                         for item in all_intl_items}
                    for item in google_intl:
                        if len(all_intl_items) >= 5:
                            break
                        content = f"{item['title']} {item['summary']}"
                        if keyword_match(content, intl_keywords, negative_keywords):
                            h = hashlib.md5(item['title'].encode()).hexdigest()
                            if h not in existing_hashes_all:
                                existing_hashes_all.add(h)
                                # ç¿»è­¯æ¨™é¡Œ
                                original_title = item['title']
                                translated_title = translate_with_gemini(original_title)
                                item['title_original'] = original_title
                                item['title'] = translated_title
                                all_intl_items.append(item)
                                time.sleep(0.5)

                all_intl_items.sort(key=lambda x: x['published'], reverse=True)
                print(f"[SEARCH] {cfg['name']} (åœ‹éš›): è£œå……å¾Œå…± {len(all_intl_items)} å‰‡æ–°è")

            # ä¿æŒæœ€æ–°çš„ 10 å‰‡
            if AUTH_ENABLED and tid in DATA_STORE.get('topic_owners', {}):
                owner_id = DATA_STORE['topic_owners'][tid]
                if owner_id in DATA_STORE:
                    DATA_STORE[owner_id]['international'][tid] = all_intl_items[:10]
            else:
                DATA_STORE['international'][tid] = all_intl_items[:10]

            if new_intl_items:
                current_count = len(DATA_STORE[owner_id]['international'][tid]) if (AUTH_ENABLED and tid in DATA_STORE.get('topic_owners', {}) and owner_id in DATA_STORE) else len(DATA_STORE['international'][tid])
                print(f"[UPDATE] {cfg['name']} (åœ‹éš›): æ–°å¢ {len(new_intl_items)} å‰‡æ–°èï¼Œç•¶å‰ {current_count} å‰‡")

    DATA_STORE['last_update'] = datetime.now(TAIPEI_TZ).isoformat()
    LOADING_STATUS['is_loading'] = False
    LOADING_STATUS['current'] = total_topics

    # å„²å­˜åˆ°å¿«å–æª”æ¡ˆ
    save_data_cache()

    print("[UPDATE] å®Œæˆ")
    # æ‘˜è¦æ›´æ–°æ”¹ç”¨æ’ç¨‹ï¼ˆæ¯å¤© 8:00 å’Œ 18:00ï¼‰ï¼Œä¸åœ¨æ–°èæ›´æ–°æ™‚è§¸ç™¼

def update_domestic_news():
    """åªæ›´æ–°åœ‹å…§æ–°èï¼ˆæ•´é»é–‹å§‹æ¯30åˆ†é˜ï¼‰"""
    global LOADING_STATUS

    # åœ¨èªè­‰æ¨¡å¼ä¸‹ï¼Œå¾ Supabase è®€å–æ‰€æœ‰ä½¿ç”¨è€…çš„å°ˆé¡Œ
    if AUTH_ENABLED:
        try:
            all_user_topics = auth.get_all_topics_admin()
            topics_to_update = {}
            for topic in all_user_topics:
                topics_to_update[topic['id']] = {
                    'name': topic['name'],
                    'keywords': topic['keywords'],
                    'negative_keywords': topic.get('negative_keywords', []),
                    'icon': topic.get('icon', 'ğŸ“Œ'),
                    'order': topic.get('order', 999),
                    'user_id': topic['user_id']
                }
            print(f"[UPDATE:DOMESTIC] å¾ Supabase è¼‰å…¥äº† {len(topics_to_update)} å€‹ä½¿ç”¨è€…å°ˆé¡Œ")
        except Exception as e:
            print(f"[UPDATE:DOMESTIC] ç„¡æ³•å¾ Supabase è®€å–å°ˆé¡Œï¼Œä½¿ç”¨æœ¬åœ°è¨­å®š: {e}")
            topics_to_update = TOPICS
    else:
        topics_to_update = TOPICS

    total_topics = len(topics_to_update)
    LOADING_STATUS = {
        'is_loading': True,
        'current': 0,
        'total': total_topics,
        'current_topic': '',
        'phase': 'domestic'
    }
    LOADING_STATUS = {
        'is_loading': True,
        'current': 0,
        'total': total_topics,
        'current_topic': '',
        'phase': 'news'
    }
    print(f"\n[UPDATE:DOMESTIC] é–‹å§‹æ›´æ–°åœ‹å…§æ–°è - {datetime.now(TAIPEI_TZ).strftime('%H:%M:%S')}")

    # 1. æŠ“å–å°ç£æ–°è
    all_news_tw = []
    for name, url in RSS_SOURCES_TW.items():
        all_news_tw.extend(fetch_rss(url, name, max_items=50))

    # 2. éæ¿¾å°ç£æ–°è
    topic_index = 0
    for tid, cfg in topics_to_update.items():
        topic_index += 1
        LOADING_STATUS['current'] = topic_index
        LOADING_STATUS['current_topic'] = cfg['name']

        # è¨˜éŒ„å°ˆé¡Œæ“æœ‰è€…ï¼ˆåœ¨èªè­‰æ¨¡å¼ä¸‹ï¼‰
        if AUTH_ENABLED and 'user_id' in cfg:
            DATA_STORE['topic_owners'][tid] = cfg['user_id']

        keywords = cfg.get('keywords', {})

        # è™•ç†èˆŠæ ¼å¼ vs æ–°æ ¼å¼
        if isinstance(keywords, list):
            keywords_zh = keywords
        else:
            keywords_zh = keywords.get('zh', [])

        negative_keywords = cfg.get('negative_keywords', [])

        if not keywords_zh:
            continue

        # å–å¾—ç¾æœ‰æ–°èåˆ—è¡¨
        existing_news = DATA_STORE['topics'].get(tid, [])
        existing_hashes = {hashlib.md5(item['title'].encode()).hexdigest(): item
                         for item in existing_news}

        # éæ¿¾æ–°æ–°è
        seen_tw = set(existing_hashes.keys())
        new_items = []

        for item in all_news_tw:
            content = f"{item['title']} {item['summary']}"
            if keyword_match(content, keywords_zh, negative_keywords):
                h = hashlib.md5(item['title'].encode()).hexdigest()
                if h not in seen_tw:
                    seen_tw.add(h)
                    new_items.append(item)

        # åˆä½µï¼šæ–°æ–°è + ç¾æœ‰æ–°èï¼ŒæŒ‰æ™‚é–“æ’åº
        all_items = new_items + existing_news
        all_items.sort(key=lambda x: x['published'], reverse=True)

        # Google News è£œå……
        if len(all_items) < 10:
            google_news = fetch_google_news_by_keywords(keywords_zh, max_items=100)
            existing_hashes_all = {hashlib.md5(item['title'].encode()).hexdigest() for item in all_items}
            for item in google_news:
                if len(all_items) >= 10:
                    break
                content = f"{item['title']} {item['summary']}"
                if keyword_match(content, keywords_zh, negative_keywords):
                    h = hashlib.md5(item['title'].encode()).hexdigest()
                    if h not in existing_hashes_all:
                        existing_hashes_all.add(h)
                        all_items.append(item)

            all_items.sort(key=lambda x: x['published'], reverse=True)

        if AUTH_ENABLED and 'user_id' in cfg:
            owner_id = cfg['user_id']
            # ç¢ºä¿è©²ä½¿ç”¨è€…çš„è³‡æ–™çµæ§‹å­˜åœ¨
            if owner_id in DATA_STORE and 'topics' in DATA_STORE[owner_id]:
                DATA_STORE[owner_id]['topics'][tid] = all_items[:10]
        else:
            DATA_STORE['topics'][tid] = all_items[:10]

        if new_items:
            print(f"[UPDATE:DOMESTIC] {cfg['name']}: æ–°å¢ {len(new_items)} å‰‡æ–°è")

    DATA_STORE['last_update'] = datetime.now(TAIPEI_TZ).isoformat()
    LOADING_STATUS['is_loading'] = False
    save_data_cache()
    print("[UPDATE:DOMESTIC] å®Œæˆ")

def update_international_news():
    """åªæ›´æ–°åœ‹éš›æ–°èï¼ˆ15åˆ†é–‹å§‹æ¯30åˆ†é˜ï¼‰"""
    global LOADING_STATUS

    # åœ¨èªè­‰æ¨¡å¼ä¸‹ï¼Œå¾ Supabase è®€å–æ‰€æœ‰ä½¿ç”¨è€…çš„å°ˆé¡Œ
    if AUTH_ENABLED:
        try:
            all_user_topics = auth.get_all_topics_admin()
            topics_to_update = {}
            for topic in all_user_topics:
                topics_to_update[topic['id']] = {
                    'name': topic['name'],
                    'keywords': topic['keywords'],
                    'negative_keywords': topic.get('negative_keywords', []),
                    'icon': topic.get('icon', 'ğŸ“Œ'),
                    'order': topic.get('order', 999),
                    'user_id': topic['user_id']
                }
            print(f"[UPDATE:INTL] å¾ Supabase è¼‰å…¥äº† {len(topics_to_update)} å€‹ä½¿ç”¨è€…å°ˆé¡Œ")
        except Exception as e:
            print(f"[UPDATE:INTL] ç„¡æ³•å¾ Supabase è®€å–å°ˆé¡Œï¼Œä½¿ç”¨æœ¬åœ°è¨­å®š: {e}")
            topics_to_update = TOPICS
    else:
        topics_to_update = TOPICS

    total_topics = len(topics_to_update)
    LOADING_STATUS = {
        'is_loading': True,
        'current': 0,
        'total': total_topics,
        'current_topic': '',
        'total': total_topics,
        'current_topic': '',
        'phase': 'news'
    }
    print(f"\n[UPDATE:INTL] é–‹å§‹æ›´æ–°åœ‹éš›æ–°è - {datetime.now(TAIPEI_TZ).strftime('%H:%M:%S')}")

    # 1. æŠ“å–åœ‹éš›æ–°è
    all_news_intl = []
    for name, url in RSS_SOURCES_INTL.items():
        all_news_intl.extend(fetch_rss(url, name, max_items=50))

    # 2. æŠ“å– Google News åœ‹éš›ç‰ˆ
    for tid, cfg in topics_to_update.items():
        keywords = cfg.get('keywords', {})
        if isinstance(keywords, dict):
            keywords_en = keywords.get('en', [])
            keywords_ja = keywords.get('ja', [])

            if keywords_ja:
                all_news_intl.extend(fetch_google_news_intl(keywords_ja, 'JP', 'ja', max_items=20))
            if keywords_en:
                all_news_intl.extend(fetch_google_news_intl(keywords_en, 'US', 'en', max_items=20))
                all_news_intl.extend(fetch_google_news_intl(keywords_en, 'FR', 'fr', max_items=20))

    # 3. éæ¿¾åœ‹éš›æ–°è
    topic_index = 0
    for tid, cfg in topics_to_update.items():
        topic_index += 1
        LOADING_STATUS['current'] = topic_index
        LOADING_STATUS['current_topic'] = cfg['name']

        # è¨˜éŒ„å°ˆé¡Œæ“æœ‰è€…ï¼ˆåœ¨èªè­‰æ¨¡å¼ä¸‹ï¼‰
        if AUTH_ENABLED and 'user_id' in cfg:
            DATA_STORE['topic_owners'][tid] = cfg['user_id']

        keywords = cfg.get('keywords', {})

        if isinstance(keywords, list):
            keywords_en = []
            keywords_ja = []
            keywords_ko = []
        else:
            keywords_en = keywords.get('en', [])
            keywords_ja = keywords.get('ja', [])
            keywords_ko = keywords.get('ko', [])

        negative_keywords = cfg.get('negative_keywords', [])
        intl_keywords = keywords_en + keywords_ja + keywords_ko

        if not intl_keywords:
            continue

        # å–å¾—ç¾æœ‰åœ‹éš›æ–°è
        existing_intl = DATA_STORE['international'].get(tid, [])
        existing_intl_hashes = {hashlib.md5(item.get('title_original', item['title']).encode()).hexdigest(): item
                               for item in existing_intl}

        seen_intl = set(existing_intl_hashes.keys())
        new_intl_items = []

        for item in all_news_intl:
            content = f"{item['title']} {item['summary']}"
            if keyword_match(content, intl_keywords, negative_keywords):
                h = hashlib.md5(item['title'].encode()).hexdigest()
                if h not in seen_intl:
                    seen_intl.add(h)
                    # ç¿»è­¯æ¨™é¡Œ
                    original_title = item['title']
                    translated_title = translate_with_gemini(original_title)
                    item['title_original'] = original_title
                    item['title'] = translated_title
                    new_intl_items.append(item)
                    time.sleep(0.5)

        # åˆä½µä¸¦æ’åº
        all_intl_items = new_intl_items + existing_intl
        all_intl_items.sort(key=lambda x: x['published'], reverse=True)

        # Google News åœ‹éš›ç‰ˆè£œå……
        if len(all_intl_items) < 5:
            for region_name, region_info in GOOGLE_NEWS_INTL_REGIONS.items():
                if len(all_intl_items) >= 5:
                    break
                
                # é¸æ“‡å°æ‡‰èªè¨€çš„é—œéµå­—
                if region_info['lang'] == 'ja':
                    search_keywords = keywords_ja
                elif region_info['lang'] == 'ko':
                    search_keywords = keywords_ko
                else:
                    search_keywords = keywords_en
                
                if not search_keywords:
                    continue

                google_intl = fetch_google_news_intl(search_keywords, region_info['code'], region_info['lang'], max_items=20)
                existing_hashes_all = {hashlib.md5(item.get('title_original', item['title']).encode()).hexdigest()
                                     for item in all_intl_items}
                for item in google_intl:
                    if len(all_intl_items) >= 5:
                        break
                    content = f"{item['title']} {item['summary']}"
                    if keyword_match(content, intl_keywords, negative_keywords):
                        h = hashlib.md5(item['title'].encode()).hexdigest()
                        if h not in existing_hashes_all:
                            existing_hashes_all.add(h)
                            original_title = item['title']
                            translated_title = translate_with_gemini(original_title)
                            item['title_original'] = original_title
                            item['title'] = translated_title
                            all_intl_items.append(item)
                            time.sleep(0.5)

            all_intl_items.sort(key=lambda x: x['published'], reverse=True)

            # ä¿æŒæœ€æ–°çš„ 10 å‰‡
            if AUTH_ENABLED and 'user_id' in cfg:
                owner_id = cfg['user_id']
                if owner_id in DATA_STORE and 'international' in DATA_STORE[owner_id]:
                    DATA_STORE[owner_id]['international'][tid] = all_intl_items[:10]
            else:
                DATA_STORE['international'][tid] = all_intl_items[:10]

        if new_intl_items:
            print(f"[UPDATE:INTL] {cfg['name']}: æ–°å¢ {len(new_intl_items)} å‰‡åœ‹éš›å ±å°")

    DATA_STORE['last_update'] = datetime.now(TAIPEI_TZ).isoformat()
    LOADING_STATUS['is_loading'] = False
    save_data_cache()
    print("[UPDATE:INTL] å®Œæˆ")

def update_all_summaries():
    print(f"\n[SUMMARY] é–‹å§‹ AI æ‘˜è¦...")

    # åœ¨èªè­‰æ¨¡å¼ä¸‹ï¼Œåªæ›´æ–°æœ‰å¿«å–çš„ä½¿ç”¨è€…å°ˆé¡Œï¼ˆæŒ‰éœ€è¼‰å…¥ç­–ç•¥ï¼‰
    if AUTH_ENABLED:
        # å¾å¿«å–ä¸­å–å¾—å·²è¼‰å…¥çš„ä½¿ç”¨è€… ID
        cached_user_ids = [uid for uid in DATA_STORE.keys() if uid not in ['topics', 'international', 'summaries', 'last_update', 'topic_owners']]

        if not cached_user_ids:
            print(f"[SUMMARY] æ²’æœ‰ä½¿ç”¨è€…å¿«å–ï¼Œè·³éæ›´æ–°")
            return

        # åªè¼‰å…¥é€™äº›ä½¿ç”¨è€…çš„å°ˆé¡Œ
        topics_to_summarize = {}
        for user_id in cached_user_ids:
            try:
                user_topics = auth.get_user_topics(user_id)
                for topic in user_topics:
                    topics_to_summarize[topic['id']] = {
                        'user_id': user_id,
                        'name': topic['name']
                    }
            except Exception as e:
                print(f"[SUMMARY] ç„¡æ³•è®€å–ä½¿ç”¨è€… {user_id} çš„å°ˆé¡Œ: {e}")

        print(f"[SUMMARY] æ›´æ–° {len(cached_user_ids)} å€‹æ´»èºä½¿ç”¨è€…çš„ {len(topics_to_summarize)} å€‹å°ˆé¡Œæ‘˜è¦")
    else:
        topics_to_summarize = {tid: {} for tid in TOPICS.keys()}

    # è¨­å®šè¼‰å…¥ç‹€æ…‹
    global LOADING_STATUS
    total_summaries = len(topics_to_summarize)
    LOADING_STATUS = {
        'is_loading': True,
        'current': 0,
        'total': total_summaries,
        'current_topic': '',
        'phase': 'summary'  # æ¨™è¨˜ç‚ºæ‘˜è¦æ›´æ–°éšæ®µ
    }

    summary_index = 0
    for tid, topic_info in topics_to_summarize.items():
        summary_index += 1
        topic_name = topic_info.get('name', 'æœªçŸ¥å°ˆé¡Œ')
        
        # æ›´æ–°è¼‰å…¥ç‹€æ…‹
        LOADING_STATUS['current'] = summary_index
        LOADING_STATUS['current_topic'] = topic_name
        
        # è¨˜éŒ„å°ˆé¡Œæ“æœ‰è€…ï¼ˆåœ¨èªè­‰æ¨¡å¼ä¸‹ï¼‰
        if AUTH_ENABLED and 'user_id' in topic_info:
            DATA_STORE['topic_owners'][tid] = topic_info['user_id']

        # é€™è£¡å‚³å…¥ topic_name å’Œ user_idï¼Œé¿å… "æœªçŸ¥å°ˆé¡Œ" éŒ¯èª¤
        summary_text = generate_topic_summary(
            tid, 
            topic_name=topic_info.get('name'), 
            user_id=topic_info.get('user_id')
        )
        
        summary_data = {
            'text': summary_text,
            'updated_at': datetime.now(TAIPEI_TZ).isoformat()
        }
        
        # å­˜å…¥å…¨åŸŸï¼ˆå‘å¾Œç›¸å®¹/ç®¡ç†å“¡æŸ¥çœ‹ï¼‰
        DATA_STORE['summaries'][tid] = summary_data
        
        # å­˜å…¥ä½¿ç”¨è€…å°ˆå±¬ä½ç½®ï¼ˆé‡è¦ï¼ï¼‰
        if AUTH_ENABLED and 'user_id' in topic_info:
            user_id = topic_info['user_id']
            if user_id in DATA_STORE:
                if 'summaries' not in DATA_STORE[user_id]:
                    DATA_STORE[user_id]['summaries'] = {}
                DATA_STORE[user_id]['summaries'][tid] = summary_data
        
        time.sleep(1)

    # å®Œæˆï¼Œé‡è¨­è¼‰å…¥ç‹€æ…‹
    LOADING_STATUS['is_loading'] = False
    LOADING_STATUS['current'] = total_summaries
    LOADING_STATUS['phase'] = ''
    
    # å„²å­˜åˆ°å¿«å–æª”æ¡ˆ
    save_data_cache()

    print("[SUMMARY] å®Œæˆ")

# ============ API ============

@app.route('/')
def index():
    return app.send_static_file('index.html')

@app.route('/admin')
def admin():
    response = make_response(app.send_static_file('admin.html'))
    # é˜²æ­¢å¿«å–ï¼Œç¢ºä¿ç¸½æ˜¯è¼‰å…¥æœ€æ–°ç‰ˆæœ¬
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    return response

@app.route('/login')
def login():
    response = make_response(app.send_static_file('login.html'))
    # é˜²æ­¢å¿«å–ï¼Œç¢ºä¿ç¸½æ˜¯è¼‰å…¥æœ€æ–°ç‰ˆæœ¬
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    return response

@app.route('/api/all')
def get_all():
    # èªè­‰æª¢æŸ¥ï¼ˆå¦‚æœèªè­‰ç³»çµ±å·²å•Ÿç”¨ï¼‰
    if AUTH_ENABLED:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not token:
            return jsonify({'error': 'æœªç™»å…¥'}), 401
        user = auth.get_user_from_token(token)
        if not user:
            return jsonify({'error': 'èªè­‰å¤±æ•—'}), 401

        user_id = user.id

        # è®€å–å‰ç«¯å‚³ä¾†çš„ check_freshness åƒæ•¸ (å­—ä¸² 'true' è½‰å¸ƒæ—å€¼)
        check_freshness = request.args.get('check_freshness', 'false').lower() == 'true'

        # æŒ‰éœ€è¼‰å…¥ï¼šç¸½æ˜¯å‘¼å« load_user_data
        # check_freshness=True: ç™»å…¥æ™‚æª¢æŸ¥ (5åˆ†é˜é–€æª»)
        # check_freshness=False: å®šæœŸè¼ªè©¢ (60åˆ†é˜é–€æª»)
        load_user_data(user_id, check_freshness)

        # å¾ Supabase è®€å–è©²ä½¿ç”¨è€…çš„å°ˆé¡Œ
        user_topics = auth.get_user_topics(user_id)

        # å–å¾—è©²ä½¿ç”¨è€…çš„è³‡æ–™
        user_data = DATA_STORE.get(user_id, {'topics': {}, 'international': {}, 'summaries': {}, 'last_update': ''})
        
        # å‘ŠçŸ¥å‰ç«¯æ˜¯å¦æ­£åœ¨è¼‰å…¥ä¸­ï¼ˆè®“å‰ç«¯çŸ¥é“è³‡æ–™å¯èƒ½ä¸å®Œæ•´ï¼‰
        is_loading = user_data.get('is_loading', False)
        
        result = {
            'topics': {}, 
            'last_update': user_data.get('last_update', ''),
            'loading_pending': is_loading  # æ–°å¢ï¼šå‘ŠçŸ¥å‰ç«¯èƒŒæ™¯è¼‰å…¥ä¸­
        }
        now = datetime.now(TAIPEI_TZ)

        for topic in user_topics:
            tid = topic['id']
            cfg = topic

            # å–å¾—æ–°èï¼ˆå¾ä½¿ç”¨è€…å¿«å–æˆ–ç©ºåˆ—è¡¨ï¼‰
            news = user_data.get('topics', {}).get(tid, [])
            intl_news = user_data.get('international', {}).get(tid, [])
            summary = user_data.get('summaries', {}).get(tid, {})

            # æ ¼å¼åŒ–å°ç£æ–°è
            fmt_news = []
            for n in news[:10]:
                dt = n['published']
                if isinstance(dt, str):
                    try:
                        dt = datetime.fromisoformat(dt)
                    except ValueError:
                        continue # Skip invalid dates

                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=TAIPEI_TZ)
                else:
                    dt = dt.astimezone(TAIPEI_TZ)

                is_date_only = n.get('is_date_only', False)
                if is_date_only:
                    time_str = dt.strftime('%m/%d')
                elif dt.date() == now.date():
                    time_str = dt.strftime('%H:%M')
                else:
                    time_str = dt.strftime('%m/%d')

                fmt_news.append({
                    'title': n['title'],
                    'link': n['link'],
                    'source': n['source'],
                    'time': time_str
                })

            # æ ¼å¼åŒ–åœ‹éš›æ–°è
            fmt_intl_news = []
            for n in intl_news[:10]:
                dt = n['published']
                if isinstance(dt, str):
                    try:
                        dt = datetime.fromisoformat(dt)
                    except ValueError:
                        continue

                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=TAIPEI_TZ)
                else:
                    dt = dt.astimezone(TAIPEI_TZ)

                if dt.date() == now.date():
                    time_str = dt.strftime('%H:%M')
                else:
                    time_str = dt.strftime('%m/%d')

                fmt_intl_news.append({
                    'title': n['title'],
                    'title_original': n.get('title_original', ''),
                    'link': n['link'],
                    'source': n['source'],
                    'time': time_str
                })

            # è™•ç†é—œéµå­—é¡¯ç¤º
            keywords = cfg.get('keywords', [])
            if isinstance(keywords, dict):
                display_keywords = keywords.get('zh', [])
            else:
                display_keywords = keywords if keywords else []

            result['topics'][tid] = {
                'id': tid,
                'name': cfg['name'],
                'icon': cfg.get('icon', 'ğŸ“Œ'),
                'keywords': display_keywords,
                'summary': summary.get('text', ''),
                'summary_updated': summary.get('updated_at'),
                'news': fmt_news,
                'international': fmt_intl_news,
                'order': cfg.get('order', 999)
            }
        
        response = make_response(jsonify(result))
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        return response
    
    else:
        # èªè­‰æœªå•Ÿç”¨æ™‚ä½¿ç”¨èˆŠé‚è¼¯ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
        result = {'topics': {}, 'last_update': DATA_STORE['last_update']}
        now = datetime.now(TAIPEI_TZ)
        
        for tid, cfg in TOPICS.items():
            news = DATA_STORE['topics'].get(tid, [])
            intl_news = DATA_STORE['international'].get(tid, [])
            summary = DATA_STORE['summaries'].get(tid, {})

            fmt_news = []
            for n in news[:10]:
                dt = n['published']
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=TAIPEI_TZ)
                else:
                    dt = dt.astimezone(TAIPEI_TZ)

                is_date_only = n.get('is_date_only', False)
                if is_date_only:
                    time_str = dt.strftime('%m/%d')
                elif dt.date() == now.date():
                    time_str = dt.strftime('%H:%M')
                else:
                    time_str = dt.strftime('%m/%d')

                fmt_news.append({
                    'title': n['title'],
                    'link': n['link'],
                    'source': n['source'],
                    'time': time_str
                })

            fmt_intl_news = []
            for n in intl_news[:10]:
                dt = n['published']
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=TAIPEI_TZ)
                else:
                    dt = dt.astimezone(TAIPEI_TZ)

                if dt.date() == now.date():
                    time_str = dt.strftime('%H:%M')
                else:
                    time_str = dt.strftime('%m/%d')

                fmt_intl_news.append({
                    'title': n['title'],
                    'title_original': n.get('title_original', ''),
                    'link': n['link'],
                    'source': n['source'],
                    'time': time_str
                })

            keywords = cfg.get('keywords', [])
            if isinstance(keywords, dict):
                display_keywords = keywords.get('zh', [])
            else:
                display_keywords = keywords

            result['topics'][tid] = {
                'id': tid,
                'name': cfg['name'],
                'icon': cfg.get('icon', 'ğŸ“Œ'),
                'keywords': display_keywords,
                'summary': summary.get('text', ''),
                'summary_updated': summary.get('updated_at'),
                'news': fmt_news,
                'international': fmt_intl_news,
                'order': cfg.get('order', 999)
            }
        return jsonify(result)

@app.route('/api/refresh', methods=['POST'])
def refresh():
    update_topic_news()
    return jsonify({'status': 'ok'})

@app.route('/api/refresh-summary', methods=['POST'])
def refresh_summary():
    update_all_summaries()
    return jsonify({'status': 'ok'})

@app.route('/api/loading-status')
def loading_status():
    """å›å‚³è¼‰å…¥é€²åº¦ç‹€æ…‹ï¼ˆä½¿ç”¨è€…å°ˆå±¬ï¼Œä¸ä½¿ç”¨å…¨åŸŸç‹€æ…‹ï¼‰"""
    if AUTH_ENABLED:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if token:
            user = auth.get_user_from_token(token)
            if user:
                user_id = user.id
                user_topics = auth.get_user_topics(user_id)
                user_topic_count = len(user_topics)

                # å¦‚æœä½¿ç”¨è€…å°ˆé¡Œç‚º 0
                if user_topic_count == 0:
                    return jsonify({
                        'is_loading': False,
                        'current': 0,
                        'total': 0,
                        'phase': '',
                        'current_topic': ''
                    })

                # å„ªå…ˆæª¢æŸ¥ä½¿ç”¨è€…å°ˆå±¬çš„ is_loading æ¨™è¨˜ï¼ˆé€™æ˜¯æœ€å¯é çš„æŒ‡æ¨™ï¼‰
                if user_id in DATA_STORE:
                    user_data = DATA_STORE[user_id]
                    
                    # å¦‚æœ is_loading=Trueï¼Œè¡¨ç¤ºèƒŒæ™¯ Worker æ­£åœ¨åŸ·è¡Œ
                    if user_data.get('is_loading'):
                        # è¨ˆç®—å·²è¼‰å…¥çš„å°ˆé¡Œæ•¸é‡ä½œç‚ºé€²åº¦
                        loaded_count = 0
                        for topic in user_topics:
                            tid = topic['id']
                            if tid in user_data.get('topics', {}) or tid in user_data.get('international', {}):
                                loaded_count += 1
                        
                        return jsonify({
                            'is_loading': True,
                            'current': loaded_count,
                            'total': user_topic_count,
                            'phase': user_data.get('phase', 'news'),
                            'current_topic': 'è³‡æ–™è¼‰å…¥ä¸­...'
                        })
                    
                    # is_loading=Falseï¼Œæª¢æŸ¥è³‡æ–™å®Œæ•´æ€§
                    if user_data.get('last_update'):
                        # æœ‰ last_update è¡¨ç¤ºè¼‰å…¥å·²å®Œæˆ
                        return jsonify({
                            'is_loading': False,
                            'current': user_topic_count,
                            'total': user_topic_count,
                            'phase': '',
                            'current_topic': ''
                        })
                
                # ä½¿ç”¨è€…ä¸åœ¨ DATA_STORE ä¸­ï¼Œè¡¨ç¤ºå°šæœªé–‹å§‹è¼‰å…¥
                return jsonify({
                    'is_loading': False,
                    'current': 0,
                    'total': user_topic_count,
                    'phase': '',
                    'current_topic': ''
                })

    # æœªç™»å…¥æ™‚è¿”å›åŸºæœ¬ç‹€æ…‹
    return jsonify({
        'is_loading': False,
        'current': 0,
        'total': 0,
        'phase': '',
        'current_topic': ''
    })

@app.route('/api/admin/topics', methods=['GET'])
def get_topics():
    # èªè­‰æª¢æŸ¥ï¼ˆå¦‚æœèªè­‰ç³»çµ±å·²å•Ÿç”¨ï¼‰
    if AUTH_ENABLED:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not token:
            return jsonify({'error': 'æœªç™»å…¥'}), 401
        user = auth.get_user_from_token(token)
        if not user:
            return jsonify({'error': 'èªè­‰å¤±æ•—'}), 401
        
        # å¾ Supabase è®€å–è©²ä½¿ç”¨è€…çš„å°ˆé¡Œ
        user_topics = auth.get_user_topics(user.id)
        
        result = {}
        for topic in user_topics:
            tid = topic['id']
            keywords = topic.get('keywords', {})
            
            # è™•ç†é—œéµå­—æ ¼å¼
            keywords_full = normalize_keywords(keywords)
            display_keywords = keywords_full.get('zh', [])
            
            # å–å¾—æ‘˜è¦ï¼ˆå¾æœ¬åœ°å¿«å–ï¼‰
            summary_data = DATA_STORE['summaries'].get(tid, {})
            
            # å–å¾—æ–°èæ•¸é‡ï¼ˆå¾æœ¬åœ°å¿«å–ï¼‰
            news_count = len(DATA_STORE['topics'].get(tid, []))
            
            result[tid] = {
                'name': topic['name'],
                'keywords': display_keywords,
                'keywords_full': keywords_full,
                'negative_keywords': topic.get('negative_keywords', []),
                'icon': topic.get('icon', 'ğŸ“Œ'),
                'summary': summary_data.get('text', ''),
                'summary_updated': summary_data.get('updated_at'),
                'news_count': news_count,
                'order': topic.get('order', 999)
            }
        
        return jsonify({'topics': result, 'last_update': DATA_STORE['last_update']})
    
    else:
        # èªè­‰æœªå•Ÿç”¨æ™‚ä½¿ç”¨èˆŠçš„å…±äº«å°ˆé¡Œï¼ˆå‘å¾Œç›¸å®¹ï¼‰
        result = {}
        for tid, cfg in TOPICS.items():
            keywords = cfg.get('keywords', [])
            keywords_full = normalize_keywords(keywords)
            display_keywords = keywords_full.get('zh', [])

            summary_data = DATA_STORE['summaries'].get(tid, {})
            news_count = len(DATA_STORE['topics'].get(tid, []))

            result[tid] = {
                'name': cfg['name'],
                'keywords': display_keywords,
                'keywords_full': keywords_full,
                'negative_keywords': cfg.get('negative_keywords', []),
                'icon': cfg.get('icon', ''),
                'summary': summary_data.get('text', ''),
                'summary_updated': summary_data.get('updated_at'),
                'news_count': news_count,
                'order': cfg.get('order', 999)
            }
        return jsonify({'topics': result, 'last_update': DATA_STORE['last_update']})

@app.route('/api/admin/topics', methods=['POST'])
def add_topic():
    # èªè­‰æª¢æŸ¥ï¼ˆå¦‚æœèªè­‰ç³»çµ±å·²å•Ÿç”¨ï¼‰
    if AUTH_ENABLED:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not token:
            return jsonify({'error': 'æœªç™»å…¥'}), 401
        user = auth.get_user_from_token(token)
        if not user:
            return jsonify({'error': 'èªè­‰å¤±æ•—'}), 401
        
        data = request.json
        name = data.get('name', '').strip()
        if not name:
            return jsonify({'error': 'Empty name'}), 400

        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ AI ç”Ÿæˆé—œéµå­—ï¼ˆé è¨­ç‚º trueï¼‰
        generate_keywords = data.get('generate_keywords', True)

        if generate_keywords:
            # AI ç”Ÿæˆé—œéµå­—
            keywords = generate_keywords_with_ai(name)
        else:
            # æª¢æŸ¥æ˜¯å¦éœ€è¦è‡ªå‹•ç¿»è­¯
            auto_translate = data.get('auto_translate', False)
            
            if auto_translate:
                # ä½¿ç”¨å°ˆé¡Œåç¨±ä½œç‚ºä¸­æ–‡é—œéµå­—ï¼Œä¸¦è‡ªå‹•ç¿»è­¯æˆå…¶ä»–èªè¨€
                print(f"[AUTO-TRANSLATE] ç‚ºå°ˆé¡Œã€Œ{name}ã€è‡ªå‹•ç¿»è­¯é—œéµå­—...")
                keywords = auto_translate_keywords([name])
            else:
                # åªä½¿ç”¨å°ˆé¡Œåç¨±ä½œç‚ºä¸­æ–‡é—œéµå­—
                keywords = {
                    'zh': [name],
                    'en': [],
                    'ja': [],
                    'ko': []
                }

        # è¨ˆç®—æ–°å°ˆé¡Œçš„ order
        user_topics = auth.get_user_topics(user.id)
        max_order = max([t.get('order', 0) for t in user_topics], default=-1)
        new_order = max_order + 1

        # å„²å­˜åˆ° Supabase
        new_topic = auth.create_topic(
            user_id=user.id,
            name=name,
            keywords=keywords,
            icon='ğŸ“Œ',
            negative_keywords=[],
            order=new_order
        )
        
        if not new_topic:
            return jsonify({'error': 'å»ºç«‹å°ˆé¡Œå¤±æ•—'}), 500
        
        tid = new_topic['id']
        
        # æ›´æ–°æœ¬åœ°å¿«å–ä¾›æ–°èæŠ“å–ä½¿ç”¨
        TOPICS[tid] = {'name': name, 'keywords': keywords, 'order': new_order, 'user_id': user.id}
        
        # âœ¨ åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­æ›´æ–°æ–°èå’Œç”Ÿæˆæ‘˜è¦ï¼Œé¿å…é˜»å¡ API å›æ‡‰
        def background_init():
            global LOADING_STATUS
            try:
                # æ›´æ–°ç‹€æ…‹æ¬„ï¼šé¡¯ç¤ºæ­£åœ¨è™•ç†æ–°å°ˆé¡Œ
                LOADING_STATUS = {
                    'is_loading': True,
                    'current': 1,
                    'total': 2,
                    'current_topic': name,
                    'phase': 'è’é›†è³‡æ–™ä¸­'
                }
                
                # æ›´æ–°æ–°å°ˆé¡Œçš„æ–°è
                update_single_topic_news(tid)
                
                # æ›´æ–°ç‹€æ…‹ï¼šæº–å‚™ç”Ÿæˆæ‘˜è¦
                LOADING_STATUS['current'] = 2
                LOADING_STATUS['phase'] = 'ç”Ÿæˆå‹•æ…‹ä¸­'
                
                # ç‚ºæ–°å°ˆé¡Œç”Ÿæˆæ‘˜è¦
                if PERPLEXITY_API_KEY:
                    print(f"[INIT] ç‚ºæ–°å°ˆé¡Œã€Œ{name}ã€ç”Ÿæˆ AI æ‘˜è¦...")
                    summary_text = generate_topic_summary(tid)
                    DATA_STORE['summaries'][tid] = {
                        'text': summary_text,
                        'updated_at': datetime.now(TAIPEI_TZ).isoformat()
                    }
                    save_data_cache()
                
                # å®Œæˆï¼šæ¸…é™¤è¼‰å…¥ç‹€æ…‹
                LOADING_STATUS = {
                    'is_loading': False,
                    'current': 2,
                    'total': 2,
                    'current_topic': '',
                    'phase': ''
                }
                    
                print(f"[INIT] å°ˆé¡Œã€Œ{name}ã€åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"[ERROR] å°ˆé¡Œã€Œ{name}ã€èƒŒæ™¯åˆå§‹åŒ–å¤±æ•—: {e}")
                # ç™¼ç”ŸéŒ¯èª¤æ™‚ä¹Ÿè¦æ¸…é™¤è¼‰å…¥ç‹€æ…‹
                LOADING_STATUS['is_loading'] = False
        
        # å•Ÿå‹•èƒŒæ™¯åŸ·è¡Œç·’
        thread = threading.Thread(target=background_init, daemon=True)
        thread.start()

        # ç«‹å³å›æ‡‰å‰ç«¯ï¼Œä¸ç­‰å¾…æ–°èå’Œæ‘˜è¦
        return jsonify({'status': 'ok', 'topic_id': tid})
    
    else:
        # èªè­‰æœªå•Ÿç”¨æ™‚ä½¿ç”¨èˆŠé‚è¼¯ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
        data = request.json
        name = data.get('name', '').strip()
        if not name:
            return jsonify({'error': 'Empty name'}), 400

        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ AI ç”Ÿæˆé—œéµå­—
        generate_keywords = data.get('generate_keywords', True)
        
        if generate_keywords:
            keywords = generate_keywords_with_ai(name)
        else:
            # æª¢æŸ¥æ˜¯å¦éœ€è¦è‡ªå‹•ç¿»è­¯
            auto_translate = data.get('auto_translate', False)
            
            if auto_translate:
                print(f"[AUTO-TRANSLATE] ç‚ºå°ˆé¡Œã€Œ{name}ã€è‡ªå‹•ç¿»è­¯é—œéµå­—...")
                keywords = auto_translate_keywords([name])
            else:
                keywords = {
                    'zh': [name],
                    'en': [],
                    'ja': [],
                    'ko': []
                }
        
        max_order = max([t.get('order', 0) for t in TOPICS.values()], default=-1)
        new_order = max_order + 1

        tid = generate_topic_id(name)
        TOPICS[tid] = {'name': name, 'keywords': keywords, 'order': new_order}
        save_topics_config()

        # âœ¨ åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­æ›´æ–°æ–°èå’Œç”Ÿæˆæ‘˜è¦ï¼Œé¿å…é˜»å¡ API å›æ‡‰  
        def background_init():
            global LOADING_STATUS
            try:
                # æ›´æ–°ç‹€æ…‹æ¬„ï¼šé¡¯ç¤ºæ­£åœ¨è™•ç†æ–°å°ˆé¡Œ
                LOADING_STATUS = {
                    'is_loading': True,
                    'current': 1,
                    'total': 2,
                    'current_topic': name,
                    'phase': 'è’é›†è³‡æ–™ä¸­'
                }
                
                # æ›´æ–°æ–°å°ˆé¡Œçš„æ–°è
                update_single_topic_news(tid)
                
                # æ›´æ–°ç‹€æ…‹ï¼šæº–å‚™ç”Ÿæˆæ‘˜è¦
                LOADING_STATUS['current'] = 2
                LOADING_STATUS['phase'] = 'ç”Ÿæˆå‹•æ…‹ä¸­'
                
                # ç‚ºæ–°å°ˆé¡Œç”Ÿæˆæ‘˜è¦
                if PERPLEXITY_API_KEY:
                    print(f"[INIT] ç‚ºæ–°å°ˆé¡Œã€Œ{name}ã€ç”Ÿæˆ AI æ‘˜è¦...")
                    summary_text = generate_topic_summary(tid)
                    DATA_STORE['summaries'][tid] = {
                        'text': summary_text,
                        'updated_at': datetime.now(TAIPEI_TZ).isoformat()
                    }
                    save_data_cache()
                
                # å®Œæˆï¼šæ¸…é™¤è¼‰å…¥ç‹€æ…‹
                LOADING_STATUS = {
                    'is_loading': False,
                    'current': 2,
                    'total': 2,
                    'current_topic': '',
                    'phase': ''
                }
                    
                print(f"[INIT] å°ˆé¡Œã€Œ{name}ã€åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"[ERROR] å°ˆé¡Œã€Œ{name}ã€èƒŒæ™¯åˆå§‹åŒ–å¤±æ•—: {e}")
                # ç™¼ç”ŸéŒ¯èª¤æ™‚ä¹Ÿè¦æ¸…é™¤è¼‰å…¥ç‹€æ…‹
                LOADING_STATUS['is_loading'] = False
        
        # å•Ÿå‹•èƒŒæ™¯åŸ·è¡Œç·’
        thread = threading.Thread(target=background_init, daemon=True)
        thread.start()

        # ç«‹å³å›æ‡‰å‰ç«¯ï¼Œä¸ç­‰å¾…æ–°èå’Œæ‘˜è¦
        return jsonify({'status': 'ok'})

@app.route('/api/admin/topics/<tid>', methods=['PUT'])
def update_topic(tid):
    # èªè­‰æª¢æŸ¥ï¼ˆå¦‚æœèªè­‰ç³»çµ±å·²å•Ÿç”¨ï¼‰
    if AUTH_ENABLED:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not token:
            return jsonify({'error': 'æœªç™»å…¥'}), 401
        user = auth.get_user_from_token(token)
        if not user:
            return jsonify({'error': 'èªè­‰å¤±æ•—'}), 401
        
        data = request.json
        updates = {}
        if 'keywords' in data:
            updates['keywords'] = data['keywords']
        if 'negative_keywords' in data:
            updates['negative_keywords'] = data['negative_keywords']
        
        # æ›´æ–° Supabaseï¼ˆæœƒé©—è­‰æ“æœ‰è€…ï¼‰
        success = auth.update_topic(tid, user.id, updates)
        if not success:
            return jsonify({'error': 'æ›´æ–°å¤±æ•—æˆ–ç„¡æ¬Šé™'}), 403
        
        # æ›´æ–°æœ¬åœ°å¿«å–
        if tid in TOPICS:
            TOPICS[tid].update(updates)
        
        # åœ¨èƒŒæ™¯ç·šç¨‹åŸ·è¡Œæ–°èæ›´æ–°
        import threading
        update_thread = threading.Thread(target=update_topic_news, daemon=True)
        update_thread.start()
        
        return jsonify({'status': 'ok', 'message': 'é—œéµå­—å·²å„²å­˜ï¼Œæ–°èæ­£åœ¨èƒŒæ™¯æ›´æ–°'})
    
    else:
        # èªè­‰æœªå•Ÿç”¨æ™‚ä½¿ç”¨èˆŠé‚è¼¯
        if tid not in TOPICS:
            return jsonify({'error': 'Not found'}), 404
        data = request.json
        if 'keywords' in data:
            TOPICS[tid]['keywords'] = data['keywords']
        if 'negative_keywords' in data:
            TOPICS[tid]['negative_keywords'] = data['negative_keywords']
        save_topics_config()
        
        import threading
        update_thread = threading.Thread(target=update_topic_news, daemon=True)
        update_thread.start()
        
        return jsonify({'status': 'ok', 'message': 'é—œéµå­—å·²å„²å­˜ï¼Œæ–°èæ­£åœ¨èƒŒæ™¯æ›´æ–°'})

@app.route('/api/admin/topics/<tid>', methods=['DELETE'])
def delete_topic(tid):
    # èªè­‰æª¢æŸ¥ï¼ˆå¦‚æœèªè­‰ç³»çµ±å·²å•Ÿç”¨ï¼‰
    if AUTH_ENABLED:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not token:
            return jsonify({'error': 'æœªç™»å…¥'}), 401
        user = auth.get_user_from_token(token)
        if not user:
            return jsonify({'error': 'èªè­‰å¤±æ•—'}), 401
        
        # å¾ Supabase åˆªé™¤ï¼ˆæœƒé©—è­‰æ“æœ‰è€…ï¼‰
        success = auth.delete_topic(tid, user.id)
        if not success:
            return jsonify({'error': 'åˆªé™¤å¤±æ•—æˆ–ç„¡æ¬Šé™'}), 403
        
        # å¾æœ¬åœ°å¿«å–åˆªé™¤
        if tid in TOPICS:
            del TOPICS[tid]
        if tid in DATA_STORE['topics']:
            del DATA_STORE['topics'][tid]
        if tid in DATA_STORE['summaries']:
            del DATA_STORE['summaries'][tid]
        
        return jsonify({'status': 'ok'})
    
    else:
        # èªè­‰æœªå•Ÿç”¨æ™‚ä½¿ç”¨èˆŠé‚è¼¯
        if tid in TOPICS:
            del TOPICS[tid]
            save_topics_config()
        return jsonify({'status': 'ok'})

@app.route('/api/admin/topics/reorder', methods=['PUT'])
def reorder_topics():
    """æ›´æ–°å°ˆé¡Œæ’åº"""
    data = request.json
    order_list = data.get('order', [])
    
    # èªè­‰æª¢æŸ¥ï¼ˆå¦‚æœèªè­‰ç³»çµ±å·²å•Ÿç”¨ï¼‰
    if AUTH_ENABLED:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not token:
            return jsonify({'error': 'æœªç™»å…¥'}), 401
        user = auth.get_user_from_token(token)
        if not user:
            return jsonify({'error': 'èªè­‰å¤±æ•—'}), 401
            
        # èªè­‰æ¨¡å¼ï¼šæ›´æ–° Supabase è³‡æ–™åº«
        updated_count = 0
        for item in order_list:
            tid = item.get('id')
            order = item.get('order')
            if tid and order is not None:
                # å‘¼å« auth.update_topic æ›´æ–°é †åº
                if auth.update_topic(tid, user.id, {'order': order}):
                    updated_count += 1
                    
        print(f"[REORDER] å·²æ›´æ–°ä½¿ç”¨è€… {user.id} çš„ {updated_count} å€‹å°ˆé¡Œé †åº")
        return jsonify({'status': 'ok', 'updated': updated_count})

    print(f"[REORDER] æ”¶åˆ°æ’åºè«‹æ±‚: {order_list}")

    # æ›´æ–°æ¯å€‹å°ˆé¡Œçš„ order æ¬„ä½
    for item in order_list:
        tid = item.get('id')
        order = item.get('order')
        if tid in TOPICS:
            TOPICS[tid]['order'] = order
            print(f"[REORDER] æ›´æ–° {TOPICS[tid]['name']} çš„é †åºç‚º {order}")

    save_topics_config()
    print("[REORDER] é †åºå·²å„²å­˜åˆ° topics_config.json")
    return jsonify({'status': 'ok'})

# ============ èªè­‰ API ============

# å˜—è©¦è¼‰å…¥èªè­‰æ¨¡çµ„ï¼ˆå¦‚æœ Supabase å·²è¨­å®šï¼‰
try:
    import auth
    AUTH_ENABLED = bool(os.getenv('SUPABASE_URL') and os.getenv('SUPABASE_KEY'))
except ImportError:
    AUTH_ENABLED = False
    print("[AUTH] èªè­‰æ¨¡çµ„æœªè¼‰å…¥ï¼ˆauth.py ä¸å­˜åœ¨æˆ– Supabase æœªè¨­å®šï¼‰")

@app.route('/api/auth/status')
def auth_status():
    """æª¢æŸ¥èªè­‰ç³»çµ±ç‹€æ…‹ï¼ˆå¯é¸ï¼šé©—è­‰ tokenï¼‰"""
    result = {
        'enabled': AUTH_ENABLED,
        'supabase_configured': bool(os.getenv('SUPABASE_URL'))
    }

    # å¦‚æœæä¾›äº† tokenï¼Œé©—è­‰å…¶æœ‰æ•ˆæ€§
    if AUTH_ENABLED:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if token:
            try:
                user = auth.get_user_from_token(token)
                if user:
                    result['authenticated'] = True
                    result['user'] = {
                        'email': user.email,
                        'id': user.id
                    }
                else:
                    result['authenticated'] = False
            except:
                result['authenticated'] = False
        else:
            result['authenticated'] = False

    return jsonify(result)

@app.route('/api/auth/signup', methods=['POST'])
def auth_signup():
    """ä½¿ç”¨è€…è¨»å†Šï¼ˆéœ€è¦é‚€è«‹ç¢¼ï¼‰"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    data = request.json
    email = data.get('email', '').strip()
    password = data.get('password', '')
    invite_code = data.get('invite_code', '').strip()
    
    if not email or not password:
        return jsonify({'error': 'è«‹å¡«å¯« Email å’Œå¯†ç¢¼'}), 400
    
    if not invite_code:
        return jsonify({'error': 'è«‹è¼¸å…¥é‚€è«‹ç¢¼'}), 400
    
    if len(password) < 6:
        return jsonify({'error': 'å¯†ç¢¼è‡³å°‘éœ€è¦ 6 å€‹å­—å…ƒ'}), 400
    
    result, error = auth.signup(email, password, invite_code)
    
    if error:
        return jsonify({'error': error}), 400
    
    # è¨»å†ŠæˆåŠŸï¼Œè‡ªå‹•ç™»å…¥
    login_result, login_error = auth.login(email, password)

    if login_error:
        return jsonify({'error': 'è¨»å†ŠæˆåŠŸï¼æˆ‘å€‘å·²ç™¼é€ç¢ºèªä¿¡åˆ°æ‚¨çš„ä¿¡ç®±ï¼Œè«‹é»æ“Šä¿¡ä¸­çš„é€£çµä»¥å•Ÿç”¨å¸³è™Ÿï¼Œç„¶å¾Œå†å›ä¾†ç™»å…¥ã€‚'}), 200
    
    return jsonify({
        'access_token': login_result.session.access_token,
        'refresh_token': login_result.session.refresh_token,
        'user': {
            'id': login_result.user.id,
            'email': login_result.user.email
        },
        'role': auth.get_user_role(login_result.user.id)
    })

@app.route('/api/auth/login', methods=['POST'])
def auth_login():
    """ä½¿ç”¨è€…ç™»å…¥"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    data = request.json
    email = data.get('email', '').strip()
    password = data.get('password', '')
    
    if not email or not password:
        return jsonify({'error': 'è«‹å¡«å¯« Email å’Œå¯†ç¢¼'}), 400
    
    result, error = auth.login(email, password)
    
    if error:
        return jsonify({'error': error}), 401
    
    return jsonify({
        'access_token': result.session.access_token,
        'refresh_token': result.session.refresh_token,
        'user': {
            'id': result.user.id,
            'email': result.user.email
        },
        'role': auth.get_user_role(result.user.id)
    })

@app.route('/api/auth/logout', methods=['POST'])
def auth_logout():
    """ä½¿ç”¨è€…ç™»å‡º"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    auth.logout(token)
    return jsonify({'status': 'ok'})

@app.route('/api/auth/me')
def auth_me():
    """å–å¾—ç•¶å‰ä½¿ç”¨è€…è³‡è¨Š"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    if not token:
        return jsonify({'error': 'æœªç™»å…¥'}), 401
    
    user = auth.get_user_from_token(token)
    if not user:
        return jsonify({'error': 'èªè­‰å¤±æ•—'}), 401
    
    return jsonify({
        'user': {
            'id': user.id,
            'email': user.email
        },
        'role': auth.get_user_role(user.id)
    })

# ============ é‚€è«‹ç¢¼ç®¡ç† APIï¼ˆç®¡ç†å“¡ï¼‰============

@app.route('/api/admin/invites', methods=['GET'])
def get_invites():
    """å–å¾—æ‰€æœ‰é‚€è«‹ç¢¼"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    # é©—è­‰ç®¡ç†å“¡æ¬Šé™
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    if not token:
        return jsonify({'error': 'æœªç™»å…¥'}), 401
    
    user = auth.get_user_from_token(token)
    if not user or not auth.is_admin(user.id):
        return jsonify({'error': 'éœ€è¦ç®¡ç†å“¡æ¬Šé™'}), 403
    
    invites = auth.get_invite_codes()
    return jsonify({'invites': invites})

@app.route('/api/admin/invites', methods=['POST'])
def create_invite():
    """å»ºç«‹é‚€è«‹ç¢¼"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    if not token:
        return jsonify({'error': 'æœªç™»å…¥'}), 401
    
    user = auth.get_user_from_token(token)
    if not user or not auth.is_admin(user.id):
        return jsonify({'error': 'éœ€è¦ç®¡ç†å“¡æ¬Šé™'}), 403
    
    data = request.json or {}
    expires_days = data.get('expires_days', 7)
    
    invite = auth.generate_invite_code(user.id, expires_days)
    if invite:
        return jsonify({'invite': invite})
    else:
        return jsonify({'error': 'å»ºç«‹é‚€è«‹ç¢¼å¤±æ•—'}), 500

@app.route('/api/admin/invites/<invite_id>', methods=['DELETE'])
def delete_invite(invite_id):
    """åˆªé™¤é‚€è«‹ç¢¼"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    if not token:
        return jsonify({'error': 'æœªç™»å…¥'}), 401
    
    user = auth.get_user_from_token(token)
    if not user or not auth.is_admin(user.id):
        return jsonify({'error': 'éœ€è¦ç®¡ç†å“¡æ¬Šé™'}), 403
    
    if auth.delete_invite_code(invite_id):
        return jsonify({'status': 'ok'})
    else:
        return jsonify({'error': 'åˆªé™¤å¤±æ•—'}), 500

# ============ ä½¿ç”¨è€…ç®¡ç† APIï¼ˆç®¡ç†å“¡ï¼‰============

@app.route('/api/admin/users', methods=['GET'])
def get_users():
    """å–å¾—æ‰€æœ‰ä½¿ç”¨è€…"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    if not token:
        return jsonify({'error': 'æœªç™»å…¥'}), 401
    
    user = auth.get_user_from_token(token)
    if not user or not auth.is_admin(user.id):
        return jsonify({'error': 'éœ€è¦ç®¡ç†å“¡æ¬Šé™'}), 403
    
    users = auth.get_all_users()
    return jsonify({'users': users})

@app.route('/api/admin/users/<user_id>/role', methods=['PUT'])
def update_user_role(user_id):
    """æ›´æ–°ä½¿ç”¨è€…è§’è‰²"""
    if not AUTH_ENABLED:
        return jsonify({'error': 'èªè­‰ç³»çµ±æœªå•Ÿç”¨'}), 503
    
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    if not token:
        return jsonify({'error': 'æœªç™»å…¥'}), 401
    
    user = auth.get_user_from_token(token)
    if not user or not auth.is_admin(user.id):
        return jsonify({'error': 'éœ€è¦ç®¡ç†å“¡æ¬Šé™'}), 403
    
    data = request.json or {}
    role = data.get('role', 'user')
    
    if auth.update_user_role(user_id, role):
        return jsonify({'status': 'ok'})
    else:
        return jsonify({'error': 'æ›´æ–°å¤±æ•—'}), 500

# ============ Main ============

def init_scheduler():
    scheduler = BackgroundScheduler(timezone='Asia/Taipei')
    # æ–°èæ›´æ–°æ’ç¨‹ï¼ˆæ¯å°æ™‚ä¸€æ¬¡ï¼ŒéŒ¯é–‹æ™‚é–“ï¼‰
    scheduler.add_job(update_domestic_news, 'cron', minute='0')
    scheduler.add_job(update_international_news, 'cron', minute='30')
    
    # æ‘˜è¦ç”Ÿæˆæ’ç¨‹ï¼ˆæ¯å¤© 08:00, 12:00, 18:00ï¼‰
    scheduler.add_job(update_all_summaries, 'cron', hour=8, minute=0)
    scheduler.add_job(update_all_summaries, 'cron', hour=12, minute=0)
    scheduler.add_job(update_all_summaries, 'cron', hour=18, minute=0)
    scheduler.start()
    print("[SCHEDULER] æ’ç¨‹å·²å•Ÿå‹• - åœ‹å…§:æ¯å°æ™‚0åˆ†, åœ‹éš›:æ¯å°æ™‚30åˆ†, æ‘˜è¦:08:00/12:00/18:00")

import urllib3
urllib3.disable_warnings()

# ============ æ¨¡çµ„è¼‰å…¥æ™‚åˆå§‹åŒ–ï¼ˆGunicorn éœ€è¦ï¼‰============
load_topics_config()
load_data_cache()  # å…ˆå¾å¿«å–è¼‰å…¥è³‡æ–™ï¼ˆå¿«é€Ÿå•Ÿå‹•ï¼‰
init_scheduler()

if __name__ == '__main__':
    import threading
    import sys

    # æŒ‰éœ€è¼‰å…¥ç­–ç•¥ï¼šä¸åœ¨å•Ÿå‹•æ™‚è¼‰å…¥æ‰€æœ‰ä½¿ç”¨è€…è³‡æ–™
    # ä½¿ç”¨è€…ç™»å…¥æ™‚æ‰æœƒè¼‰å…¥ä»–å€‘çš„å°ˆé¡Œè³‡æ–™
    print("[SERVER] ä¼ºæœå™¨å•Ÿå‹•ä¸­... (ä½¿ç”¨æŒ‰éœ€è¼‰å…¥ç­–ç•¥ï¼Œä½¿ç”¨è€…ç™»å…¥æ™‚æ‰è¼‰å…¥è³‡æ–™)")
    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)
